{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unavailable-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.8.3 (default, May 19 2020, 18:47:26) \n",
      "[GCC 7.3.0]\n",
      "__pyTorch VERSION: 1.7.0\n",
      "__CUDA VERSION\n",
      "/usr/bin/sh: 1: nvcc: not found\n",
      "__CUDNN VERSION: 8003\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/i-have-3-gpu-why-torch-cuda-device-count-only-return-1/7245/4\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchucooleg\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import json\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import collections\n",
    "\n",
    "import operator as op\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-cooperative",
   "metadata": {},
   "source": [
    "## Data Generation -- Full Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "agreed-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = {\n",
    "    'color': ['red', 'green', 'blue', 'orange', 'cyan', 'magenta', 'black', 'yellow'],\n",
    "    'fill': ['void', 'dashed', 'solid', 'checkered', 'dotted', 'mosaic', 'noise', 'brushed'],\n",
    "    'shape': ['square', 'circle', 'triangle', 'star', 'hexagon', 'pentagon', 'ellipse', 'rectangle'],\n",
    "    'config': ['OOO', 'OOX', 'OXO', 'OXX', 'XOO', 'XOX', 'XXO', 'XXX']\n",
    "}\n",
    "\n",
    "attr_order = ['color', 'fill', 'shape', 'config']\n",
    "\n",
    "def generate_cards(attributes, attr_order, num_attributes, num_attr_vals):\n",
    "    \n",
    "    attr_order = attr_order[:num_attributes]\n",
    "    attributes = {att_k:attributes[att_k][:num_attr_vals] for att_k in attributes if att_k in attr_order}\n",
    "    \n",
    "    cards = []\n",
    "    idx_to_card = {}\n",
    "    card_to_idx = {}\n",
    "\n",
    "    i = 0\n",
    "    attr_vals = [attributes[attr] for attr in attr_order]\n",
    "    for combo in itertools.product(*attr_vals):\n",
    "        card = tuple(combo)\n",
    "        cards.append(card)\n",
    "        card_to_idx[card] = i\n",
    "        idx_to_card[i] = card\n",
    "        i += 1\n",
    "    \n",
    "    null_card = tuple(['None'] * num_attributes)\n",
    "    null_card_idx = len(idx_to_card)\n",
    "    cards.append(null_card)\n",
    "    idx_to_card[null_card_idx] = null_card\n",
    "    card_to_idx[null_card] = null_card_idx\n",
    "    \n",
    "    assert len(cards) == len(set(cards))\n",
    "    print(f'Generated {len(cards)} unqiue cards')                \n",
    "    return cards, idx_to_card, card_to_idx\n",
    "\n",
    "def num_shared_attributes(card1, card2):\n",
    "    matching_concepts = tuple(s1 if s1==s2 else '-' for s1,s2 in zip(card1,card2))\n",
    "    num_matching_concepts = len([c for c in matching_concepts if c != '-'])\n",
    "    return matching_concepts, num_matching_concepts\n",
    "\n",
    "def generate_card_pairs(cards, card_to_idx):\n",
    "    '''\n",
    "    find all combos of cards, filter down to the ones that share concepts.\n",
    "    '''\n",
    "    cardpairs_with_shared_concepts = []\n",
    "    cardpairs_without_shared_concepts = []\n",
    "\n",
    "    cardpair_to_idx, idx_to_cardpair, idx = {}, {}, 0\n",
    "    num_matching_concepts_all = {}\n",
    "    for card1, card2 in itertools.product(cards, repeat=2):\n",
    "        if 'None' not in card1 and 'None' not in card2:\n",
    "            matching_concepts, num_matching_concepts = num_shared_attributes(card1, card2)\n",
    "            num_matching_concepts_all[(card1, card2)] = num_matching_concepts\n",
    "            if num_matching_concepts:\n",
    "                cardpairs_with_shared_concepts.append(((card_to_idx[card1], card_to_idx[card2]), matching_concepts))\n",
    "            else:\n",
    "                cardpairs_without_shared_concepts.append(((card_to_idx[card1], card_to_idx[card2]), matching_concepts))\n",
    "            idx_to_cardpair[idx] = (card_to_idx[card1], card_to_idx[card2])\n",
    "            cardpair_to_idx[(card_to_idx[card1], card_to_idx[card2])] = idx\n",
    "            idx += 1\n",
    "    print(f'Generated {len(cardpairs_with_shared_concepts) + len(cardpairs_without_shared_concepts)} cardpairs')\n",
    "    print(f'-- {len(cardpairs_with_shared_concepts)} cardpairs with shared concept')\n",
    "    print(f'-- {len(cardpairs_without_shared_concepts)} cardpairs without shared concept')\n",
    "    print('Number of cardpairs per shared concept', Counter(num_matching_concepts_all.values()).most_common())\n",
    "    return cardpairs_with_shared_concepts, cardpairs_without_shared_concepts, cardpair_to_idx, idx_to_cardpair\n",
    "\n",
    "def match_concept_to_card(concept, card):\n",
    "    '''\n",
    "    Given a concept, determine if card matches.\n",
    "    \n",
    "    Arguments:\n",
    "        concept: ('red', 'void', '-', '-')\n",
    "        card: ex1. ('red', 'void', 'triangle', 'XOX')\n",
    "              ex2. ('green', 'void', 'square', 'OXX')\n",
    "    Returns:\n",
    "        match: bool. ex1. True,\n",
    "                     ex2. False\n",
    "    '''\n",
    "    match = 0\n",
    "    for ct, cd in zip(concept, card):\n",
    "        # As long as one concept matches, it is a match!\n",
    "        if ct == cd:\n",
    "            match += 1\n",
    "    return match\n",
    "\n",
    "\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=100, debug=False):\n",
    "    \n",
    "    # all cards\n",
    "    cards, idx_to_card, card_to_idx = generate_cards(attributes, attr_order, num_attributes, num_attr_vals)\n",
    "    # all card pairs\n",
    "    cardpairs_with_shared_concepts, cardpairs_without_shared_concepts, cardpair_to_idx, idx_to_cardpair = \\\n",
    "        generate_card_pairs(cards, card_to_idx)\n",
    "    \n",
    "    # generate answers\n",
    "    all_matches = []\n",
    "    number_of_shared_concepts_per_match = []\n",
    "    cardpair_to_matches = defaultdict(lambda : defaultdict(int))\n",
    "    \n",
    "    for cardpair in cardpairs_with_shared_concepts:\n",
    "        shared_concept = cardpair[1]\n",
    "        # look for all matching cards\n",
    "        # ((card pair query), matching card)\n",
    "        for card in cards:\n",
    "            num_matched_concepts = match_concept_to_card(shared_concept, card)\n",
    "            if num_matched_concepts:\n",
    "                all_matches.append((cardpair_to_idx[cardpair[0]], card_to_idx[card]))\n",
    "                cardpair_to_matches[cardpair_to_idx[cardpair[0]]][card_to_idx[card]] = num_matched_concepts\n",
    "                number_of_shared_concepts_per_match.append(num_matched_concepts)\n",
    "                \n",
    "    for cardpair in cardpairs_without_shared_concepts:\n",
    "        all_matches.append((cardpair_to_idx[cardpair[0]], len(cards)-1))\n",
    "        cardpair_to_matches[cardpair_to_idx[cardpair[0]]][len(cards)-1] = 0\n",
    "        number_of_shared_concepts_per_match.append(0)\n",
    "                \n",
    "    cardpair_to_matches = default_to_regular(cardpair_to_matches)\n",
    "    print('Total number of matches = ', len(all_matches))\n",
    "    print('Number of matches per key concept hit = ', Counter(number_of_shared_concepts_per_match).most_common())\n",
    "    \n",
    "    # hold out some cardpairs\n",
    "    unseen_cardpair_indices = list(np.random.choice(len(cardpair_to_idx), size=num_unseen_cardpairs, replace=False))\n",
    "    train_cardpair_indices = [idx for idx in range(len(cardpair_to_idx)) if idx not in unseen_cardpair_indices]\n",
    "    assert len(set(unseen_cardpair_indices) | set(train_cardpair_indices)) == len(cardpair_to_idx)\n",
    "    \n",
    "    data = {\n",
    "        'num_attributes': num_attributes,\n",
    "        'num_attr_vals': num_attr_vals,\n",
    "        'idx_to_key': idx_to_card,\n",
    "        'key_to_idx': card_to_idx,\n",
    "        'query_to_idx': cardpair_to_idx, \n",
    "        'idx_to_query': idx_to_cardpair,\n",
    "        'query_support_size': len(idx_to_cardpair),\n",
    "        'key_support_size': len(idx_to_card),\n",
    "        'all_matches': all_matches, # list of tuples (query idx, answer card)\n",
    "        'query_to_keys': cardpair_to_matches, # lookup query idx:{'card1':num matched concepts, 'card2':num matched concepts,...}\n",
    "        'unseen_query_indices': unseen_cardpair_indices,\n",
    "        'seen_query_indices': train_cardpair_indices\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gothic-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_table(game_data):\n",
    "    count_table = np.zeros((len(game_data['idx_to_key']), len(game_data['idx_to_query'])))\n",
    "    for q, k in game_data['all_matches']:\n",
    "        count_table[k, q] += 1\n",
    "    return count_table\n",
    "\n",
    "def get_distribution(count_table, distribution_epsilon=0.0):\n",
    "    xy = count_table/np.sum(count_table)\n",
    "    xy += distribution_epsilon\n",
    "    xy /= np.sum(xy)\n",
    "    x = np.sum(xy,0)\n",
    "    y = np.sum(xy,1)\n",
    "    xyind = y[None].T @ x[None]\n",
    "    return xy, xyind, xy/xyind\n",
    "\n",
    "def report_countable_distribution(count_table, distribution_epsilon=0.0):\n",
    "    sparsity = np.sum(count_table) * 1.0 / (count_table.shape[0] * count_table.shape[1])\n",
    "    xy, xyind, xy_div_xyind = get_distribution(count_table, distribution_epsilon)\n",
    "    xy_rank = np.linalg.matrix_rank(xy)\n",
    "    xy_div_xyind_rank = np.linalg.matrix_rank(xy_div_xyind)\n",
    "    \n",
    "    distribution = {\n",
    "        'shape': xy.shape,\n",
    "        'size': xy.shape[0] * xy.shape[1],\n",
    "        'sparsity': sparsity,\n",
    "        'xy_rank': xy_rank,\n",
    "        'xy_div_xyind_rank': xy_div_xyind_rank\n",
    "    }\n",
    "    \n",
    "    return count_table, xy, xyind, xy_div_xyind, distribution\n",
    "\n",
    "def report_gamedata_distribution(game_data, distribution_epsilon=0.0):\n",
    "    count_table = get_count_table(game_data)\n",
    "    return report_countable_distribution(count_table, distribution_epsilon=0.0)\n",
    "\n",
    "def plot_distribution(xy, xy_div_xyind, dist_name, figsize):\n",
    "    figrange = (0, xy.shape[0], 0, xy.shape[1])\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.title(dist_name+' xy, All {} quries'.format(xy.shape[1]))\n",
    "    plt.imshow((xy)[figrange[0]:figrange[1], figrange[2]:figrange[3]])\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.title(dist_name+' xy_div_xyind, All {} quries'.format(xy.shape[1]))\n",
    "    plt.imshow((xy_div_xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])\n",
    "    if xy.shape[1] > 300:\n",
    "        plt.figure(figsize = figsize)\n",
    "        plt.title(dist_name+' xy, First 300 queries')\n",
    "        plt.imshow((xy)[figrange[0]:figrange[1], :300])\n",
    "        plt.title(dist_name+' xy_div_xyind, First 300 queries')\n",
    "        plt.figure(figsize = figsize)\n",
    "        plt.imshow((xy_div_xyind)[figrange[0]:figrange[1], :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anticipated-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 unqiue cards\n",
      "Generated 81 cardpairs\n",
      "-- 45 cardpairs with shared concept\n",
      "-- 36 cardpairs without shared concept\n",
      "Number of cardpairs per shared concept [(1, 36), (0, 36), (2, 9)]\n",
      "Total number of matches =  189\n",
      "Number of matches per key concept hit =  [(1, 144), (0, 36), (2, 9)]\n"
     ]
    }
   ],
   "source": [
    "num_attributes = 2\n",
    "num_attr_vals = 3\n",
    "game_data_full = gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=0, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shape': (10, 81), 'size': 810, 'sparsity': 0.23333333333333334, 'xy_rank': 10, 'xy_div_xyind_rank': 10}\n",
      "Full\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_table, xy, _, xy_div_xyind, distribution = report_gamedata_distribution(game_data_full, distribution_epsilon=0.0)\n",
    "print(distribution)\n",
    "plot_distribution(xy, xy_div_xyind, dist_name='Full Matrix', figsize = (20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-symbol",
   "metadata": {},
   "source": [
    "## Data Generation -- Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gross-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_key_idx(num_attrs, num_attr_vals, card_idx):\n",
    "    '''\n",
    "    decode card idx into attr val idx\n",
    "    '''\n",
    "    assert card_idx < num_attr_vals ** num_attrs\n",
    "    ans = []\n",
    "    if card_idx == num_attr_vals ** num_attrs:\n",
    "        return np.array(ans)\n",
    "    else:\n",
    "        card_idx_copy = card_idx\n",
    "        for i in range(num_attrs):\n",
    "            digit = card_idx_copy % num_attr_vals\n",
    "            ans = [digit] + ans\n",
    "            card_idx_copy = card_idx_copy // num_attr_vals\n",
    "        assert len(ans) == num_attrs\n",
    "        return np.array(ans)\n",
    "\n",
    "def decode_query_idx(num_attrs, num_attr_vals, query_idx):\n",
    "    '''decode query_idx into pair of card indices'''\n",
    "    num_cards = num_attr_vals ** num_attrs\n",
    "    card1_idx = query_idx // num_cards\n",
    "    card2_idx = query_idx % num_cards\n",
    "    return card1_idx, card2_idx\n",
    "\n",
    "def check_if_query_key_match(query_part1, query_part2, key, debug=False):\n",
    "    shared_attr_filter_q1q2 = query_part1 == query_part2\n",
    "    shared_attr_filter_q1k = query_part1 == key\n",
    "    matches = (shared_attr_filter_q1q2 & shared_attr_filter_q1k) # F, F , T\n",
    "    num_matches = np.sum(matches) # 1\n",
    "    if debug: print(query_part1, query_part2, key, num_matches)\n",
    "    return num_matches\n",
    "\n",
    "def queryidx_to_querypair(num_attrs, num_attr_vals, query_idx):  \n",
    "    q1_idx, q2_idx = decode_query_idx(num_attrs, num_attr_vals, query_idx)\n",
    "    query_part1 = decode_key_idx(num_attrs, num_attr_vals, q1_idx)\n",
    "    query_part2 = decode_key_idx(num_attrs, num_attr_vals, q2_idx)\n",
    "    return query_part1, query_part2\n",
    "    \n",
    "def check_if_query_key_match_by_idx(num_attrs, num_attr_vals, query_idx, key_idx):\n",
    "    '''return an int'''\n",
    "    query_part1, query_part2 = queryidx_to_querypair(num_attrs, num_attr_vals, query_idx)\n",
    "    return check_q1q2k_match(num_attrs, num_attr_vals, query_part1, query_part2, key_idx)\n",
    "\n",
    "def check_q1q2k_match(num_attrs, num_attr_vals, query_part1, query_part2, key_idx):\n",
    "    if key_idx == num_attr_vals ** num_attrs: # null key card\n",
    "        if not (query_part1 == query_part2).any():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else: # key card with real values\n",
    "        key = decode_key_idx(num_attrs, num_attr_vals, key_idx)\n",
    "        num_matched_attributes = check_if_query_key_match(query_part1, query_part2, key)\n",
    "        return num_matched_attributes \n",
    "\n",
    "def query_has_real_matches(query_part1, query_part2):\n",
    "    return (query_part1 == query_part2).any()\n",
    "\n",
    "def query_idx_has_real_matches(num_attrs, num_attr_vals, query_idx):\n",
    "    query_part1, query_part2 = queryidx_to_querypair(num_attrs, num_attr_vals, query_idx)\n",
    "    return query_has_real_matches(query_part1, query_part2)\n",
    "    \n",
    "def sample_query_key_idx(num_attrs, num_attr_vals):\n",
    "    num_keys = num_attr_vals ** num_attrs\n",
    "    num_queries = num_keys * num_keys\n",
    "    key_idx = np.random.choice(num_keys + 1)\n",
    "    query_idx = np.random.choice(num_queries)\n",
    "    return query_idx, key_idx\n",
    "    \n",
    "def sample_valid_query_key_idx(num_attrs, num_attr_vals):\n",
    "    valid = False\n",
    "    trials = 0\n",
    "    while not valid:\n",
    "        tmp_query_idx, tmp_key_idx = sample_query_key_idx(num_attrs, num_attr_vals)\n",
    "        valid = check_if_query_key_match_by_idx(num_attrs, num_attr_vals, tmp_query_idx, tmp_key_idx)\n",
    "        if valid: valids = (tmp_query_idx, tmp_key_idx)\n",
    "        trials += 1\n",
    "    return trials, valids\n",
    "\n",
    "def sample_N_datapoints(num_attrs, num_attr_vals, N):\n",
    "    all_valids = []\n",
    "    cts = 0\n",
    "    for i in range(N):\n",
    "        trials, valids = sample_valid_query_key_idx(num_attrs, num_attr_vals)\n",
    "        cts += trials\n",
    "        all_valids.append(valids)\n",
    "    print('Sparsity Estimate:', 1/(cts / N))\n",
    "    return all_valids\n",
    "    \n",
    "def test_sampling():\n",
    "\n",
    "    for i in range(10000):\n",
    "        q, c = sample_query_key_idx(num_attrs=4, num_attr_vals=3)\n",
    "        assert c < 81 and q < 3240\n",
    "\n",
    "    sample_N_datapoints(num_attrs, num_attr_vals, N) # should be around 0.184471\n",
    "\n",
    "    \n",
    "def sample_dataset(num_attrs, num_attr_vals, N_train, N_val):\n",
    "    \n",
    "    N = N_train + N_val\n",
    "    datapoints = sample_N_datapoints(num_attrs, num_attr_vals, N)\n",
    "        \n",
    "    data = {\n",
    "        'num_attributes':num_attrs,\n",
    "        'num_attr_vals':num_attr_vals,\n",
    "        'key_support_size': num_attr_vals**num_attrs + 1,\n",
    "        'query_support_size': (num_attr_vals**num_attrs)**2,\n",
    "        'train_datapoints': datapoints[:N_train],\n",
    "        'val_datapoints': datapoints[N_train:N_train+N_val]\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "better-sailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Estimate: 0.2338733682017264\n"
     ]
    }
   ],
   "source": [
    "game_data_sampled = sample_dataset(num_attributes, num_attr_vals, N_train=10000, N_val=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-understanding",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passing-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 unqiue cards\n",
      "Generated 81 cardpairs\n",
      "-- 45 cardpairs with shared concept\n",
      "-- 36 cardpairs without shared concept\n",
      "Number of cardpairs per shared concept [(1, 36), (0, 36), (2, 9)]\n",
      "Total number of matches =  189\n",
      "Number of matches per key concept hit =  [(1, 144), (0, 36), (2, 9)]\n",
      "------------------------------------------------------------------------\n",
      "query\n",
      " 50 : 5 5 ('green', 'solid') ('green', 'solid')\n",
      "key\n",
      " 0 ('red', 'void')\n",
      "all matches \n",
      " [('red', 'solid'), ('green', 'void'), ('green', 'dashed'), ('green', 'solid'), ('blue', 'solid')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, tensor([50]), tensor([0]), tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameDatasetFromFullMatrix():\n",
    "    '''Directly sample from full distribution matrix'''\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by gen_card_data.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        self.num_attrs = self.raw_data['num_attributes']\n",
    "        self.num_attr_vals = self.raw_data['num_attr_vals']\n",
    "        self.query_support_size = self.raw_data['query_support_size'] # y\n",
    "        self.key_support_size = self.raw_data['key_support_size'] # x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.query_support_size * self.key_support_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: (xy_i) * (xy.shape[1]=self.query_support_size) + (xy_j)\n",
    "        '''\n",
    "        x_i, y_j = idx//self.query_support_size, idx%self.query_support_size\n",
    "        all_matches = list(self.raw_data['query_to_keys'].get(y_j, {}).keys())\n",
    "        gt = np.zeros(self.key_support_size)\n",
    "        gt[all_matches] = 1.0\n",
    "        \n",
    "        if self.debug:\n",
    "            yj1, yj2 = self.raw_data['idx_to_query'][y_j]\n",
    "            print('query\\n', y_j,\":\", yj1, yj2, self.raw_data['idx_to_key'][yj1], self.raw_data['idx_to_key'][yj2])\n",
    "            print('key\\n', x_i, self.raw_data['idx_to_key'][x_i])\n",
    "            print('all matches \\n', [self.raw_data['idx_to_key'][m] for m in all_matches])\n",
    "        \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        )    \n",
    "\n",
    "num_attributes = 2\n",
    "num_attr_vals = 3\n",
    "game_data_full = gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=0, debug=False)\n",
    "print('------------------------------------------------------------------------')\n",
    "game_dataset = GameDatasetFromFullMatrix(raw_data=game_data_full, debug=True)\n",
    "game_dataset[50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "awful-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDatasetFromDataPoints(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by sample_dataset()\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        self.num_attrs = self.raw_data['num_attributes']\n",
    "        self.num_attr_vals = self.raw_data['num_attr_vals']\n",
    "        self.query_support_size = self.raw_data['query_support_size']\n",
    "        self.key_support_size = self.raw_data['key_support_size']\n",
    "        \n",
    "    def compute_gt(self, query_idx):\n",
    "        q1_idx, q2_idx = decode_query_idx(self.num_attrs, self.num_attr_vals, query_idx)\n",
    "        query1 = decode_key_idx(self.num_attrs, self.num_attr_vals, q1_idx)\n",
    "        query2 = decode_key_idx(self.num_attrs, self.num_attr_vals, q2_idx)\n",
    "        \n",
    "        gt = [\n",
    "            float(check_q1q2k_match(self.num_attrs, self.num_attr_vals, query1, query2, k_idx) > 0) \\\n",
    "            for k_idx in range(self.key_support_size)\n",
    "        ]\n",
    "        return np.array(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "leading-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Estimate: 0.23733494433416763\n",
      "------------------------------------------------------------------------\n",
      "query\n",
      " 18 : 2 0 [0 2] [0 0]\n",
      "key\n",
      " 2 [0 2]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, tensor([18]), tensor([2]), tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameDatasetTrainDataset(GameDatasetFromDataPoints):\n",
    "    '''Sample from Presampled Datapoints. Better for Sparse distribution matrix.'''\n",
    "    \n",
    "    def __init__(self, raw_data, split, debug=False):\n",
    "        assert split in ('train', 'val')\n",
    "        super().__init__(raw_data, debug)\n",
    "        self.split = split\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data[self.split + '_datapoints'])\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx: int.\n",
    "        '''\n",
    "        # query, key\n",
    "        y_j, x_i = self.raw_data[self.split + '_datapoints'][idx]\n",
    "        gt = self.compute_gt(y_j)\n",
    "           \n",
    "        if self.debug:\n",
    "            yj1, yj2 = decode_query_idx(self.num_attrs, self.num_attr_vals, y_j)\n",
    "            print(\n",
    "                'query\\n', y_j,\":\", yj1, yj2, \n",
    "                decode_key_idx(self.num_attrs, self.num_attr_vals, yj1), \n",
    "                decode_key_idx(self.num_attrs, self.num_attr_vals, yj2)\n",
    "            )\n",
    "            print('key\\n', x_i, decode_key_idx(self.num_attrs, self.num_attr_vals, x_i))\n",
    "            print('all matches \\n', [decode_key_idx(self.num_attrs, self.num_attr_vals, i) for i,v in enumerate(gt) if v == 1.])\n",
    "                  \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        )   \n",
    "                 \n",
    "num_attributes = 2\n",
    "num_attr_vals = 3\n",
    "game_data_sampled = sample_dataset(num_attributes, num_attr_vals, N_train=10000, N_val=1000)\n",
    "print('------------------------------------------------------------------------')\n",
    "game_dataset_sampled = GameDatasetTrainDataset(raw_data=game_data_sampled, split='train', debug=True)\n",
    "game_dataset_sampled[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "through-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "query\n",
      " 10 : 1 1 [0 1] [0 1]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2]), array([1, 1]), array([2, 1])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, tensor([10]), tensor([0]), tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameTestFullDataset(GameDatasetFromDataPoints):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        super().__init__(raw_data, debug)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.raw_data['query_support_size']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: int. 0 to query_support_size-1\n",
    "        '''\n",
    "        y_j = idx\n",
    "        x_i = torch.empty(1) # just a meaningless value\n",
    "        gt = self.compute_gt(y_j)\n",
    "\n",
    "        if self.debug:\n",
    "            yj1, yj2 = decode_query_idx(self.num_attrs, self.num_attr_vals, y_j)\n",
    "            print(\n",
    "                'query\\n', y_j,\":\", yj1, yj2, \n",
    "                decode_key_idx(self.num_attrs, self.num_attr_vals, yj1), \n",
    "                decode_key_idx(self.num_attrs, self.num_attr_vals, yj2)\n",
    "            )\n",
    "            print('all matches \\n', [decode_key_idx(self.num_attrs, self.num_attr_vals, i) for i,v in enumerate(gt) if v == 1.])\n",
    "                  \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        )\n",
    "    \n",
    "print('------------------------------------------------------------------------')\n",
    "game_dataset_sampled = GameTestFullDataset(raw_data=game_data_sampled, debug=True)\n",
    "game_dataset_sampled[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "natural-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size, raw_data, debug=False):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = GameDatasetTrainDataset(raw_data=raw_data, split='train', debug=debug)\n",
    "        self.val_dataset = GameDatasetTrainDataset(raw_data=raw_data, split='val', debug=debug)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train = self.train_dataset\n",
    "            self.val = self.val_dataset\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val, batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-banking",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "better-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_full_model(hparams):\n",
    "    '''\n",
    "    return: nn.Module.\n",
    "    '''\n",
    "    # embeddings\n",
    "    query_embed_X = ScaledEmbedding(hparams['query_support_size'], hparams['d_model'])\n",
    "    key_embed_X = ScaledEmbedding(hparams['key_support_size'], hparams['d_model'])\n",
    "    embed_dropout = nn.Dropout(hparams['embed_dropout'])\n",
    "    \n",
    "    # full model\n",
    "    model = EncoderPredictor(\n",
    "        inp_query_layer = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('scaled_embed', query_embed_X),\n",
    "                ('embed_dropout', embed_dropout)\n",
    "            ])\n",
    "        ),\n",
    "        inp_key_layer = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('scaled_embed', key_embed_X),\n",
    "                ('embed_dropout', embed_dropout)\n",
    "            ])\n",
    "        ),\n",
    "        classifier = nn.Sequential(\n",
    "            OrderedDict([   \n",
    "                ('linear1', nn.Linear(2*hparams['d_model'], hparams['d_model'])),\n",
    "                ('nonLinear1', nn.ReLU()),\n",
    "                ('linear-out', nn.Linear(hparams['d_model'], 1)),\n",
    "            ])\n",
    "        ) if not hparams['dotproduct_bottleneck'] else None, \n",
    "        \n",
    "        key_support_size = hparams['key_support_size'],\n",
    "        d_model = hparams['d_model'],\n",
    "        debug = hparams['debug'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "homeless-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, V, d_model):\n",
    "        super(ScaledEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(V, d_model)\n",
    "        # scale embedding to have variance 0.01\n",
    "        nn.init.normal_(self.embedding.weight, mean=0., std=(0.01)**(1/2))\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        '''\n",
    "        tokens: shape (batch_size=b, len)\n",
    "        '''\n",
    "        # shape (b, len, d_model)\n",
    "        embedded = self.embedding(tokens)\n",
    "        if torch.max(embedded) > 2000.:\n",
    "            import pdb; pdb.set_trace()\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faced-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_query_layer, inp_key_layer, classifier, key_support_size, d_model, debug=False):\n",
    "        super().__init__()\n",
    "        self.inp_query_layer = inp_query_layer\n",
    "        self.inp_key_layer = inp_key_layer\n",
    "        self.classifier = classifier\n",
    "        self.key_support_size = key_support_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, X_query, X_key, val_bool, debug=False):\n",
    "        '''\n",
    "        X_query: (b, 1)\n",
    "        X_key: (b, 1) if test bool, else (b, num matched cards) or None.\n",
    "        '''\n",
    "        if X_key is not None: assert X_query.shape == X_key.shape\n",
    "        if val_bool:\n",
    "            return self.forward_norm_support(X_query, debug=debug)\n",
    "        else:\n",
    "            assert X_key is not None, 'X_key should not be None for normalizing over minibatch keys.'\n",
    "            return self.forward_norm_minibatch(X_query, X_key, debug=debug)\n",
    "\n",
    "    def forward_norm_minibatch(self, X_query, X_key, debug=False):\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        query_repr = self.encode_query(X_query).squeeze(1)\n",
    "        assert query_repr.shape == (b, self.d_model)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        key_repr = self.encode_key(X_key).squeeze(1)\n",
    "        assert key_repr.shape == (b, self.d_model)\n",
    "\n",
    "        if self.classifier:\n",
    "            raise ValueError('Not supposed to use classifier!')\n",
    "            # shape(b, b, d_model)\n",
    "            query_repr_tiled = query_repr.unsqueeze(1).expand(b, b, self.d_model)\n",
    "            # shape(b, b, d_model)\n",
    "            key_repr_tiled = key_repr.unsqueeze(0).expand(b, b, self.d_model)\n",
    "            # shape(b, b, 2*d_model)\n",
    "            query_key_concat = torch.cat([query_repr_tiled, key_repr_tiled], dim=2)\n",
    "            assert query_key_concat.shape == (b, b, 2*self.d_model)\n",
    "            # shape(b*b, 2*d_model)\n",
    "            query_key_concat = query_key_concat.reshape(b*b, 2*self.d_model)\n",
    "            # shape(b*b, 1)\n",
    "            logits = self.classifier(query_key_concat)\n",
    "            assert logits.shape == (b*b, 1)\n",
    "            # shape(b, b)\n",
    "            logits = logits.squeeze(1).reshape(b, b)\n",
    "        else:\n",
    "            # shape(b, b) dotproduct=logit matrix\n",
    "            logits = torch.matmul(query_repr, key_repr.T)\n",
    "        assert logits.shape == (b, b)\n",
    "        \n",
    "        # shape(b, b)\n",
    "        return logits\n",
    "\n",
    "    def forward_norm_support(self, X_query, debug=False):\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        query_repr = self.encode_query(X_query).squeeze(1)\n",
    "        assert query_repr.shape == (b, self.d_model)\n",
    "\n",
    "        # shape(size(support), d_model)\n",
    "        keys_repr = self.encode_all_keys()\n",
    "        assert keys_repr.shape == (self.key_support_size, self.d_model)\n",
    "        \n",
    "        if self.classifier:\n",
    "            # shape(b, size(support), d_model)\n",
    "            query_repr_tiled = query_repr.unsqueeze(1).expand(b, self.key_support_size, self.d_model)\n",
    "            # shape(b, size(support), d_model)\n",
    "            key_repr_tiled = keys_repr.unsqueeze(0).expand(b, self.key_support_size, self.d_model)\n",
    "            # shape(b, size(support), 2*d_model)\n",
    "            query_key_concat = torch.cat([query_repr_tiled, key_repr_tiled], dim=2)\n",
    "            # shape(b*size(support), 2*d_model)\n",
    "            query_key_concat = query_key_concat.reshape(b*self.key_support_size, 2*self.d_model)\n",
    "            # shape(b*size(support), 1)\n",
    "            logits = self.classifier(query_key_concat)\n",
    "            # shape(b, size(support))\n",
    "            logits = logits.squeeze(1).reshape(b, self.key_support_size)\n",
    "        else:\n",
    "            # shape(b, size(support)) dotproduct=logit matrix\n",
    "            logits = torch.matmul(query_repr, keys_repr.T)\n",
    "        assert logits.shape == (b, self.key_support_size)\n",
    "        \n",
    "        # shape(b, size(support)) \n",
    "        return logits\n",
    "\n",
    "    def encode_query(self, X):\n",
    "        '''\n",
    "        X: (batch_size=b,1)\n",
    "        '''\n",
    "        b = X.shape[0] \n",
    "        # shape(b, 1, embed_dim)\n",
    "        inp_embed = self.inp_query_layer(X)\n",
    "        assert inp_embed.shape == (b, 1, self.d_model)\n",
    "        return inp_embed     \n",
    "        \n",
    "    def encode_key(self, X):\n",
    "        '''\n",
    "        X: (batch_size=b)\n",
    "        '''\n",
    "        b = X.shape[0] \n",
    "        # shape(b, 1, embed_dim)\n",
    "        inp_embed = self.inp_key_layer(X) \n",
    "        assert inp_embed.shape == (b, 1, self.d_model)\n",
    "        return inp_embed  \n",
    "\n",
    "    def encode_all_keys(self):\n",
    "        \n",
    "        # shape(size(support), embed_dim)\n",
    "        all_embed = self.inp_key_layer.scaled_embed.embedding.weight\n",
    "        assert all_embed.requires_grad == True\n",
    "        assert all_embed.shape == (self.key_support_size, self.d_model)\n",
    "        \n",
    "        return all_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-power",
   "metadata": {},
   "source": [
    "# Loss, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "flexible-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_loss_per_row= tensor(20.0001)\n",
      "sum_loss_per_col= tensor(60.0001)\n",
      "avg loss= tensor(40.0001)\n",
      "-------------\n",
      "sum_loss_per_row= tensor(0.6933)\n",
      "sum_loss_per_col= tensor(40.0001)\n",
      "avg loss= tensor(20.3467)\n"
     ]
    }
   ],
   "source": [
    "class InfoCELoss(nn.Module):\n",
    "    '''\n",
    "    InfoCE Loss on a (b, b) logits matrix with Temperature scaling\n",
    "    '''\n",
    "\n",
    "    def __init__(self, temperature_const=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature_const = temperature_const\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, logits, debug=False):\n",
    "        '''\n",
    "        logits: shape (batch_size=b, b)\n",
    "        '''\n",
    "        assert logits.shape[0] == logits.shape[1]\n",
    "        b = logits.shape[0]\n",
    "        \n",
    "        logits /= self.temperature_const\n",
    "        \n",
    "        labels = torch.arange(b).type_as(logits).long()\n",
    "        sum_loss_per_row = self.CE_loss(logits, labels)\n",
    "        sum_loss_per_col = self.CE_loss(logits.T, labels)\n",
    "        \n",
    "        if debug:\n",
    "            print('sum_loss_per_row=',sum_loss_per_row)\n",
    "            print('sum_loss_per_col=',sum_loss_per_col)\n",
    "\n",
    "        loss = (sum_loss_per_row + sum_loss_per_col) * 0.5\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ---------------------------    \n",
    "loss_criterion = InfoCELoss(temperature_const=0.1)\n",
    "\n",
    "logits = torch.tensor([\n",
    "    [1.,2.,3.],\n",
    "    [6.,7.,4.],\n",
    "    [5.,8.,9.]\n",
    "])\n",
    "print('avg loss=',loss_criterion(logits, True))\n",
    "print('-------------')\n",
    "logits = torch.tensor([\n",
    "    [3.,2.,3.],\n",
    "    [6.,7.,4.],\n",
    "    [5.,8.,9.]\n",
    "])\n",
    "print('avg loss=',loss_criterion(logits, True))\n",
    "\n",
    "\n",
    "# ref\n",
    "# sum_loss_per_row= tensor(20.0001)\n",
    "# sum_loss_per_col= tensor(60.0001)\n",
    "# avg loss= tensor(40.0001)\n",
    "# -------------\n",
    "# sum_loss_per_row= tensor(0.6933)\n",
    "# sum_loss_per_col= tensor(40.0001)\n",
    "# avg loss= tensor(20.3467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "irish-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdedMetrics(nn.Module):\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        '''\n",
    "        tot_k: total number of candidates. e.g. 81 cards\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.num_attrs = self.raw_data['num_attributes']\n",
    "        self.num_attr_vals = self.raw_data['num_attr_vals']\n",
    "        self.key_support_size = self.raw_data['key_support_size']\n",
    "        self.threshold = 1.0 / (self.key_support_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def breakdown_errors(self, X_query, corrects):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        corrects: shape (b, support size)\n",
    "        '''\n",
    "        b = X_query.shape[0]\n",
    "        assert corrects.shape == (b, self.key_support_size)\n",
    "        \n",
    "        X_query_list = X_query.squeeze(-1)\n",
    "        wrongs = (1 - corrects).cpu().numpy()\n",
    "        \n",
    "        num_matched_concepts = [\n",
    "            check_if_query_key_match_by_idx(\n",
    "                self.num_attrs, self.num_attr_vals, X_query_list[batch_i].item(), key_idx\n",
    "            ) for batch_i in range(b) for key_idx in range(self.key_support_size)  \n",
    "        ]\n",
    "        \n",
    "        num_matched_concepts = np.array(num_matched_concepts).reshape(b, self.key_support_size)\n",
    "        assert num_matched_concepts.shape == wrongs.shape\n",
    "        \n",
    "        wrongs_mask = wrongs.reshape(-1).tolist()\n",
    "        num_matched_concepts = num_matched_concepts.reshape(-1).tolist()\n",
    "\n",
    "        error_count_by_num_matched_concepts = {k:0 for k in range(self.num_attrs+1)}\n",
    "        total_count_by_num_matched_concepts = {k:0 for k in range(self.num_attrs+1)}\n",
    "        for w,k in zip(wrongs_mask, num_matched_concepts):\n",
    "            if w == 1:\n",
    "                error_count_by_num_matched_concepts[k] += 1\n",
    "            total_count_by_num_matched_concepts[k] += 1    \n",
    "\n",
    "        error_counts, total_counts = {}, {}\n",
    "        for k in range(num_attributes):\n",
    "            err_ct = error_count_by_num_matched_concepts[k]\n",
    "            tot_ct = total_count_by_num_matched_concepts[k]\n",
    "            error_counts[f'error_rate_for_{k}_matched_concepts'] = 0 if tot_ct == 0 else (err_ct *1.0 / tot_ct)\n",
    "            total_counts[f'total_count_for_{k}_matched_concepts'] = tot_ct\n",
    "\n",
    "        return {**error_counts, **total_counts}    \n",
    "    \n",
    "    def forward(self, X_query, logits, X_keys, full_test_bool=False, breakdown_errors_bool=False, debug=False):\n",
    "        \n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        b, key_support_size = logits.shape\n",
    "        assert key_support_size == self.key_support_size\n",
    "        assert logits.shape == X_keys.shape\n",
    "        \n",
    "        if full_test_bool:\n",
    " \n",
    "            # filter down to query cards with >0 true matches\n",
    "            X_query_flat = X_query.squeeze(-1)\n",
    "            # NOTE: Not great\n",
    "            fil = torch.tensor(\n",
    "                [query_idx_has_real_matches(self.num_attrs, self.num_attr_vals, qId.item()) \\\n",
    "                 for qId in X_query_flat\n",
    "                ]).type_as(X_query).type(torch.bool)            \n",
    "            assert fil.shape == (b, )\n",
    "            \n",
    "            # queries with shared attributes\n",
    "            fil_metrics = self.compute_metrics(X_query[fil], logits[fil], X_keys[fil], breakdown_errors_bool, debug)\n",
    "            fil_metrics = {'nonNullQueries_'+k:fil_metrics[k] for k in fil_metrics}\n",
    "            \n",
    "            # queries without shared attributes\n",
    "            not_fil = torch.logical_not(fil)\n",
    "            not_fil_metrics = self.compute_metrics(X_query[not_fil], logits[not_fil], X_keys[not_fil], breakdown_errors_bool, debug)\n",
    "            not_fil_metrics = {'NullQueries_'+k:not_fil_metrics[k] for k in not_fil_metrics}\n",
    "            \n",
    "            # all queries\n",
    "            all_metrics = self.compute_metrics(X_query, logits, X_keys, breakdown_errors_bool, debug)\n",
    "            \n",
    "            return {**fil_metrics, **not_fil_metrics, **all_metrics}\n",
    "        else:\n",
    "            return self.compute_metrics(X_query, logits, X_keys, breakdown_errors_bool, debug)\n",
    "    \n",
    "    def compute_metrics(self, X_query, logits, X_keys, breakdown_errors_bool=False, debug=False):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        logits: shape (b, support size)\n",
    "        X_keys: shape (b, support size). value 1.0 at where card matches. value 0 otherwise.\n",
    "        '''\n",
    "        b, key_support_size = logits.shape\n",
    "        \n",
    "        # model predictions, shape (b, support size)\n",
    "        binary_predictions = (self.softmax(logits) >= self.threshold).type(torch.float)\n",
    "        # ground truth, shape (b, support size)\n",
    "        gt = X_keys\n",
    "        # correct predictions, shape (b, support size)\n",
    "        corrects = (binary_predictions == gt).type(torch.float)\n",
    "        \n",
    "        # accuracy, computed per query, average across queries\n",
    "        # (b,)\n",
    "        accuracy_row = torch.sum(corrects, dim=1) / key_support_size\n",
    "        # scalar\n",
    "        accuracy_meanrows = torch.mean(accuracy_row)\n",
    "        # accuracy, computed per query-key, average across all\n",
    "        accuracy_all = torch.sum(corrects) / (b * key_support_size)\n",
    "        \n",
    "        # precision, computed per query, average across queries\n",
    "        # (b,)\n",
    "        precision_row = torch.sum((corrects * binary_predictions), dim=1) / torch.sum(binary_predictions, dim=1)\n",
    "        # scalar\n",
    "        precision_meanrows = torch.mean(precision_row)\n",
    "        # precision, computed per query-key, average across all\n",
    "        precision_all = torch.sum((corrects * binary_predictions)) / torch.sum(binary_predictions)\n",
    "\n",
    "        # recall, computed per query, average across queries\n",
    "        # (b,)\n",
    "        recall_row = torch.sum((corrects * gt), dim=1) / torch.sum(gt, dim=1)\n",
    "        # scalar\n",
    "        recall_meanrows = torch.mean(recall_row)\n",
    "        # recall, computed per query-key, average across all\n",
    "        recall_all = torch.sum((corrects * gt)) / torch.sum(gt)\n",
    "        \n",
    "        # f1, computed per query, average across queries\n",
    "        # (b,)\n",
    "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row)\n",
    "        # scalar\n",
    "        f1_meanrows = torch.mean(f1_row)\n",
    "        # f1, computed per query-key, average across all\n",
    "        f1_all = 2 * (precision_all * recall_all) / (precision_all + recall_all)\n",
    "        \n",
    "        if breakdown_errors_bool:\n",
    "            error_breakdown_by_num_matched_concepts = self.breakdown_errors(X_query, corrects)\n",
    "        else:\n",
    "            error_breakdown_by_num_matched_concepts = {} \n",
    "            \n",
    "        if debug:\n",
    "            print('####################################################')\n",
    "            print('Metrics Per Query:')\n",
    "            print('accuracy_rows', accuracy_row)\n",
    "            print('precision_row', precision_row)\n",
    "            print('recall_row', recall_row)\n",
    "            print('f1_row', f1_row)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across Queries')\n",
    "            print('accuracy_meanrows', accuracy_meanrows)\n",
    "            print('precision_meanrows', precision_meanrows)\n",
    "            print('recall_meanrows', recall_meanrows)\n",
    "            print('f1_meanrows', f1_meanrows)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across All Query-Key Pairs:')\n",
    "            print('accuracy_all', accuracy_all)\n",
    "            print('precision_all', precision_all)\n",
    "            print('recall_all', recall_all)\n",
    "            print('f1_all', f1_all)\n",
    "            print('####################################################')\n",
    "            print('error_breakdown', error_breakdown_by_num_matched_concepts)\n",
    "            \n",
    "        metrics = {\n",
    "            'accuracy_by_Query': accuracy_meanrows,\n",
    "            'precision_by_Query': precision_meanrows,\n",
    "            'recall_by_Query': recall_meanrows,\n",
    "            'f1_by_Query': f1_meanrows,\n",
    "            'accuracy_by_QueryKey': accuracy_all,\n",
    "            'precision_by_QueryKey': precision_all,\n",
    "            'recall_by_QueryKey': recall_all,\n",
    "            'f1_by_QueryKey': f1_all\n",
    "        }\n",
    "        metrics = {\n",
    "            **metrics, **error_breakdown_by_num_matched_concepts\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def test_metric_module():\n",
    "    key_support_size=3\n",
    "    thresh=1./key_support_size\n",
    "\n",
    "    m_f = ThresholdedMetrics(raw_data=game_data)\n",
    "    m_f.key_support_size = 3\n",
    "\n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [0.4, 0.2, 0.4],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    \n",
    "    \n",
    "# test_metric_module()\n",
    "\n",
    "# reference output\n",
    "# accuracy_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.5000, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.5556), 'precision': tensor(0.5556), 'recall': tensor(1.), 'f1': tensor(0.6667)}\n",
    "# ----------------------------------------------\n",
    "# accuracy_row tensor([0.6667, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.5000, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.6667, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.6667), 'precision': tensor(0.6111), 'recall': tensor(1.), 'f1': tensor(0.7222)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-green",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "focal-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lightning Module\n",
    "# https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=UIXLW8CO-W8w\n",
    "\n",
    "class TrainModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams, gt_distributions={}):\n",
    "        '''\n",
    "        hparams: dictionary of hyperparams\n",
    "        gt_distributions: dictionary that stores the groundtruth 'xy', 'xyind' distributions.\n",
    "                         each is a key_support_size by query_support_size matrix that sums up to 1.0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.debug = hparams['debug']\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = construct_full_model(hparams)\n",
    "        self.loss_criterion = InfoCELoss(temperature_const=self.hparams['loss_temperature_const'])\n",
    "        self.metrics = thresholded_metrics\n",
    "\n",
    "        self.key_support_size = self.hparams['key_support_size']\n",
    "        self.query_support_size = self.hparams['query_support_size']\n",
    "        \n",
    "        # for pulling model p(x,y) and p(x,y)/[pxpy]\n",
    "        self.populate_logits_matrix = hparams['populate_logits_matrix']\n",
    "        if self.populate_logits_matrix:\n",
    "            assert gt_distributions\n",
    "            self.register_buffer(\n",
    "                name='model_logits_matrix',\n",
    "                tensor= torch.zeros(hparams['key_support_size'], hparams['query_support_size'])\n",
    "            )\n",
    "            self.setup_gt_distributions(gt_distributions)\n",
    "        \n",
    "    def log_metrics(self, metrics_dict):\n",
    "        for k, v in metrics_dict.items():\n",
    "            self.log(k, v)\n",
    "            \n",
    "    def get_max_memory_alloc(self):\n",
    "        devices_max_memory_alloc = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.device(f'cuda:{i}')\n",
    "            devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            torch.cuda.reset_max_memory_allocated(device)\n",
    "        return devices_max_memory_alloc\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def forward(self, X_query, X_key, X_keys, val_bool, full_test_bool=False, debug=False):\n",
    "        '''\n",
    "        X_query: (b, 1)\n",
    "        X_key: (b, 1)\n",
    "        X_keys: (b, key_support_size) 1s and 0s.\n",
    "        test_bool: boolean.\n",
    "        '''\n",
    "        batch_size = X_query.shape[0]\n",
    "        \n",
    "        # ToDo batch_size_b\n",
    "        \n",
    "        # shape (b,support) if test_bool else (b, b)\n",
    "        logits = self.model(X_query, X_key, val_bool, debug=debug)\n",
    "        # scalar\n",
    "        loss = None if val_bool else self.loss_criterion(logits, debug=debug)\n",
    "        # scalar\n",
    "        metrics = self.metrics(\n",
    "            logits=logits, X_keys=X_keys, X_query=X_query, \n",
    "            debug=debug, full_test_bool=full_test_bool, breakdown_errors_bool=True, \n",
    "        ) if val_bool else None\n",
    "\n",
    "        return logits, loss, metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def setup_gt_distributions(self, gt_distributions):\n",
    "        '''called once during init to setup groundtruth distributions'''\n",
    "        assert gt_distributions['xy'].shape == gt_distributions['xyind'].shape\n",
    "        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy',\n",
    "            tensor= torch.tensor(gt_distributions['xy'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xyind',\n",
    "            tensor= torch.tensor(gt_distributions['xyind'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy_div_xyind',\n",
    "            tensor= self.gt_xy/self.gt_xyind\n",
    "        )\n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='one',\n",
    "            tensor= torch.tensor([1.0])\n",
    "        )   \n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='gt_mi',\n",
    "            tensor= self.compute_mutual_information(self.gt_xy, self.gt_xy_div_xyind)\n",
    "        ) \n",
    "   \n",
    "    \n",
    "    def populate_model_logits_matrix(self, query_idx, logits):\n",
    "        '''\n",
    "        query_idx: shape (b,)\n",
    "        logits: shape(b, key_support_size)\n",
    "        '''  \n",
    "        assert query_idx.shape[0] == logits.shape[0]\n",
    "        b = query_idx.shape[0]\n",
    "        assert logits.shape[1] == self.key_support_size\n",
    "        for i in range(b):\n",
    "            self.model_logits_matrix[:,query_idx[i]] = logits[i]\n",
    "    \n",
    "    def compute_mutual_information(self, xy, xy_div_xyind):\n",
    "        '''\n",
    "        xy: p(xy). shape(b, key_support_size)\n",
    "        xy_div_xyind_hat: p(xy)/[p(x)(y)].\n",
    "                          shape(b, key_support_size)\n",
    "        '''\n",
    "        assert torch.isclose(torch.sum(xy), self.one.type_as(xy))\n",
    "        assert xy.shape == xy_div_xyind.shape == (\n",
    "            self.key_support_size, self.query_support_size\n",
    "        )\n",
    "        pmi = torch.log(xy_div_xyind)\n",
    "        # turn -inf values into zeros, ok because p(x,y) is also zero for those values\n",
    "        pmi_notInf_mask = torch.isinf(pmi)\n",
    "        pmi[pmi_notInf_mask] = 0.0\n",
    "        mi = torch.sum(xy * pmi)\n",
    "        return mi\n",
    "    \n",
    "    def pull_model_distribution(self, debug=True):\n",
    "\n",
    "        # sanity check\n",
    "        sum_logits = torch.sum(self.model_logits_matrix)\n",
    "        assert sum_logits != 0.0\n",
    "        \n",
    "        if debug:\n",
    "            print('Sum of model logits matrix\\n', sum_logits)\n",
    "            print('Number of model logits with zero value\\n', torch.sum(self.model_logits_matrix == 0.0)) \n",
    "            print('Variance of model logits\\n', torch.var(self.model_logits_matrix))\n",
    "        \n",
    "        # estimate the full distribution\n",
    "        # hat( k * pxy/(pxpy)\n",
    "        f = torch.exp(self.model_logits_matrix)\n",
    "        # hat( k * pxy)\n",
    "        xy_hat = f * self.gt_xyind\n",
    "        # hat( pxy)\n",
    "        xy_hat = (xy_hat / torch.sum(xy_hat))\n",
    "        \n",
    "        # estimate exp(pmi)\n",
    "        # hat(k)\n",
    "        k_hat = torch.sum(f) / torch.sum(self.gt_xy_div_xyind)\n",
    "        # hat(pxy/(pxpy)\n",
    "        xy_div_xyind_hat = (f / k_hat)\n",
    "        if torch.any(torch.isnan(xy_div_xyind_hat)):\n",
    "            import pdb; pdb.set_trace()\n",
    "        \n",
    "        # estimate MI\n",
    "        # scalar\n",
    "        mi_hat = self.compute_mutual_information(xy_hat, xy_div_xyind_hat)\n",
    "        \n",
    "        # estimate KL divergence\n",
    "        kl_div_val = F.kl_div(torch.log(xy_hat), self.gt_xy)\n",
    "\n",
    "        # estimate ranks\n",
    "        xy_hat = xy_hat.detach().cpu().numpy()\n",
    "        xy_div_xyind_hat = xy_div_xyind_hat.detach().cpu().numpy()\n",
    "        # hat(pxy rank)\n",
    "        xy_hat_rank = np.linalg.matrix_rank(xy_hat)\n",
    "        # hat(pxy/(pxpy rank)\n",
    "        xy_div_xyind_hat_rank = np.linalg.matrix_rank(xy_div_xyind_hat) \n",
    "        \n",
    "        print('SELF GT MI', self.gt_mi)\n",
    "        pulled_distribution_results = {\n",
    "            'xy_hat':xy_hat,\n",
    "            'xy_div_xyind_hat':xy_div_xyind_hat,\n",
    "            'xy_hat_rank':xy_hat_rank,\n",
    "            'xy_div_xyind_hat_rank':xy_div_xyind_hat_rank,\n",
    "            'mi_gt':self.gt_mi,\n",
    "            'mi_hat':mi_hat,\n",
    "            'kl_div':kl_div_val\n",
    "        }\n",
    "        \n",
    "        return pulled_distribution_results\n",
    "\n",
    "    ###################################################\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        # scalar\n",
    "        _, loss, _ = self(X_query, X_key, None, val_bool=False, debug=self.debug)\n",
    "        # dict\n",
    "        _, _, metrics = self(X_query, None, X_keys, val_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('train step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\nX_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:\\n', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {**{'train_loss': loss}, **{'train_'+m:metrics[m] for m in metrics}}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        _, loss, _ = self(X_query, X_key, None, val_bool=False, debug=self.debug)\n",
    "        _, _, metrics = self(X_query, None, X_keys, val_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('validation step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\X_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "            \n",
    "        # log \n",
    "        step_metrics = {**{'val_loss': loss}, **{'val_'+m:metrics[m] for m in metrics}}\n",
    "        devices_max_memory_alloc = self.get_max_memory_alloc()\n",
    "        for device, val in devices_max_memory_alloc.items():\n",
    "            step_metrics[f'step_max_memory_alloc_cuda:{device}'] = val\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        # (b,1), (b,1), _, (b, support size)\n",
    "        query_idx, X_query, _, X_keys = batch\n",
    "        \n",
    "        # compute scores for all keys\n",
    "        # shape(b, key_support_size), _, dictionary\n",
    "        logits, _, metrics = self(X_query, None, X_keys, val_bool=True, full_test_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.populate_logits_matrix:\n",
    "            self.populate_model_logits_matrix(query_idx, logits)\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {'test_'+m:metrics[m] for m in metrics}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics \n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def aggregate_metrics_at_epoch_end(self, outputs):\n",
    "        # log metrics\n",
    "        epoch_metrics = {}\n",
    "        metric_names = outputs[0].keys()\n",
    "        for m in metric_names:\n",
    "            if not ('max_memory_alloc_cuda' in m or 'count' in m or 'rate' in m):\n",
    "                epoch_metrics['avg_'+m] = torch.stack([x[m] for x in outputs]).mean()\n",
    "            elif '_matched_concepts' in m:\n",
    "                epoch_metrics['avg_'+m] = np.mean([x[m] for x in outputs])\n",
    "        self.log_metrics(epoch_metrics)\n",
    "        return epoch_metrics         \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        return averaged_metrics\n",
    "    \n",
    "    def test_epoch_end(self, outputs):        \n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        assert 'avg_test_error_rate_for_1_matched_concepts' in averaged_metrics\n",
    "            \n",
    "        return averaged_metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.hparams['lr'],\n",
    "            betas=(\n",
    "                self.hparams['adam_beta1'], self.hparams['adam_beta2']),\n",
    "            eps=self.hparams['adam_epsilon'],\n",
    "            weight_decay=self.hparams['adam_weight_decay']\n",
    "        )\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-nightmare",
   "metadata": {},
   "source": [
    "## hparams, init train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "urban-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "generate_full_matrix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efficient-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_attributes 2\n",
      "num_attr_vals 4\n",
      "key_support_size 17\n",
      "query_support_size 256\n"
     ]
    }
   ],
   "source": [
    "# generate on the fly -- becareful of sampling time\n",
    "# num_attributes = 2\n",
    "# num_attr_vals = 3\n",
    "# game_data = sample_dataset(num_attributes, num_attr_vals, N_train=10000, N_val=1000)\n",
    "\n",
    "# read from disk instead\n",
    "with open('../Raw_Datasets/2Attr-4Vals-2hat23Train-2hat3Val.json', 'r') as f:\n",
    "    game_data = json.load(f)\n",
    "\n",
    "for k in game_data:\n",
    "    if not 'datapoints' in k:\n",
    "        print(k,':', game_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "built-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from disk instead\n",
    "hparams = {\n",
    "    'seed': 42,\n",
    "    'batch_size': 128,\n",
    "    'max_epochs': 50,\n",
    "    # embedding\n",
    "    'd_model': 8,\n",
    "    'embed_dropout': 0.0,\n",
    "    # final prediction\n",
    "    'dotproduct_bottleneck':True,\n",
    "    # loss\n",
    "    'loss_temperature_const': 1.0,\n",
    "    # optimizer\n",
    "    'lr': 0.001,\n",
    "    'adam_beta1': 0.9,\n",
    "    'adam_beta2': 0.999,\n",
    "    'adam_epsilon': 1e-08,\n",
    "    'warmup_steps': 12000,\n",
    "    'adam_weight_decay':0,\n",
    "    'gradient_clip_val': 0,\n",
    "    # others\n",
    "    'debug':False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "committed-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 unqiue cards\n",
      "Generated 81 cardpairs\n",
      "-- 45 cardpairs with shared concept\n",
      "-- 36 cardpairs without shared concept\n",
      "Number of cardpairs per shared concept [(1, 36), (0, 36), (2, 9)]\n",
      "Total number of matches =  189\n",
      "Number of matches per key concept hit =  [(1, 144), (0, 36), (2, 9)]\n",
      "{'shape': (10, 81), 'size': 810, 'sparsity': 0.23333333333333334, 'xy_rank': 10, 'xy_div_xyind_rank': 10}\n"
     ]
    }
   ],
   "source": [
    "# main() do this only if data is small\n",
    "if generate_full_matrix:\n",
    "    game_data_full = gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=0, debug=False)\n",
    "    count_table, xy, xyind, xy_div_xyind, distribution = report_gamedata_distribution(game_data_full, distribution_epsilon=0.0)\n",
    "    gt = {\n",
    "        'count_table':count_table,\n",
    "        'xy':xy,\n",
    "        'xyind':xyind,\n",
    "        'xy_div_xyind':xy_div_xyind,\n",
    "        'distribution':distribution\n",
    "    }\n",
    "    print(distribution)\n",
    "else:\n",
    "    gt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "informational-original",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                                         | Type               | Params\n",
      "-------------------------------------------------------------------------------------\n",
      "0  | model                                        | EncoderPredictor   | 728   \n",
      "1  | model.inp_query_layer                        | Sequential         | 648   \n",
      "2  | model.inp_query_layer.scaled_embed           | ScaledEmbedding    | 648   \n",
      "3  | model.inp_query_layer.scaled_embed.embedding | Embedding          | 648   \n",
      "4  | model.inp_query_layer.embed_dropout          | Dropout            | 0     \n",
      "5  | model.inp_key_layer                          | Sequential         | 80    \n",
      "6  | model.inp_key_layer.scaled_embed             | ScaledEmbedding    | 80    \n",
      "7  | model.inp_key_layer.scaled_embed.embedding   | Embedding          | 80    \n",
      "8  | loss_criterion                               | InfoCELoss         | 0     \n",
      "9  | loss_criterion.CE_loss                       | CrossEntropyLoss   | 0     \n",
      "10 | metrics                                      | ThresholdedMetrics | 0     \n",
      "11 | metrics.softmax                              | Softmax            | 0     \n",
      "-------------------------------------------------------------------------------------\n",
      "728       Trainable params\n",
      "0         Non-trainable params\n",
      "728       Total params \n",
      "\n",
      "RUN NAME :\n",
      " CardGame:OR;attr2-val3;d_model8;dot-product;params0.73K\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "\n",
    "hparams['key_support_size'] = game_data['key_support_size']\n",
    "hparams['query_support_size'] = game_data['query_support_size']\n",
    "hparams['num_attributes'] = game_data['num_attributes']\n",
    "hparams['num_attr_vals'] = game_data['num_attr_vals']\n",
    "hparams['populate_logits_matrix'] = generate_full_matrix\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# model\n",
    "thresholded_metrics = ThresholdedMetrics(raw_data=game_data)\n",
    "trainmodule =  TrainModule(hparams, gt_distributions=gt if hparams['populate_logits_matrix'] else {})\n",
    "model_summary = pl.core.memory.ModelSummary(trainmodule, mode='full')\n",
    "print(model_summary,'\\n')\n",
    "\n",
    "# dataset\n",
    "game_datamodule = GameDataModule(\n",
    "    batch_size = hparams['batch_size'],\n",
    "    raw_data = game_data,\n",
    "    debug=hparams['debug']\n",
    ")\n",
    "\n",
    "# testloader\n",
    "test_loader = DataLoader(\n",
    "            GameTestFullDataset(raw_data=game_data, debug=hparams['debug']), \n",
    "            batch_size=hparams['batch_size'], shuffle=False\n",
    "        )\n",
    "\n",
    "# logger\n",
    "run_name = 'CardGame:OR;attr{}-val{};d_model{};{};params{}K'.format(\n",
    "    num_attributes, num_attr_vals, \n",
    "    hparams['d_model'],  \n",
    "    'dot-product' if hparams['dotproduct_bottleneck'] else '',\n",
    "    round(max(model_summary.param_nums)/1000,2))\n",
    "project_name = 'ContrastiveLearning-cardgame-Scaling-SecondPass'\n",
    "wd_logger = WandbLogger(name=run_name, project=project_name)\n",
    "print('RUN NAME :\\n', run_name)\n",
    "\n",
    "# check point path\n",
    "ckpt_dir_PATH = os.path.join('checkpoints', project_name, run_name)\n",
    "os.makedirs(ckpt_dir_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-magic",
   "metadata": {},
   "source": [
    "## run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "furnished-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()\n",
    "def run_train(trainmodule, datamodule, ckpt_dir_PATH, hparams, wd_logger):\n",
    "\n",
    "    # checkpoints\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='avg_val_accuracy_by_Query',\n",
    "        dirpath=ckpt_dir_PATH,\n",
    "        filename='{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "        mode='max',\n",
    "    )\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=[1], \n",
    "        min_epochs=2, max_epochs=hparams['max_epochs'], \n",
    "        precision=32, \n",
    "        logger=wd_logger,\n",
    "        log_gpu_memory='all',\n",
    "        weights_summary = 'full',\n",
    "        gradient_clip_val=hparams['gradient_clip_val'],\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    #fit\n",
    "    with torch.autograd.detect_anomaly():\n",
    "        trainer.fit(trainmodule, datamodule)\n",
    "\n",
    "    wandb.save(os.path.join(ckpt_dir_PATH, 'last.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adolescent-smile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory checkpoints/ContrastiveLearning-cardgame-Scaling-SecondPass/CardGame:OR;attr2-val3;d_model8;dot-product;params0.73K exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "<ipython-input-25-8819e521c74f>:27: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">CardGame:OR;attr2-val3;d_model8;dot-product;params0.73K</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling-SecondPass\" target=\"_blank\">https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling-SecondPass</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling-SecondPass/runs/2613upgc\" target=\"_blank\">https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling-SecondPass/runs/2613upgc</a><br/>\n",
       "                Run data is saved locally in <code>/app/Contrastive-Learning-Benchmarking/SecondPass-CardGame-experiments/wandb/run-20210215_195541-2613upgc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                                         | Type               | Params\n",
      "-------------------------------------------------------------------------------------\n",
      "0  | model                                        | EncoderPredictor   | 728   \n",
      "1  | model.inp_query_layer                        | Sequential         | 648   \n",
      "2  | model.inp_query_layer.scaled_embed           | ScaledEmbedding    | 648   \n",
      "3  | model.inp_query_layer.scaled_embed.embedding | Embedding          | 648   \n",
      "4  | model.inp_query_layer.embed_dropout          | Dropout            | 0     \n",
      "5  | model.inp_key_layer                          | Sequential         | 80    \n",
      "6  | model.inp_key_layer.scaled_embed             | ScaledEmbedding    | 80    \n",
      "7  | model.inp_key_layer.scaled_embed.embedding   | Embedding          | 80    \n",
      "8  | loss_criterion                               | InfoCELoss         | 0     \n",
      "9  | loss_criterion.CE_loss                       | CrossEntropyLoss   | 0     \n",
      "10 | metrics                                      | ThresholdedMetrics | 0     \n",
      "11 | metrics.softmax                              | Softmax            | 0     \n",
      "-------------------------------------------------------------------------------------\n",
      "728       Trainable params\n",
      "0         Non-trainable params\n",
      "728       Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/87 [00:00<00:06, 13.31it/s, loss=622, v_num=upgc]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/torch/cuda/memory.py:231: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  91%| | 79/87 [00:04<00:00, 16.89it/s, loss=589, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|| 82/87 [00:04<00:00, 17.07it/s, loss=589, v_num=upgc]\n",
      "Epoch 0:  99%|| 86/87 [00:04<00:00, 17.24it/s, loss=589, v_num=upgc]\n",
      "Epoch 0: 100%|| 87/87 [00:05<00:00, 17.12it/s, loss=589, v_num=upgc]\n",
      "Epoch 1:  92%|| 80/87 [00:04<00:00, 17.05it/s, loss=580, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  97%|| 84/87 [00:04<00:00, 17.21it/s, loss=580, v_num=upgc]\n",
      "Epoch 1: 100%|| 87/87 [00:05<00:00, 17.19it/s, loss=580, v_num=upgc]\n",
      "Epoch 2:  92%|| 80/87 [00:04<00:00, 17.33it/s, loss=563, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  97%|| 84/87 [00:04<00:00, 17.47it/s, loss=563, v_num=upgc]\n",
      "Epoch 2: 100%|| 87/87 [00:04<00:00, 17.46it/s, loss=563, v_num=upgc]\n",
      "Epoch 3:  92%|| 80/87 [00:04<00:00, 16.98it/s, loss=539, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  97%|| 84/87 [00:04<00:00, 17.13it/s, loss=539, v_num=upgc]\n",
      "Epoch 3: 100%|| 87/87 [00:05<00:00, 17.11it/s, loss=539, v_num=upgc]\n",
      "Epoch 4:  92%|| 80/87 [00:04<00:00, 17.34it/s, loss=516, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  25%|       | 2/8 [00:00<00:00, 18.69it/s]\u001b[A\n",
      "Epoch 4:  97%|| 84/87 [00:04<00:00, 17.40it/s, loss=516, v_num=upgc]\n",
      "Epoch 4: 100%|| 87/87 [00:05<00:00, 17.34it/s, loss=516, v_num=upgc]\n",
      "Epoch 5:  92%|| 80/87 [00:04<00:00, 16.97it/s, loss=497, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  25%|       | 2/8 [00:00<00:00, 19.88it/s]\u001b[A\n",
      "Epoch 5:  97%|| 84/87 [00:04<00:00, 17.05it/s, loss=497, v_num=upgc]\n",
      "Epoch 5: 100%|| 87/87 [00:05<00:00, 17.03it/s, loss=497, v_num=upgc]\n",
      "Epoch 6:  92%|| 80/87 [00:04<00:00, 16.80it/s, loss=482, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  97%|| 84/87 [00:04<00:00, 16.97it/s, loss=482, v_num=upgc]\n",
      "Epoch 6: 100%|| 87/87 [00:05<00:00, 16.97it/s, loss=482, v_num=upgc]\n",
      "Epoch 7:  92%|| 80/87 [00:04<00:00, 17.70it/s, loss=475, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  25%|       | 2/8 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 7:  97%|| 84/87 [00:04<00:00, 17.68it/s, loss=475, v_num=upgc]\n",
      "Epoch 7: 100%|| 87/87 [00:04<00:00, 17.64it/s, loss=475, v_num=upgc]\n",
      "Epoch 8:  92%|| 80/87 [00:04<00:00, 16.83it/s, loss=467, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  97%|| 84/87 [00:04<00:00, 16.99it/s, loss=467, v_num=upgc]\n",
      "Epoch 8: 100%|| 87/87 [00:05<00:00, 16.96it/s, loss=467, v_num=upgc]\n",
      "Epoch 9:  92%|| 80/87 [00:04<00:00, 16.83it/s, loss=462, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  97%|| 84/87 [00:04<00:00, 17.01it/s, loss=462, v_num=upgc]\n",
      "Epoch 9: 100%|| 87/87 [00:05<00:00, 17.00it/s, loss=462, v_num=upgc]\n",
      "Epoch 10:  92%|| 80/87 [00:04<00:00, 16.28it/s, loss=460, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  97%|| 84/87 [00:05<00:00, 16.44it/s, loss=460, v_num=upgc]\n",
      "Epoch 10: 100%|| 87/87 [00:05<00:00, 16.46it/s, loss=460, v_num=upgc]\n",
      "Epoch 11:  92%|| 80/87 [00:04<00:00, 17.65it/s, loss=457, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  97%|| 84/87 [00:04<00:00, 17.75it/s, loss=457, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 20.61it/s]\u001b[A\n",
      "Epoch 11: 100%|| 87/87 [00:04<00:00, 17.71it/s, loss=457, v_num=upgc]\n",
      "Epoch 12:  92%|| 80/87 [00:04<00:00, 17.16it/s, loss=455, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  97%|| 84/87 [00:04<00:00, 17.31it/s, loss=455, v_num=upgc]\n",
      "Epoch 12: 100%|| 87/87 [00:05<00:00, 17.30it/s, loss=455, v_num=upgc]\n",
      "Epoch 13:  92%|| 80/87 [00:04<00:00, 17.33it/s, loss=457, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  97%|| 84/87 [00:04<00:00, 17.46it/s, loss=457, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 13: 100%|| 87/87 [00:04<00:00, 17.44it/s, loss=457, v_num=upgc]\n",
      "Epoch 14:  92%|| 80/87 [00:04<00:00, 17.60it/s, loss=455, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  97%|| 84/87 [00:04<00:00, 17.70it/s, loss=455, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 19.96it/s]\u001b[A\n",
      "Epoch 14: 100%|| 87/87 [00:04<00:00, 17.65it/s, loss=455, v_num=upgc]\n",
      "Epoch 15:  92%|| 80/87 [00:04<00:00, 16.98it/s, loss=453, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  97%|| 84/87 [00:04<00:00, 17.13it/s, loss=453, v_num=upgc]\n",
      "Epoch 15: 100%|| 87/87 [00:05<00:00, 17.11it/s, loss=453, v_num=upgc]\n",
      "Epoch 16:  92%|| 80/87 [00:04<00:00, 17.42it/s, loss=453, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  97%|| 84/87 [00:04<00:00, 17.55it/s, loss=453, v_num=upgc]\n",
      "Epoch 16: 100%|| 87/87 [00:04<00:00, 17.53it/s, loss=453, v_num=upgc]\n",
      "Epoch 17:  92%|| 80/87 [00:04<00:00, 17.38it/s, loss=455, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  97%|| 84/87 [00:04<00:00, 17.55it/s, loss=455, v_num=upgc]\n",
      "Epoch 17: 100%|| 87/87 [00:04<00:00, 17.52it/s, loss=455, v_num=upgc]\n",
      "Epoch 18:  92%|| 80/87 [00:04<00:00, 16.53it/s, loss=453, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  97%|| 84/87 [00:05<00:00, 16.69it/s, loss=453, v_num=upgc]\n",
      "Epoch 18: 100%|| 87/87 [00:05<00:00, 16.68it/s, loss=453, v_num=upgc]\n",
      "Epoch 19:  92%|| 80/87 [00:04<00:00, 16.47it/s, loss=449, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  97%|| 84/87 [00:05<00:00, 16.66it/s, loss=449, v_num=upgc]\n",
      "Epoch 19: 100%|| 87/87 [00:05<00:00, 16.65it/s, loss=449, v_num=upgc]\n",
      "Epoch 20:  92%|| 80/87 [00:04<00:00, 16.85it/s, loss=451, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  97%|| 84/87 [00:04<00:00, 16.98it/s, loss=451, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 20.37it/s]\u001b[A\n",
      "Epoch 20: 100%|| 87/87 [00:05<00:00, 16.90it/s, loss=451, v_num=upgc]\n",
      "Epoch 21:  92%|| 80/87 [00:04<00:00, 17.29it/s, loss=450, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  97%|| 84/87 [00:04<00:00, 17.44it/s, loss=450, v_num=upgc]\n",
      "Epoch 21: 100%|| 87/87 [00:04<00:00, 17.42it/s, loss=450, v_num=upgc]\n",
      "Epoch 22:  92%|| 80/87 [00:04<00:00, 17.21it/s, loss=450, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  97%|| 84/87 [00:04<00:00, 17.35it/s, loss=450, v_num=upgc]\n",
      "Epoch 22: 100%|| 87/87 [00:05<00:00, 17.34it/s, loss=450, v_num=upgc]\n",
      "Epoch 23:  92%|| 80/87 [00:04<00:00, 17.56it/s, loss=448, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  97%|| 84/87 [00:04<00:00, 17.67it/s, loss=448, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 20.55it/s]\u001b[A\n",
      "Epoch 23: 100%|| 87/87 [00:04<00:00, 17.61it/s, loss=448, v_num=upgc]\n",
      "Epoch 24:  92%|| 80/87 [00:04<00:00, 17.24it/s, loss=447, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  97%|| 84/87 [00:04<00:00, 17.38it/s, loss=447, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 21.06it/s]\u001b[A\n",
      "Epoch 24: 100%|| 87/87 [00:05<00:00, 17.29it/s, loss=447, v_num=upgc]\n",
      "Epoch 25:  92%|| 80/87 [00:04<00:00, 16.96it/s, loss=450, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  97%|| 84/87 [00:04<00:00, 17.09it/s, loss=450, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 19.93it/s]\u001b[A\n",
      "Epoch 25: 100%|| 87/87 [00:05<00:00, 17.06it/s, loss=450, v_num=upgc]\n",
      "Epoch 26:  92%|| 80/87 [00:04<00:00, 17.39it/s, loss=447, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  97%|| 84/87 [00:04<00:00, 17.55it/s, loss=447, v_num=upgc]\n",
      "Epoch 26: 100%|| 87/87 [00:04<00:00, 17.53it/s, loss=447, v_num=upgc]\n",
      "Epoch 27:  92%|| 80/87 [00:04<00:00, 17.42it/s, loss=447, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  97%|| 84/87 [00:04<00:00, 17.55it/s, loss=447, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 27: 100%|| 87/87 [00:04<00:00, 17.52it/s, loss=447, v_num=upgc]\n",
      "Epoch 28:  92%|| 80/87 [00:04<00:00, 17.81it/s, loss=447, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  97%|| 84/87 [00:04<00:00, 17.95it/s, loss=447, v_num=upgc]\n",
      "Epoch 28: 100%|| 87/87 [00:04<00:00, 17.91it/s, loss=447, v_num=upgc]\n",
      "Epoch 29:  92%|| 80/87 [00:04<00:00, 17.62it/s, loss=445, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  97%|| 84/87 [00:04<00:00, 17.76it/s, loss=445, v_num=upgc]\n",
      "Epoch 29: 100%|| 87/87 [00:04<00:00, 17.74it/s, loss=445, v_num=upgc]\n",
      "Epoch 30:  92%|| 80/87 [00:04<00:00, 17.62it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  97%|| 84/87 [00:04<00:00, 17.71it/s, loss=444, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 19.79it/s]\u001b[A\n",
      "Epoch 30: 100%|| 87/87 [00:04<00:00, 17.64it/s, loss=444, v_num=upgc]\n",
      "Epoch 31:  92%|| 80/87 [00:04<00:00, 17.35it/s, loss=446, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  97%|| 84/87 [00:04<00:00, 17.53it/s, loss=446, v_num=upgc]\n",
      "Epoch 31: 100%|| 87/87 [00:04<00:00, 17.52it/s, loss=446, v_num=upgc]\n",
      "Epoch 32:  92%|| 80/87 [00:04<00:00, 17.38it/s, loss=445, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  97%|| 84/87 [00:04<00:00, 17.48it/s, loss=445, v_num=upgc]\n",
      "Validating:  62%|   | 5/8 [00:00<00:00, 20.09it/s]\u001b[A\n",
      "Epoch 32: 100%|| 87/87 [00:04<00:00, 17.45it/s, loss=445, v_num=upgc]\n",
      "Epoch 33:  92%|| 80/87 [00:04<00:00, 17.60it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  97%|| 84/87 [00:04<00:00, 17.75it/s, loss=444, v_num=upgc]\n",
      "Epoch 33: 100%|| 87/87 [00:04<00:00, 17.72it/s, loss=444, v_num=upgc]\n",
      "Epoch 34:  92%|| 80/87 [00:04<00:00, 17.17it/s, loss=445, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  97%|| 84/87 [00:04<00:00, 17.33it/s, loss=445, v_num=upgc]\n",
      "Epoch 34: 100%|| 87/87 [00:05<00:00, 17.30it/s, loss=445, v_num=upgc]\n",
      "Epoch 35:  92%|| 80/87 [00:04<00:00, 17.53it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  97%|| 84/87 [00:04<00:00, 17.70it/s, loss=444, v_num=upgc]\n",
      "Epoch 35: 100%|| 87/87 [00:04<00:00, 17.67it/s, loss=444, v_num=upgc]\n",
      "Epoch 36:  92%|| 80/87 [00:04<00:00, 17.56it/s, loss=445, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  97%|| 84/87 [00:04<00:00, 17.68it/s, loss=445, v_num=upgc]\n",
      "Epoch 36: 100%|| 87/87 [00:04<00:00, 17.66it/s, loss=445, v_num=upgc]\n",
      "Epoch 37:  92%|| 80/87 [00:04<00:00, 17.38it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  97%|| 84/87 [00:04<00:00, 17.56it/s, loss=444, v_num=upgc]\n",
      "Epoch 37: 100%|| 87/87 [00:04<00:00, 17.53it/s, loss=444, v_num=upgc]\n",
      "Epoch 38:  92%|| 80/87 [00:04<00:00, 17.61it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  97%|| 84/87 [00:04<00:00, 17.77it/s, loss=444, v_num=upgc]\n",
      "Epoch 38: 100%|| 87/87 [00:04<00:00, 17.74it/s, loss=444, v_num=upgc]\n",
      "Epoch 39:  92%|| 80/87 [00:04<00:00, 17.45it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  97%|| 84/87 [00:04<00:00, 17.59it/s, loss=443, v_num=upgc]\n",
      "Epoch 39: 100%|| 87/87 [00:04<00:00, 17.57it/s, loss=443, v_num=upgc]\n",
      "Epoch 40:  92%|| 80/87 [00:04<00:00, 17.58it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  97%|| 84/87 [00:04<00:00, 17.72it/s, loss=443, v_num=upgc]\n",
      "Epoch 40: 100%|| 87/87 [00:04<00:00, 17.69it/s, loss=443, v_num=upgc]\n",
      "Epoch 41:  92%|| 80/87 [00:04<00:00, 17.33it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41:  97%|| 84/87 [00:04<00:00, 17.49it/s, loss=444, v_num=upgc]\n",
      "Epoch 41: 100%|| 87/87 [00:04<00:00, 17.48it/s, loss=444, v_num=upgc]\n",
      "Epoch 42:  92%|| 80/87 [00:04<00:00, 17.59it/s, loss=444, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  97%|| 84/87 [00:04<00:00, 17.74it/s, loss=444, v_num=upgc]\n",
      "Epoch 42: 100%|| 87/87 [00:04<00:00, 17.72it/s, loss=444, v_num=upgc]\n",
      "Epoch 43:  92%|| 80/87 [00:04<00:00, 17.45it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  97%|| 84/87 [00:04<00:00, 17.59it/s, loss=443, v_num=upgc]\n",
      "Epoch 43: 100%|| 87/87 [00:04<00:00, 17.58it/s, loss=443, v_num=upgc]\n",
      "Epoch 44:  92%|| 80/87 [00:04<00:00, 17.33it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  97%|| 84/87 [00:04<00:00, 17.48it/s, loss=443, v_num=upgc]\n",
      "Epoch 44: 100%|| 87/87 [00:04<00:00, 17.47it/s, loss=443, v_num=upgc]\n",
      "Epoch 45:  92%|| 80/87 [00:04<00:00, 17.63it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  97%|| 84/87 [00:04<00:00, 17.80it/s, loss=443, v_num=upgc]\n",
      "Epoch 45: 100%|| 87/87 [00:04<00:00, 17.77it/s, loss=443, v_num=upgc]\n",
      "Epoch 46:  92%|| 80/87 [00:04<00:00, 17.52it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  97%|| 84/87 [00:04<00:00, 17.66it/s, loss=443, v_num=upgc]\n",
      "Epoch 46: 100%|| 87/87 [00:04<00:00, 17.64it/s, loss=443, v_num=upgc]\n",
      "Epoch 47:  92%|| 80/87 [00:04<00:00, 17.77it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  97%|| 84/87 [00:04<00:00, 17.90it/s, loss=443, v_num=upgc]\n",
      "Epoch 47: 100%|| 87/87 [00:04<00:00, 17.85it/s, loss=443, v_num=upgc]\n",
      "Epoch 48:  92%|| 80/87 [00:04<00:00, 17.40it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  97%|| 84/87 [00:04<00:00, 17.56it/s, loss=443, v_num=upgc]\n",
      "Epoch 48: 100%|| 87/87 [00:04<00:00, 17.54it/s, loss=443, v_num=upgc]\n",
      "Epoch 49:  92%|| 80/87 [00:04<00:00, 17.40it/s, loss=443, v_num=upgc]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  97%|| 84/87 [00:04<00:00, 17.56it/s, loss=443, v_num=upgc]\n",
      "Epoch 49: 100%|| 87/87 [00:04<00:00, 17.55it/s, loss=443, v_num=upgc]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|| 87/87 [00:05<00:00, 17.30it/s, loss=443, v_num=upgc]\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "run_train(trainmodule, game_datamodule, ckpt_dir_PATH, hparams, wd_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-grenada",
   "metadata": {},
   "source": [
    "## run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nonprofit-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(ckpt_name, project_name, run_Id, gt, test_loader, trainmodule, datamodule, ckpt_dir_PATH, hparams, wd_logger, gpu, figsize=(10,15)):\n",
    "\n",
    "    checkpoint_PATH = os.path.join(ckpt_dir_PATH, ckpt_name) #'last.ckpt'\n",
    "    run_PATH = os.path.join(project_name, run_Id) # e.g. 'tki26w1t' also from wandb interface\n",
    "\n",
    "    wandb.restore(checkpoint_PATH, run_path=run_PATH)\n",
    "    checkpoint = torch.load(checkpoint_PATH, map_location=lambda storage, loc: storage)\n",
    "    trainmodule.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=[gpu], \n",
    "        min_epochs=1, max_epochs=1, \n",
    "        precision=32, \n",
    "        logger=wd_logger,\n",
    "        log_gpu_memory='all',\n",
    "        weights_summary = 'full',\n",
    "        gradient_clip_val=hparams['gradient_clip_val'],\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "                GameTestFullDataset(raw_data=game_data, debug=True), \n",
    "                batch_size=hparams['batch_size'], shuffle=False\n",
    "            )\n",
    "    \n",
    "    res = trainer.test(model=trainmodule, test_dataloaders=test_loader)\n",
    "    \n",
    "    if hparams['populate_logits_matrix']:  \n",
    "        model_distribution_res = trainmodule.pull_model_distribution(debug=hparams['debug'])\n",
    "        print('xy_hat_rank:', model_distribution_res['xy_hat_rank'])\n",
    "        print('xy_div_xyind_hat_rank:', model_distribution_res['xy_div_xyind_hat_rank'])\n",
    "        print('mi_hat:', model_distribution_res['mi_hat'])\n",
    "        print('mi_gt:', model_distribution_res['mi_gt'])\n",
    "        print('kl_div:', model_distribution_res['kl_div'])\n",
    "        \n",
    "        plot_distribution(model_distribution_res['xy_hat'], model_distribution_res['xy_div_xyind_hat'], 'Model', figsize)\n",
    "        plot_distribution(gt['xy'], gt['xy_div_xyind'],'Ground-Truth', figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "super-discretion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      " 0 : 0 0 [0 0] [0 0]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 1 : 0 1 [0 0] [0 1]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 2 : 0 2 [0 0] [0 2]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 3 : 0 3 [0 0] [1 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 4 : 0 4 [0 0] [1 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 5 : 0 5 [0 0] [1 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 6 : 0 6 [0 0] [2 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 7 : 0 7 [0 0] [2 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 8 : 0 8 [0 0] [2 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 9 : 1 0 [0 1] [0 0]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 10 : 1 1 [0 1] [0 1]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 11 : 1 2 [0 1] [0 2]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 12 : 1 3 [0 1] [1 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 13 : 1 4 [0 1] [1 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 14 : 1 5 [0 1] [1 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 15 : 1 6 [0 1] [2 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 16 : 1 7 [0 1] [2 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 17 : 1 8 [0 1] [2 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 18 : 2 0 [0 2] [0 0]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 19 : 2 1 [0 2] [0 1]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2])]\n",
      "query\n",
      " 20 : 2 2 [0 2] [0 2]\n",
      "all matches \n",
      " [array([0, 0]), array([0, 1]), array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 21 : 2 3 [0 2] [1 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 22 : 2 4 [0 2] [1 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 23 : 2 5 [0 2] [1 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 24 : 2 6 [0 2] [2 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 25 : 2 7 [0 2] [2 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 26 : 2 8 [0 2] [2 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 27 : 3 0 [1 0] [0 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 28 : 3 1 [1 0] [0 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 29 : 3 2 [1 0] [0 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 30 : 3 3 [1 0] [1 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([1, 1]), array([1, 2]), array([2, 0])]\n",
      "query\n",
      " 31 : 3 4 [1 0] [1 1]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 32 : 3 5 [1 0] [1 2]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 33 : 3 6 [1 0] [2 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 34 : 3 7 [1 0] [2 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 35 : 3 8 [1 0] [2 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 36 : 4 0 [1 1] [0 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 37 : 4 1 [1 1] [0 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 38 : 4 2 [1 1] [0 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 39 : 4 3 [1 1] [1 0]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 40 : 4 4 [1 1] [1 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 0]), array([1, 1]), array([1, 2]), array([2, 1])]\n",
      "query\n",
      " 41 : 4 5 [1 1] [1 2]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 42 : 4 6 [1 1] [2 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 43 : 4 7 [1 1] [2 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 44 : 4 8 [1 1] [2 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 45 : 5 0 [1 2] [0 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 46 : 5 1 [1 2] [0 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 47 : 5 2 [1 2] [0 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 48 : 5 3 [1 2] [1 0]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 49 : 5 4 [1 2] [1 1]\n",
      "all matches \n",
      " [array([1, 0]), array([1, 1]), array([1, 2])]\n",
      "query\n",
      " 50 : 5 5 [1 2] [1 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 0]), array([1, 1]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 51 : 5 6 [1 2] [2 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 52 : 5 7 [1 2] [2 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 53 : 5 8 [1 2] [2 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 54 : 6 0 [2 0] [0 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 55 : 6 1 [2 0] [0 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 56 : 6 2 [2 0] [0 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 57 : 6 3 [2 0] [1 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0])]\n",
      "query\n",
      " 58 : 6 4 [2 0] [1 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 59 : 6 5 [2 0] [1 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 60 : 6 6 [2 0] [2 0]\n",
      "all matches \n",
      " [array([0, 0]), array([1, 0]), array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 61 : 6 7 [2 0] [2 1]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 62 : 6 8 [2 0] [2 2]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 63 : 7 0 [2 1] [0 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 64 : 7 1 [2 1] [0 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 65 : 7 2 [2 1] [0 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 66 : 7 3 [2 1] [1 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 67 : 7 4 [2 1] [1 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 1])]\n",
      "query\n",
      " 68 : 7 5 [2 1] [1 2]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 69 : 7 6 [2 1] [2 0]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 70 : 7 7 [2 1] [2 1]\n",
      "all matches \n",
      " [array([0, 1]), array([1, 1]), array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 71 : 7 8 [2 1] [2 2]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 72 : 8 0 [2 2] [0 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 73 : 8 1 [2 2] [0 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 74 : 8 2 [2 2] [0 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 75 : 8 3 [2 2] [1 0]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 76 : 8 4 [2 2] [1 1]\n",
      "all matches \n",
      " ['']\n",
      "query\n",
      " 77 : 8 5 [2 2] [1 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 2])]\n",
      "query\n",
      " 78 : 8 6 [2 2] [2 0]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 79 : 8 7 [2 2] [2 1]\n",
      "all matches \n",
      " [array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "query\n",
      " 80 : 8 8 [2 2] [2 2]\n",
      "all matches \n",
      " [array([0, 2]), array([1, 2]), array([2, 0]), array([2, 1]), array([2, 2])]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-a4beacb88efb>:67: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  fil = torch.tensor(\n",
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The testing_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_NullQueries_accuracy_by_Query': 1.0,\n",
      " 'avg_test_NullQueries_accuracy_by_QueryKey': 1.0,\n",
      " 'avg_test_NullQueries_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'avg_test_NullQueries_error_rate_for_1_matched_concepts': 0.0,\n",
      " 'avg_test_NullQueries_f1_by_Query': 1.0,\n",
      " 'avg_test_NullQueries_f1_by_QueryKey': 1.0,\n",
      " 'avg_test_NullQueries_precision_by_Query': 1.0,\n",
      " 'avg_test_NullQueries_precision_by_QueryKey': 1.0,\n",
      " 'avg_test_NullQueries_recall_by_Query': 1.0,\n",
      " 'avg_test_NullQueries_recall_by_QueryKey': 1.0,\n",
      " 'avg_test_NullQueries_total_count_for_0_matched_concepts': 324.0,\n",
      " 'avg_test_NullQueries_total_count_for_1_matched_concepts': 36.0,\n",
      " 'avg_test_accuracy_by_Query': 0.9975309371948242,\n",
      " 'avg_test_accuracy_by_QueryKey': 0.9975309371948242,\n",
      " 'avg_test_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'avg_test_error_rate_for_1_matched_concepts': 0.011111111111111112,\n",
      " 'avg_test_f1_by_Query': 0.9972565174102783,\n",
      " 'avg_test_f1_by_QueryKey': 0.9946808218955994,\n",
      " 'avg_test_nonNullQueries_accuracy_by_Query': 0.995555579662323,\n",
      " 'avg_test_nonNullQueries_accuracy_by_QueryKey': 0.995555579662323,\n",
      " 'avg_test_nonNullQueries_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'avg_test_nonNullQueries_error_rate_for_1_matched_concepts': 0.013888888888888888,\n",
      " 'avg_test_nonNullQueries_f1_by_Query': 0.9950617551803589,\n",
      " 'avg_test_nonNullQueries_f1_by_QueryKey': 0.9934210777282715,\n",
      " 'avg_test_nonNullQueries_precision_by_Query': 1.0,\n",
      " 'avg_test_nonNullQueries_precision_by_QueryKey': 1.0,\n",
      " 'avg_test_nonNullQueries_recall_by_Query': 0.9911110997200012,\n",
      " 'avg_test_nonNullQueries_recall_by_QueryKey': 0.9869281053543091,\n",
      " 'avg_test_nonNullQueries_total_count_for_0_matched_concepts': 297.0,\n",
      " 'avg_test_nonNullQueries_total_count_for_1_matched_concepts': 144.0,\n",
      " 'avg_test_precision_by_Query': 1.0,\n",
      " 'avg_test_precision_by_QueryKey': 1.0,\n",
      " 'avg_test_recall_by_Query': 0.9950616955757141,\n",
      " 'avg_test_recall_by_QueryKey': 0.9894179701805115,\n",
      " 'avg_test_total_count_for_0_matched_concepts': 621.0,\n",
      " 'avg_test_total_count_for_1_matched_concepts': 180.0,\n",
      " 'test_NullQueries_accuracy_by_Query': 1.0,\n",
      " 'test_NullQueries_accuracy_by_QueryKey': 1.0,\n",
      " 'test_NullQueries_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'test_NullQueries_error_rate_for_1_matched_concepts': 0.0,\n",
      " 'test_NullQueries_f1_by_Query': 1.0,\n",
      " 'test_NullQueries_f1_by_QueryKey': 1.0,\n",
      " 'test_NullQueries_precision_by_Query': 1.0,\n",
      " 'test_NullQueries_precision_by_QueryKey': 1.0,\n",
      " 'test_NullQueries_recall_by_Query': 1.0,\n",
      " 'test_NullQueries_recall_by_QueryKey': 1.0,\n",
      " 'test_NullQueries_total_count_for_0_matched_concepts': 324,\n",
      " 'test_NullQueries_total_count_for_1_matched_concepts': 36,\n",
      " 'test_accuracy_by_Query': 0.9975309371948242,\n",
      " 'test_accuracy_by_QueryKey': 0.9975309371948242,\n",
      " 'test_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'test_error_rate_for_1_matched_concepts': 0.011111111111111112,\n",
      " 'test_f1_by_Query': 0.9972565174102783,\n",
      " 'test_f1_by_QueryKey': 0.9946808218955994,\n",
      " 'test_nonNullQueries_accuracy_by_Query': 0.995555579662323,\n",
      " 'test_nonNullQueries_accuracy_by_QueryKey': 0.995555579662323,\n",
      " 'test_nonNullQueries_error_rate_for_0_matched_concepts': 0.0,\n",
      " 'test_nonNullQueries_error_rate_for_1_matched_concepts': 0.013888888888888888,\n",
      " 'test_nonNullQueries_f1_by_Query': 0.9950617551803589,\n",
      " 'test_nonNullQueries_f1_by_QueryKey': 0.9934210777282715,\n",
      " 'test_nonNullQueries_precision_by_Query': 1.0,\n",
      " 'test_nonNullQueries_precision_by_QueryKey': 1.0,\n",
      " 'test_nonNullQueries_recall_by_Query': 0.9911110997200012,\n",
      " 'test_nonNullQueries_recall_by_QueryKey': 0.9869281053543091,\n",
      " 'test_nonNullQueries_total_count_for_0_matched_concepts': 297,\n",
      " 'test_nonNullQueries_total_count_for_1_matched_concepts': 144,\n",
      " 'test_precision_by_Query': 1.0,\n",
      " 'test_precision_by_QueryKey': 1.0,\n",
      " 'test_recall_by_Query': 0.9950616955757141,\n",
      " 'test_recall_by_QueryKey': 0.9894179701805115,\n",
      " 'test_total_count_for_0_matched_concepts': 621,\n",
      " 'test_total_count_for_1_matched_concepts': 180}\n",
      "--------------------------------------------------------------------------------\n",
      "SELF GT MI tensor(1.2546, device='cuda:0', dtype=torch.float64)\n",
      "xy_hat_rank: 10\n",
      "xy_div_xyind_hat_rank: 10\n",
      "mi_hat: tensor(1.6565, device='cuda:0', dtype=torch.float64)\n",
      "mi_gt: tensor(1.2546, device='cuda:0', dtype=torch.float64)\n",
      "kl_div: tensor(0.0005, device='cuda:0', dtype=torch.float64)\n",
      "Full\n",
      "Full\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAB0CAYAAAB30m0qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUCklEQVR4nO3deZhcVZnH8e/PTmJIWCKLYlZgjCAyEjCiIjAYtkQZ4HFcCIjoyCCPjqC4++goIIqODsu4DQNKGFnEKIsKI2hAEOcBEohIWFyQJISEECEQApIA7/xxTkN1V1fdrqKqurrv7/M8eVJV99a55751b9Xb555zriICMzMzM6vtRUNdATMzM7Nu54TJzMzMrIATJjMzM7MCTpjMzMzMCjhhMjMzMyvghMnMzMysgBMmsw6RtJ2kkDSqw9v9oqQfdHKb9VTWR9JUSY9L6hmCehwp6eom39vSz1LSfZL2z4+76vNqhKSrJB091PUwawcnTDaiSDpc0k2S1ktanR9/UJKGum61SPpuThoel7RB0saK51c1WNa+ku5vV11bLSKWRcSmEfHMEGz7gog4sFPbk7RpM5/pAOW8U9JdktZJulPSYRXLdpH0C0lrJHV8kr2ImBMR8zq9XbNOcMJkI4akjwFnAv8ObAu8DDgOeBMwpsZ7Ot6y0V9EHJeThk2BLwM/7H0eEXN61+t0y5S13D8BTwEHSNq2mQIkTQJ+AJwIbA58ArhQ0kvzKhuBS4D3v/DqNlQvSfLviY1oPsBtRJC0BXAy8MGImB8R6yK5LSKOjIin8nrnSfqOpCslrQfeLOlVkq6TtFbSEkmHVJR7naRjKp6/V9JvKp6HpOMk/TG//1u9rVmSeiR9Pf+1fy/w1ib26z5Jn5J0O7Be0qi8zVdUrHOepC9JGg9cBUysaKGamFcbI+n83CqxRNLMGtvbM9d3Sn6+q6RHJO0k6ROSftxv/bMknVmwD9tL+nXe9jXA1hXLnru0Jeldkhb2e+9HJV1Rp+wxkhZL+nB+3iPpRkn/JmlbSU9I2qpi/d0lPSRpdKc/S+Bo4LvA7cC7m3g/wGRgbURclY/vnwPrgb8DiIh7IuJcYMlgCpN0gKS7JT0q6Zv5czomL+tzaVD9LkPmc+NUSTcCTwA7DHC+/LNSa9gjueVrWn5dkk5XagV+TNLvJe3SZEzMOsIJk40UbwReDFw+iHWPAE4FNgNuAn4KXA28FPgwcIGkHRvY9sHA64DXAO8EDsqv/0tethswE3h7A2VWmkv6gZ4QEU/XWiki1gNzgAcqWqgeyIsPAS4GJgBXAN+sUcZvgf8C5knahNSa8fmIuDs/ni1pAjzX4nU4cH5B/S8EFpESpVNIicNAfgrsKGl6xWtH5PfX2ucNpOTjZEmvAj4N9ACnRsQq4DrSZ9LrKODiiNhYo8i2fJY5UdgXuCD/e08j76+wELhL0iE5iTuM1Gp1e6MFSdoa+AnwOdJn82dSa2wjjgKOJZ1LS/uVfyjwWeBtwDbADcBFefGBwD7AK4EtSLH+a6P7YNZJTphspNgaWFOZUEj6bW4peFLSPhXrXh4RN0bEs8AMYFPgtIjYEBELgJ+RkpTBOi0i1kbEMuDaXCakH4EzImJ5RDwMfKXJfTsrl/Fkk+8H+E1EXJn7Cv0PsGuddb9I+hG7GVgBfAsgIlYC1wPvyOvNJsV8Ua2CJE0lJSCfj4inIuJ6UmJUJSKeICW8c/N7pwM7kRK8miLiDuBLwGXAx4GjKvpEzSO35ihdfp1L2v9a2vVZHgXcHhF3khLXV0varcEyyPt1PimJfCr//4GcLDfqLcCS3CK7ETgDWNVgGedFxJKIeHqAJPQ44CsRcVc+L78MzMjJ40ZSkrUToLzOyib2waxjnDDZSPFXYGtV9POJiD0jYkJeVnmsL694PBFYnpOnXkuBSQ1su/JH5glSAvZc2f3KBUDS3hWXzYounywvWN5MHceqRp+o/MN3HrAL8I3oe4fu5xKQ/H+95ANSDB7p94O+tNbKpASgN1k9ArgsJ1JF5gHTgCsj4o8Vr18O7Cxpe+AA4NGIuLlOOQ1/loP0HlLLEhGxAvg1tVvaalIaSfc1UmvVGOAfgHMkzWi0LPrtU/6cGz3W6q0/DTgz/9GyFngYEDAp/2HyTVIyvlrS2ZI2b3DbZh3lhMlGiv8j/cV96CDWrUwAHgCmqG+H1amklhVI/UPGVSxrpLPuSmBKv3JTBSJuqLhs9uoG6gvph7xWnV7wyCiljsVfAL4PfEPSiysWXwa8Jvc3OZicBNSxEniJUv+qXlNrrQxcA2yTE4C51Lkc18+3SS2DB0naq/fFiPgbqRP0u0mtPEUJXi01P8sikvYEpgOfkbRK0irg9cARtZLWOmYA10fEwoh4NiJuIV1W3r/BcqDfPuX+WpX7OJhjv97xtpzU+jWh4t8m+bIvEXFWRLwW2Jl0ae4TTeyDWcc4YbIRISLWAicB35b0dkmbSXpR/uEdX+etN5ESkE/mjsD7Av9IumwCsBh4m6RxSh2tGxl9dAlwvKTJkl5C6l/TCotJP7Y9kmaTWhl6PQhspdQJvmH5R/M84FzSvq4k9TsCnktA5pMSmZvzpauaImIpqd/NSbmD9l6k+NZafyPwI9JIxy1JCVRRnY8CXgu8Fzie1P9q04pVzs/LDqH5hKnuZ5k7SF9X471Hk/ZjZ1LCM4PUercJqc9ZI24B9u5tUcqX9fYm92HKnanHkkeFShrbL+Gt9HPSpcG35cTtePomRYuBfZTmytoC+EyDdf0uKUl8da7LFpLekR+/TtLrJY0mJWZ/A56tXZTZ0HPCZCNGRHyNNNz6k6TE4UFSB+ZPAb+t8Z4NpB/wOcAaUkvFe3InZ4DTgQ25rHkUt6hU+m/gF8DvgFtJHWxb4YRc57XAkaRWHwByvS8C7s2XQiYOVEAdx5M6v38+X6J5H/A+SXtXrDMP+HsGn3wcQWpReZjUcjWYTuL7Az+q18kdnusjdQbpM3s8Ii4kJWin964TETeSfoxvzQlcM4o+yynAjQPUbyyp/9N/RsSqin9/IcWvoctyEfFrUh+z+ZLWAT8GvhwRvRNwTgOe5PlRck8C99Qoaw2pP9pppMvW0yv3ISKuAX5ISsYWkVrwGqnrpcBXgYslPQbcwfMJ4uakmD5Curz5V1KSbNa11Ld7gplZfTlJuRvYNiIeG+r6DIakBcCFEXFOm8pfDOwXEcN6pFduJftBu+JkNpx5IjwzG7Tc1+tE0tD84ZIsvQ7YncH1b2tKRMxoV9lm1h2cMJnZoOSO2w+SLqHM7rfs8RpvmxMRN7Rg20tIl5v6+0BE1LxMKmkecBhwQkSse6H1MLPy8iU5MzMzswLu9G1mZmZWwAmTmZmZWYFB9WHKc72cSbpH0zkRcVq99XvGjY/RE7YccNmYzTY0Wsdif6h1W6g6Xjm6teUVlLlh3Ziay+rGpNm6dFK9WNZRNyYrm7nTQ4e14Rja8PLaU0Y1FZN2HOdWLh3+rmymzJafN1Y663hkTURsU2+dwj5M+f5LfyDdVuB+0sRpc/M9kQY0duKU2O6YEwdcNnVWs9Og1BazVhSv1I8W1L7zRTPlFZW5bMFA/VWTejFpti6dVG+/66kXkylfGnDapK7SjmNo+ef2rLmsmZi0o45WLp3+rmymzFafN1Y+v4z5iyJiZr11BnNJbg/gTxFxb57k72LaODzXzMzMrNsMJmGaRN8bLN7PADcmlXSspIWSFj7zhJtAzczMbORoWafviDg7ImZGxMyecfVu3WVmZmY2vAwmYVpB3ztYT+b5O7mbmZmZjXiDSZhuAaZL2l7SGOBw4Ir2VsvMzMysexROKxART0v6V9KdunuA70XEkoK3tVyrR/SMhBFCtUaadMvIlZGuHTFp9YieTn9unRw52GysfJw3ph0xaXWZnR4J18mRg90y4tYGOQ9TRFwJXNnmupiZmZl1Jc/0bWZmZlbACZOZmZlZASdMZmZmZgWcMJmZmZkVcMJkZmZmVmBQo+SsO3mqhb6Gw01CRzpPtVBtONxo1sd5YzzVQrVWH0Odvrk5p8wvfL9bmMzMzMwKOGEyMzMzK+CEyczMzKyAEyYzMzOzAk6YzMzMzAo4YTIzMzMroIhoeaFjJ06J7Y45ccBl3TLs1cNo7YXq9LDXqbOWNlVmLd10nNeL5bIF0wZ8vV48umnf6qm337XUige0Zyj5cJiuox1TLVi5/DLmL4qImfXWcQuTmZmZWQEnTGZmZmYFnDCZmZmZFXDCZGZmZlbACZOZmZlZASdMZmZmZgVGtaPQMSvXd/0dxjs97LiTdxjvlqG+I91wiEmn6zgcjqHhUMdu0o6YtLrMTk8d0MmpFtoxRYmnWmiOW5jMzMzMCjhhMjMzMyvghMnMzMysgBMmMzMzswJOmMzMzMwKDGqUnKT7gHXAM8DTRTeos2rtGJXgkYN9DYebhI50wyEm3TRysFv4OG+MRw5Wa/Ux1Ombm3PK/ML3NzKtwJsjYk0D65uZmZmNCL4kZ2ZmZlZgsAlTAFdLWiTp2HZWyMzMzKzbDPaS3F4RsULSS4FrJN0dEddXrpATqWMBxjKuxdU0MzMzGzqDamGKiBX5/9XApcAeA6xzdkTMjIiZo3lxa2tpZmZmNoQKEyZJ4yVt1vsYOBC4o90VMzMzM+sWioj6K0g7kFqVIF3CuzAiTq33nrETp8R2x5w44LJW32gWumfIopVLp4e9dstUC1Yuw2G6jnrnzdRZSxsur9l6dFq9OC5bMK3msnoxGe77XU+9mNxzyomLiqZMKuzDFBH3Ars2XjUzMzOzkcHTCpiZmZkVcMJkZmZmVsAJk5mZmVkBJ0xmZmZmBZwwmZmZmRVo5Oa7gzZm5fqW3ym51UMdOz10stXDxaH20MpuGeo70rUjJt1+3hTp5FQL3TJFyUjXjpgM9zh3cqqF4R6rkcQtTGZmZmYFnDCZmZmZFXDCZGZmZlbACZOZmZlZASdMZmZmZgWcMJmZmZkVaMu0Alat1cPFwVMt9Dcc7qo+0nmqhWqtPoY6OUUJ+DgfiKdaqNbqY6heeUPFLUxmZmZmBZwwmZmZmRVwwmRmZmZWwAmTmZmZWQEnTGZmZmYFnDCZmZmZFVBEtLzQzbVlvF77tbzcWn7xwOKG33PQxBktLa+ozGbVq0ut7bWj/s3U44UY7p9pNx1DtTRbx3q65RhqdludjkmrjYTjvNXnfrM6fQw1833eLd+9MDw+03p+GfMXRcTMeuu4hcnMzMysgBMmMzMzswJOmMzMzMwKOGEyMzMzK+CEyczMzKyAEyYzMzOzAm2ZVkDSQ8DS/HRrYE3LNzK8OSbVHJNqjkk1x6Qvx6OaY1LNManWPybTImKbem9oS8LUZwPSwqK5DcrGManmmFRzTKo5Jn05HtUck2qOSbVmYuJLcmZmZmYFnDCZmZmZFehEwnR2B7Yx3Dgm1RyTao5JNcekL8ejmmNSzTGp1nBM2t6HyczMzGy48yU5MzMzswJtTZgkzZZ0j6Q/Sfp0O7fVrSR9T9JqSXdUvLalpGsk/TH//5KhrGMnSZoi6VpJd0paIumE/HqZYzJW0s2SfpdjclJ+fXtJN+Xz54eSxgx1XTtNUo+k2yT9LD8vdUwk3Sfp95IWS1qYXyvtuQMgaYKk+ZLulnSXpDeWOSaSdszHR++/xyR9pMwxAZD00fz9eoeki/L3bkPfJ21LmCT1AN8C5gA7A3Ml7dyu7XWx84DZ/V77NPCriJgO/Co/L4ungY9FxM7AG4AP5eOizDF5CpgVEbsCM4DZkt4AfBU4PSJeATwCvH/oqjhkTgDuqnjumMCbI2JGxZDoMp87AGcC/xsROwG7ko6X0sYkIu7Jx8cM4LXAE8CllDgmkiYBxwMzI2IXoAc4nAa/T9rZwrQH8KeIuDciNgAXA4e2cXtdKSKuBx7u9/KhwLz8eB5wWCfrNJQiYmVE3JofryN9uU2i3DGJiHg8Px2d/wUwC5ifXy9VTAAkTQbeCpyTn4uSx6SG0p47krYA9gHOBYiIDRGxlhLHpJ/9gD9HxFIck1HAJpJGAeOAlTT4fdLOhGkSsLzi+f35NYOXRcTK/HgV8LKhrMxQkbQdsBtwEyWPSb70tBhYDVwD/BlYGxFP51XKeP6cAXwSeDY/3wrHJICrJS2SdGx+rcznzvbAQ8D386XbcySNp9wxqXQ4cFF+XNqYRMQK4OvAMlKi9CiwiAa/T9zpe4hFGqZYuqGKkjYFfgx8JCIeq1xWxphExDO5CX0yqXV2p6Gt0dCSdDCwOiIWDXVdusxeEbE7qavDhyTtU7mwhOfOKGB34DsRsRuwnn6XmkoYEwByf5xDgB/1X1a2mOT+WoeSEuyJwHiqu8oUamfCtAKYUvF8cn7N4EFJLwfI/68e4vp0lKTRpGTpgoj4SX651DHplS8nXAu8EZiQm4+hfOfPm4BDJN1Hupw/i9RXpcwx6f1LmYhYTeqXsgflPnfuB+6PiJvy8/mkBKrMMek1B7g1Ih7Mz8sck/2Bv0TEQxGxEfgJ6Tumoe+TdiZMtwDTcy/0MaSmwSvauL3h5Arg6Pz4aODyIaxLR+V+KOcCd0XEf1QsKnNMtpE0IT/eBDiA1LfrWuDtebVSxSQiPhMRkyNiO9J3x4KIOJISx0TSeEmb9T4GDgTuoMTnTkSsApZL2jG/tB9wJyWOSYW5PH85Dsodk2XAGySNy79BvcdJQ98nbZ24UtJbSP0QeoDvRcSpbdtYl5J0EbAv6c7IDwJfAC4DLgGmAkuBd0ZE/47hI5KkvYAbgN/zfN+Uz5L6MZU1Jq8hdTjsIf0Rc0lEnCxpB1LrypbAbcC7I+Kpoavp0JC0L/DxiDi4zDHJ+35pfjoKuDAiTpW0FSU9dwAkzSANDBgD3Au8j3weUd6YjCclCTtExKP5tbIfJycB7yKN1L4NOIbUZ2nQ3yee6dvMzMysgDt9m5mZmRVwwmRmZmZWwAmTmZmZWQEnTGZmZmYFnDCZmZmZFXDCZGZmZlbACZOZmZlZASdMZmZmZgX+H+K9BTAbVYUFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "            GameTestFullDataset(raw_data=game_data, debug=True), \n",
    "            batch_size=hparams['batch_size'], shuffle=False\n",
    "        )\n",
    "\n",
    "run_test('last.ckpt', project_name, '2613upgc', gt, test_loader, trainmodule, game_datamodule, ckpt_dir_PATH, hparams, wd_logger, gpu=0, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-norfolk",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
