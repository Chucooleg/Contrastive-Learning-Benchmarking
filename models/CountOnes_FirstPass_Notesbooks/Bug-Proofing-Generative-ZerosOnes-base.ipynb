{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import json\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-playback",
   "metadata": {},
   "source": [
    "## Data -- Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_strings_data(str_len=5):\n",
    "    strings = []\n",
    "    for i in range(2**str_len):\n",
    "        bit_repr = \"{0:b}\".format(i).zfill(5)\n",
    "        ones = bit_repr.count('1')\n",
    "        strings.append((bit_repr, ones))\n",
    "        \n",
    "    idx_to_string = {i:string for i,string in enumerate(strings)} \n",
    "    string_to_idx = {string:i for i,string in enumerate(strings)}\n",
    "    idx_to_key, idx_to_query = idx_to_string, idx_to_string\n",
    "    key_to_idx, query_to_idx = string_to_idx, string_to_idx\n",
    "    \n",
    "    query_to_keys = {}\n",
    "    for i in range(len(idx_to_key)):\n",
    "        for j in range(len(idx_to_query)):\n",
    "            if idx_to_string[i][1] == idx_to_string[j][1]: # match\n",
    "                if j not in query_to_keys: # start new list\n",
    "                    query_to_keys[j] = {}\n",
    "                query_to_keys[j][i] = idx_to_string[j][1] # idx, num matches\n",
    "    \n",
    "    data = {\n",
    "        'idx_to_key': idx_to_key,\n",
    "        'key_to_idx': key_to_idx,\n",
    "        'idx_to_query': idx_to_query,\n",
    "        'query_to_idx': query_to_idx,\n",
    "        'query_to_keys': query_to_keys,\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "string_length = 6\n",
    "game_data = generate_strings_data(string_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-nomination",
   "metadata": {},
   "source": [
    "## Data -- Distributions, Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_table = np.zeros((2**string_length, 2**string_length))\n",
    "\n",
    "for i in range(2**string_length):\n",
    "    for j in range(2**string_length): \n",
    "        count_table[i][j] = int(game_data['idx_to_key'][i][1] == game_data['idx_to_query'][j][1])\n",
    "        \n",
    "xy = count_table/np.sum(count_table)\n",
    "# xy += 1e-9\n",
    "xy /= np.sum(xy)\n",
    "\n",
    "x = np.sum(xy,0)\n",
    "y = np.sum(xy,1)\n",
    "xyind = y[None].T @ x[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list(xy.reshape(-1))).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(count_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank((xy/xyind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow((xy)[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow((xyind)[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow((xy/xyind)[:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list((xy/xyind).reshape(-1))).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-sector",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One embed per query!\n",
    "\n",
    "class GameDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by gen_card_data.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        # y\n",
    "        self.query_support_size = len(self.raw_data['idx_to_query'])\n",
    "        # x\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.query_support_size * self.key_support_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: (xy_i) * (xy.shape[1]=self.query_support_size) + (xy_j)\n",
    "        '''\n",
    "        x_i, y_j = idx//self.query_support_size, idx%self.query_support_size\n",
    "        all_matches = list(self.raw_data['query_to_keys'].get(y_j, {}).keys())\n",
    "        gt = np.zeros(self.key_support_size)\n",
    "        gt[all_matches] = 1.0\n",
    "        \n",
    "        if self.debug:\n",
    "            print('query\\n', y_j, self.raw_data['idx_to_query'][y_j])\n",
    "            print('key\\n', x_i, self.raw_data['idx_to_key'][x_i])\n",
    "            print('all matches \\n', [self.raw_data['idx_to_key'][m] for m in all_matches])\n",
    "        \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        )    \n",
    "\n",
    "game_dataset = GameDataset(raw_data=game_data, debug=True)\n",
    "game_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameTestFullDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by gen_card_data.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        # y\n",
    "        self.query_support_size = len(self.raw_data['idx_to_query'])\n",
    "        # x\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data['idx_to_query'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: int. 0 to query_support_size-1\n",
    "        '''\n",
    "        y_j = idx\n",
    "        x_i = torch.empty(1) # just a meaningless value\n",
    "        all_matches = list(self.raw_data['query_to_keys'].get(y_j, {}).keys())\n",
    "        gt = np.zeros(self.key_support_size)\n",
    "        gt[all_matches] = 1.0\n",
    "\n",
    "        if self.debug:\n",
    "            print('query\\n', y_j, self.raw_data['idx_to_query'][y_j])\n",
    "            print('all matches \\n', [self.raw_data['idx_to_key'][m] for m in all_matches])\n",
    "        \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        ) \n",
    "    \n",
    "game_testdataset = GameTestFullDataset(raw_data=game_data, debug=True)\n",
    "game_testdataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size, raw_data, seen_train_xy, seen_val_xy, debug=False):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = GameDataset(raw_data=raw_data, debug=debug)\n",
    "        self.testdataset = GameTestFullDataset(raw_data=raw_data, debug=debug)\n",
    "        self.seen_train_xy = seen_train_xy\n",
    "        self.seen_val_xy = seen_val_xy\n",
    "        self.setup_samplers()\n",
    "        \n",
    "    def setup_samplers(self):\n",
    "        self.train_sampler = WeightedRandomSampler(\n",
    "            weights=self.seen_train_xy.reshape(-1), num_samples=self.batch_size, replacement=True\n",
    "        ) \n",
    "        self.val_sampler = WeightedRandomSampler(\n",
    "            weights=self.seen_val_xy.reshape(-1), num_samples=self.batch_size, replacement=True\n",
    "        )\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train = self.dataset\n",
    "            self.val = self.dataset\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test = self.testdataset\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=False, sampler=self.train_sampler\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val, batch_size=self.batch_size,  shuffle=False, sampler=self.val_sampler\n",
    "        )\n",
    "        return val_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(\n",
    "            self.test, batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "        return test_loader  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-research",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lightning Module\n",
    "# https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=UIXLW8CO-W8w\n",
    "\n",
    "class TrainModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams, gt_distributions, raw_data):\n",
    "        '''\n",
    "        hparams: dictionary of hyperparams\n",
    "        gt_distributions: dictionary that stores the groundtruth 'xy', 'xyind' distributions.\n",
    "                         each is a key_support_size by query_support_size matrix that sums up to 1.0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.debug = hparams['debug']\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = construct_full_model(hparams)\n",
    "        self.loss_criterion = InfoCELoss(temperature_const=self.hparams['loss_temperature_const'])\n",
    "        self.metrics = ThresholdedMetrics(raw_data=raw_data)\n",
    "\n",
    "        self.key_support_size = self.hparams['key_support_size']\n",
    "        self.query_support_size = self.hparams['query_support_size']\n",
    "        \n",
    "        # for pulling model p(x,y) and p(x,y)/[pxpy]\n",
    "        self.populate_logits_matrix = hparams['populate_logits_matrix']\n",
    "        if self.populate_logits_matrix:\n",
    "            self.register_buffer(\n",
    "                name='model_logits_matrix',\n",
    "                tensor= torch.zeros(hparams['key_support_size'], hparams['query_support_size'])\n",
    "            )\n",
    "            self.setup_gt_distributions(gt_distributions)\n",
    "        \n",
    "    def log_metrics(self, metrics_dict):\n",
    "        for k, v in metrics_dict.items():\n",
    "            self.log(k, v)\n",
    "            \n",
    "    def get_max_memory_alloc(self):\n",
    "        devices_max_memory_alloc = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.device(f'cuda:{i}')\n",
    "            devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            torch.cuda.reset_max_memory_allocated(device)\n",
    "        return devices_max_memory_alloc\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def forward(self, X_query, X_key, X_keys, test_bool, debug=False):\n",
    "        '''\n",
    "        X_query: (b, 1)\n",
    "        X_key: (b, 1)\n",
    "        X_keys: (b, key_support_size) 1s and 0s.\n",
    "        test_bool: boolean.\n",
    "        '''\n",
    "        batch_size = X_query.shape[0]\n",
    "        \n",
    "        # ToDo batch_size_b\n",
    "        \n",
    "        # shape (b,support) if test_bool else (b, b)\n",
    "        logits = self.model(X_query, X_key, test_bool, debug=debug)\n",
    "        # scalar\n",
    "        loss = None if test_bool else self.loss_criterion(logits, debug=debug)\n",
    "        # scalar\n",
    "        metrics = self.metrics(\n",
    "            logits=logits, X_keys=X_keys, X_query=X_query, \n",
    "            debug=debug, breakdown_errors_bool=True, \n",
    "        ) if test_bool else None\n",
    "\n",
    "        return logits, loss, metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def setup_gt_distributions(self, gt_distributions):\n",
    "        '''called once during init to setup groundtruth distributions'''\n",
    "        assert gt_distributions['xy'].shape == gt_distributions['xyind'].shape\n",
    "        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy',\n",
    "            tensor= torch.tensor(gt_distributions['xy'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xyind',\n",
    "            tensor= torch.tensor(gt_distributions['xyind'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy_div_xyind',\n",
    "            tensor= self.gt_xy/self.gt_xyind\n",
    "        )\n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='one',\n",
    "            tensor= torch.tensor([1.0])\n",
    "        )   \n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='gt_mi',\n",
    "            tensor= self.compute_mutual_information(self.gt_xy, self.gt_xy_div_xyind)\n",
    "        ) \n",
    "   \n",
    "    \n",
    "    def populate_model_logits_matrix(self, query_idx, logits):\n",
    "        '''\n",
    "        query_idx: shape (b,)\n",
    "        logits: shape(b, key_support_size)\n",
    "        '''  \n",
    "        assert query_idx.shape[0] == logits.shape[0]\n",
    "        b = query_idx.shape[0]\n",
    "        assert logits.shape[1] == self.key_support_size\n",
    "        for i in range(b):\n",
    "            self.model_logits_matrix[:,query_idx[i]] = logits[i]\n",
    "    \n",
    "    def compute_mutual_information(self, xy, xy_div_xyind):\n",
    "        '''\n",
    "        xy: p(xy). shape(b, key_support_size)\n",
    "        xy_div_xyind_hat: p(xy)/[p(x)(y)].\n",
    "                          shape(b, key_support_size)\n",
    "        '''\n",
    "        assert torch.isclose(torch.sum(xy), self.one.type_as(xy))\n",
    "        assert xy.shape == xy_div_xyind.shape == (\n",
    "            self.key_support_size, self.query_support_size\n",
    "        )\n",
    "        pmi = torch.log(xy_div_xyind)\n",
    "        mi = torch.sum(xy * pmi)\n",
    "        return mi\n",
    "    \n",
    "    def pull_model_distribution(self, debug=True):\n",
    "\n",
    "        # sanity check\n",
    "        sum_logits = torch.sum(self.model_logits_matrix)\n",
    "        assert sum_logits != 0.0\n",
    "        \n",
    "        if debug:\n",
    "            print('Sum of model logits matrix\\n', sum_logits)\n",
    "            print('Number of model logits with zero value\\n', torch.sum(self.model_logits_matrix == 0.0)) \n",
    "            print('Variance of model logits\\n', torch.var(self.model_logits_matrix))\n",
    "        \n",
    "        # estimate the full distribution\n",
    "        # hat( k * pxy/(pxpy)\n",
    "        f = torch.exp(self.model_logits_matrix)\n",
    "        # hat( k * pxy)\n",
    "        xy_hat = f * self.gt_xyind\n",
    "        # hat( pxy)\n",
    "        xy_hat = (xy_hat / torch.sum(xy_hat))\n",
    "        \n",
    "        # estimate exp(pmi)\n",
    "        # hat(k)\n",
    "        k_hat = torch.sum(f) / torch.sum(self.gt_xy_div_xyind)\n",
    "        # hat(pxy/(pxpy)\n",
    "        xy_div_xyind_hat = (f / k_hat)\n",
    "        \n",
    "        # estimate MI\n",
    "        # scalar\n",
    "        mi_hat = self.compute_mutual_information(xy_hat, xy_div_xyind_hat)\n",
    "        # scalar\n",
    "        mi_gt_minus_hat = self.gt_mi - mi_hat\n",
    "        \n",
    "        # estimate KL divergence\n",
    "        kl_div_val = F.kl_div(torch.log(xy_hat), self.gt_xy)\n",
    "\n",
    "        # estimate ranks\n",
    "        xy_hat = xy_hat.detach().cpu().numpy()\n",
    "        xy_div_xyind_hat = xy_div_xyind_hat.detach().cpu().numpy()\n",
    "        # hat(pxy rank)\n",
    "        xy_hat_rank = np.linalg.matrix_rank(xy_hat)\n",
    "        # hat(pxy/(pxpy rank)\n",
    "        xy_div_xyind_hat_rank = np.linalg.matrix_rank(xy_div_xyind_hat) \n",
    "        \n",
    "        pulled_distribution_results = {\n",
    "            'xy_hat':xy_hat,\n",
    "            'xy_div_xyind_hat':xy_div_xyind_hat,\n",
    "            'xy_hat_rank':xy_hat_rank,\n",
    "            'xy_div_xyind_hat_rank':xy_div_xyind_hat_rank,\n",
    "            'mi_hat':mi_hat,\n",
    "            'mi_gt_minus_hat':mi_gt_minus_hat,\n",
    "            'kl_div':kl_div_val\n",
    "        }\n",
    "        \n",
    "        return pulled_distribution_results\n",
    "\n",
    "    ###################################################\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        # scalar\n",
    "        _, loss, _ = self(X_query, X_key, None, test_bool=False, debug=self.debug)\n",
    "        # dict\n",
    "        _, _, metrics = self(X_query, None, X_keys, test_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('train step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\nX_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:\\n', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {**{'train_loss': loss}, **{'train_'+m:metrics[m] for m in metrics}}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        _, loss, _ = self(X_query, X_key, None, test_bool=False, debug=self.debug)\n",
    "        _, _, metrics = self(X_query, None, X_keys, test_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('validation step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\X_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "            \n",
    "        # log \n",
    "        step_metrics = {**{'val_loss': loss}, **{'val_'+m:metrics[m] for m in metrics}}\n",
    "        devices_max_memory_alloc = self.get_max_memory_alloc()\n",
    "        for device, val in devices_max_memory_alloc.items():\n",
    "            step_metrics[f'step_max_memory_alloc_cuda:{device}'] = val\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        # (b,1), (b,1), _, (b, support size)\n",
    "        query_idx, X_query, _, X_keys = batch\n",
    "        \n",
    "        # compute scores for all keys\n",
    "        # shape(b, key_support_size), _, dictionary\n",
    "        logits, _, metrics = self(X_query, None, X_keys, test_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.populate_logits_matrix:\n",
    "            self.populate_model_logits_matrix(query_idx, logits)\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {'test_'+m:metrics[m] for m in metrics}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics \n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def aggregate_metrics_at_epoch_end(self, outputs):\n",
    "        # log metrics\n",
    "        epoch_metrics = {}\n",
    "        metric_names = outputs[0].keys()\n",
    "        for m in metric_names:\n",
    "            if not ('max_memory_alloc_cuda' in m or 'count' in m or 'rate' in m):\n",
    "                epoch_metrics['avg_'+m] = torch.stack([x[m] for x in outputs]).mean()\n",
    "            elif 'count' in m or 'rate' in m:\n",
    "                epoch_metrics['avg_'+m] = np.mean([x[m] for x in outputs])\n",
    "        self.log_metrics(epoch_metrics)\n",
    "        return epoch_metrics        \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        return averaged_metrics\n",
    "    \n",
    "    def test_epoch_end(self, outputs):        \n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        return averaged_metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.hparams['lr'],\n",
    "            betas=(\n",
    "                self.hparams['adam_beta1'], self.hparams['adam_beta2']),\n",
    "            eps=self.hparams['adam_epsilon'],\n",
    "            weight_decay=self.hparams['adam_weight_decay']\n",
    "        )\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-residence",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-request",
   "metadata": {},
   "source": [
    "## Loss, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdedMetrics(nn.Module):\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        '''\n",
    "        tot_k: total number of candidates. e.g. 81 cards\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        self.threshold = 1.0 / (self.key_support_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def breakdown_errors(self, X_query, corrects):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        corrects: shape (b, support size)\n",
    "        '''\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        assert corrects.shape == (b, self.key_support_size)\n",
    "        \n",
    "        X_query_list = X_query.squeeze(-1)\n",
    "        wrongs = (1 - corrects).cpu().numpy()\n",
    "        \n",
    "        num_matched_concepts = [\n",
    "             self.raw_data['query_to_keys'].get(X_query_list[batch_i].item(), {}).get(card_idx, 0)\n",
    "            for batch_i in range(b) for card_idx in range(self.key_support_size)  \n",
    "        ]\n",
    "        \n",
    "        num_matched_concepts = np.array(num_matched_concepts).reshape(b, self.key_support_size)\n",
    "        assert num_matched_concepts.shape == wrongs.shape\n",
    "            \n",
    "        error_count_by_num_matched_concepts = \\\n",
    "            Counter((num_matched_concepts * wrongs).reshape(-1).tolist())\n",
    "        \n",
    "        total_count_by_num_matched_concepts = \\\n",
    "            Counter((num_matched_concepts).reshape(-1).tolist())        \n",
    "        \n",
    "        error_counts = {f'error_rate_for_{k}_matched_concepts':error_count_by_num_matched_concepts[k]/total_count_by_num_matched_concepts[k] for k in error_count_by_num_matched_concepts.keys() if k!=0}\n",
    "        total_counts = {f'total_count_for_{k}_matched_concepts':total_count_by_num_matched_concepts[k] for k in total_count_by_num_matched_concepts.keys() if k!=0}\n",
    "        \n",
    "        return {**error_counts, **total_counts}\n",
    "        \n",
    "    def forward(self, X_query, logits, X_keys, breakdown_errors_bool=False, debug=False):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        logits: shape (b, support size)\n",
    "        X_keys: shape (b, support size). value 1.0 at where card matches. value 0 otherwise.\n",
    "        '''\n",
    "        b, key_support_size = logits.shape\n",
    "        assert key_support_size == self.key_support_size\n",
    "        assert logits.shape == X_keys.shape\n",
    "        \n",
    "        # model predictions, shape (b, support size)\n",
    "        binary_predictions = (self.softmax(logits) >= self.threshold).type(torch.float)\n",
    "        # ground truth, shape (b, support size)\n",
    "        gt = X_keys\n",
    "        # correct predictions, shape (b, support size)\n",
    "        corrects = (binary_predictions == gt).type(torch.float)\n",
    "        \n",
    "        # accuracy, computed per query, average across queries\n",
    "        # (b,)\n",
    "        accuracy_row = torch.sum(corrects, dim=1) / key_support_size\n",
    "        # scalar\n",
    "        accuracy_meanrows = torch.mean(accuracy_row)\n",
    "        # accuracy, computed per query-key, average across all\n",
    "        accuracy_all = torch.sum(corrects) / (b * key_support_size)\n",
    "        \n",
    "        # precision, computed per query, average across queries\n",
    "        # (b,)\n",
    "        precision_row = torch.sum((corrects * binary_predictions), dim=1) / torch.sum(binary_predictions, dim=1)\n",
    "        # scalar\n",
    "        precision_meanrows = torch.mean(precision_row)\n",
    "        # precision, computed per query-key, average across all\n",
    "        precision_all = torch.sum((corrects * binary_predictions)) / torch.sum(binary_predictions)\n",
    "        \n",
    "        \n",
    "        # recall, computed per query, average across queries\n",
    "        # (b,)\n",
    "        recall_row = torch.sum((corrects * gt), dim=1) / torch.sum(gt, dim=1)\n",
    "        # scalar\n",
    "        recall_meanrows = torch.mean(recall_row)\n",
    "        # recall, computed per query-key, average across all\n",
    "        recall_all = torch.sum((corrects * gt)) / torch.sum(gt)\n",
    "        \n",
    "        # f1, computed per query, average across queries\n",
    "        # (b,)\n",
    "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row)\n",
    "        # scalar\n",
    "        f1_meanrows = torch.mean(f1_row)\n",
    "        # f1, computed per query-key, average across all\n",
    "        f1_all = (precision_all * recall_all) / (precision_all + recall_all)\n",
    "        \n",
    "        if breakdown_errors_bool:\n",
    "            error_breakdown_by_num_matched_concepts = self.breakdown_errors(X_query, corrects)\n",
    "        else:\n",
    "            error_breakdown_by_num_matched_concepts = {} \n",
    "            \n",
    "        if debug:\n",
    "            print('####################################################')\n",
    "            print('Metrics Per Query:')\n",
    "            print('accuracy_rows', accuracy_row)\n",
    "            print('precision_row', precision_row)\n",
    "            print('recall_row', recall_row)\n",
    "            print('f1_row', f1_row)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across Queries')\n",
    "            print('accuracy_meanrows', accuracy_meanrows)\n",
    "            print('precision_meanrows', precision_meanrows)\n",
    "            print('recall_meanrows', recall_meanrows)\n",
    "            print('f1_meanrows', f1_meanrows)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across All Query-Key Pairs:')\n",
    "            print('accuracy_all', accuracy_all)\n",
    "            print('precision_all', precision_all)\n",
    "            print('recall_all', recall_all)\n",
    "            print('f1_all', f1_all)\n",
    "            print('####################################################')\n",
    "            print('error_breakdown', error_breakdown_by_num_matched_concepts)\n",
    "            \n",
    "        metrics = {\n",
    "            'accuracy_by_Query': accuracy_meanrows,\n",
    "            'precision_by_Query': precision_meanrows,\n",
    "            'recall_by_Query': recall_meanrows,\n",
    "            'f1_by_Query': f1_meanrows,\n",
    "            'accuracy_by_QueryKey': accuracy_all,\n",
    "            'precision_by_QueryKey': precision_all,\n",
    "            'recall_by_QueryKey': recall_all,\n",
    "            'f1_by__QueryKey': f1_all\n",
    "        }\n",
    "        metrics = {\n",
    "            **metrics, **error_breakdown_by_num_matched_concepts\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def test_metric_module():\n",
    "    key_support_size=3\n",
    "    thresh=1./key_support_size\n",
    "\n",
    "    m_f = ThresholdedMetrics(raw_data=game_data)\n",
    "    m_f.key_support_size = 3\n",
    "\n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [0.4, 0.2, 0.4],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    \n",
    "    \n",
    "# test_metric_module()\n",
    "\n",
    "# reference output\n",
    "# accuracy_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.5000, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.5556), 'precision': tensor(0.5556), 'recall': tensor(1.), 'f1': tensor(0.6667)}\n",
    "# ----------------------------------------------\n",
    "# accuracy_row tensor([0.6667, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.5000, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.6667, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.6667), 'precision': tensor(0.6111), 'recall': tensor(1.), 'f1': tensor(0.7222)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-cause",
   "metadata": {},
   "source": [
    "## hparams, init train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B References\n",
    "# https://docs.wandb.com/library/integrations/lightning\n",
    "# colab example\n",
    "# https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Supercharge_your_Training_with_Pytorch_Lightning_%2B_Weights_%26_Biases.ipynb\n",
    "# step by step guide\n",
    "# https://wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-Biases--Vmlldzo2NjQ1Mw\n",
    "\n",
    "\n",
    "# Distributed Weighted Sampler\n",
    "# https://discuss.pytorch.org/t/how-to-use-my-own-sampler-when-i-already-use-distributedsampler/62143/8\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/discussions/3716#discussioncomment-238296\n",
    "\n",
    "# torch lightning -- how sampler is added or removed\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/multi_gpu.html?highlight=sampler#remove-samplers\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    'batch_size': 128,\n",
    "    # Arch\n",
    "    'key_support_size': len(game_data['idx_to_key']),\n",
    "    'query_support_size': len(game_data['idx_to_query']),\n",
    "    # embedding\n",
    "    'd_model': 128,\n",
    "    'embed_dropout': 0.0,\n",
    "    # final prediction\n",
    "    'dotproduct_bottleneck':False,\n",
    "    # loss\n",
    "    'loss_temperature_const': 1.0,\n",
    "    # optimizer\n",
    "    'lr': 0.001,\n",
    "    'adam_beta1': 0.9,\n",
    "    'adam_beta2': 0.999,\n",
    "    'adam_epsilon': 1e-08,\n",
    "    'warmup_steps': 12000,\n",
    "    'adam_weight_decay':0,\n",
    "    'gradient_clip_val': 0,\n",
    "    # others\n",
    "    'debug':False,\n",
    "    'populate_logits_matrix': True\n",
    "}\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# model\n",
    "trainmodule =  TrainModule(hparams, gt_distributions={'xy':xy, 'xyind':xyind}, raw_data=game_data)\n",
    "model_summary = pl.core.memory.ModelSummary(trainmodule, mode='full')\n",
    "print(model_summary,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-graduate",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "game_datamodule = GameDataModule(\n",
    "    batch_size = hparams['batch_size'],\n",
    "    raw_data = game_data,\n",
    "    seen_train_xy = xy,\n",
    "    seen_val_xy = xy,\n",
    "    debug=hparams['debug']\n",
    ")\n",
    "\n",
    "# logger\n",
    "run_name = f'Seq2Seq-CountOnes-6digits'\n",
    "project_name = 'ContrastiveLearning-cardgame-Scaling'\n",
    "\n",
    "wd_logger = WandbLogger(name=run_name, project=project_name)\n",
    "print('RUN NAME :\\n', run_name)\n",
    "\n",
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[0], \n",
    "    min_epochs=2, max_epochs=6000, \n",
    "    precision=32, \n",
    "    logger=wd_logger,\n",
    "    log_gpu_memory='all',\n",
    "    weights_summary = 'full',\n",
    "    gradient_clip_val=hparams['gradient_clip_val'],\n",
    "    replace_sampler_ddp=False,\n",
    "    # track_grad_norm\n",
    ")\n",
    "\n",
    "# fit\n",
    "with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(trainmodule, game_datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-cinema",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distribution_res = trainmodule.pull_model_distribution(debug=True)\n",
    "print('xy_hat_rank:', model_distribution_res['xy_hat_rank'])\n",
    "print('xy_div_xyind_hat_rank:', model_distribution_res['xy_div_xyind_hat_rank'])\n",
    "print('mi_hat:', model_distribution_res['mi_hat'])\n",
    "print('mi_gt_minus_hat:', model_distribution_res['mi_gt_minus_hat'])\n",
    "print('kl_div:', model_distribution_res['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(model_distribution_res['xy_hat'][:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(xy[:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(model_distribution_res['xy_div_xyind_hat'][:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow((xy/xyind)[:, :300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
