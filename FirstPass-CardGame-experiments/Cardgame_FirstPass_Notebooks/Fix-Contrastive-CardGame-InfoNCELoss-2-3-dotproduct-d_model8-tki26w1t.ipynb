{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corresponding-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchucooleg\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import json\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-registration",
   "metadata": {},
   "source": [
    "## Data -- Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informative-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9 unqiue cards\n",
      "Generated 36 cardpairs\n",
      "-- 18 cardpairs with shared concept\n",
      "-- 18 cardpairs without shared concept\n",
      "[(1, 18), (0, 18)]\n",
      "Total number of matches =  54\n",
      "Number of shared concepts per match =  [(1, 54)]\n"
     ]
    }
   ],
   "source": [
    "def generate_cards(attributes, attr_order, num_attributes, num_attr_vals):\n",
    "    \n",
    "    attr_order = attr_order[:num_attributes]\n",
    "    attributes = {att_k:attributes[att_k][:num_attr_vals] for att_k in attributes if att_k in attr_order}\n",
    "    \n",
    "    cards = []\n",
    "    idx_to_card = {}\n",
    "    card_to_idx = {}\n",
    "\n",
    "    i = 0\n",
    "    attr_vals = [attributes[attr] for attr in attr_order]\n",
    "    for combo in itertools.product(*attr_vals):\n",
    "        card = tuple(combo)\n",
    "        cards.append(card)\n",
    "        card_to_idx[card] = i\n",
    "        idx_to_card[i] = card\n",
    "        i += 1\n",
    "    \n",
    "    assert len(cards) == len(set(cards))\n",
    "    print(f'Generated {len(cards)} unqiue cards')                \n",
    "    return cards, idx_to_card, card_to_idx\n",
    "\n",
    "\n",
    "def num_shared_attributes(card1, card2):\n",
    "    matching_concepts = tuple(s1 if s1==s2 else '-' for s1,s2 in zip(card1,card2))\n",
    "    num_matching_concepts = len([c for c in matching_concepts if c != '-'])\n",
    "    return matching_concepts, num_matching_concepts\n",
    "\n",
    "\n",
    "def generate_card_pairs(cards, card_to_idx):\n",
    "    '''\n",
    "    find all combos of cards, filter down to the ones that share concepts.\n",
    "    '''\n",
    "    cardpairs_with_shared_concepts = []\n",
    "    cardpairs_without_shared_concepts = []\n",
    "\n",
    "    cardpair_to_idx, idx_to_cardpair, idx = {}, {}, 0\n",
    "    num_matching_concepts_all = {}\n",
    "    for card1, card2 in itertools.combinations(cards,2):\n",
    "        if card1 != card2:\n",
    "            matching_concepts, num_matching_concepts = num_shared_attributes(card1, card2)\n",
    "            num_matching_concepts_all[(card1, card2)] = num_matching_concepts\n",
    "            if num_matching_concepts:\n",
    "                cardpairs_with_shared_concepts.append(((card_to_idx[card1], card_to_idx[card2]), matching_concepts))\n",
    "            else:\n",
    "                cardpairs_without_shared_concepts.append(((card_to_idx[card1], card_to_idx[card2]), matching_concepts))\n",
    "            idx_to_cardpair[idx] = (card_to_idx[card1], card_to_idx[card2])\n",
    "            cardpair_to_idx[(card_to_idx[card1], card_to_idx[card2])] = idx\n",
    "            idx += 1\n",
    "    print(f'Generated {len(cardpairs_with_shared_concepts) + len(cardpairs_without_shared_concepts)} cardpairs')\n",
    "    print(f'-- {len(cardpairs_with_shared_concepts)} cardpairs with shared concept')\n",
    "    print(f'-- {len(cardpairs_without_shared_concepts)} cardpairs without shared concept')\n",
    "    print(Counter(num_matching_concepts_all.values()).most_common())\n",
    "    return cardpairs_with_shared_concepts, cardpairs_without_shared_concepts, cardpair_to_idx, idx_to_cardpair\n",
    "\n",
    "\n",
    "def match_concept_to_card(concept, card):\n",
    "    '''\n",
    "    Given a concept, determine if card matches.\n",
    "    \n",
    "    Arguments:\n",
    "        concept: ('red', 'void', '-', '-')\n",
    "        card: ex1. ('red', 'void', 'triangle', 'XOX')\n",
    "              ex2. ('green', 'void', 'square', 'OXX')\n",
    "    Returns:\n",
    "        match: bool. ex1. True,\n",
    "                     ex2. False\n",
    "    '''\n",
    "    match = 0\n",
    "    for ct, cd in zip(concept, card):\n",
    "        # As long as one concept matches, it is a match!\n",
    "        if ct == cd:\n",
    "            match += 1\n",
    "    return match\n",
    "\n",
    "\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=100, debug=False):\n",
    "    \n",
    "    # all cards\n",
    "    cards, idx_to_card, card_to_idx = generate_cards(attributes, attr_order, num_attributes, num_attr_vals)\n",
    "    # all card pairs\n",
    "    cardpairs_with_shared_concepts, cardpairs_without_shared_concepts, cardpair_to_idx, idx_to_cardpair = \\\n",
    "        generate_card_pairs(cards, card_to_idx)\n",
    "    \n",
    "    # generate answers\n",
    "    all_matches = []\n",
    "    number_of_shared_concepts_per_match = []\n",
    "    cardpair_to_matches = defaultdict(lambda : defaultdict(int))\n",
    "#     for cardpair in cardpairs_with_shared_concepts + cardpairs_without_shared_concepts: # NOTE!!\n",
    "    for cardpair in cardpairs_with_shared_concepts:\n",
    "        shared_concept = cardpair[1]\n",
    "        # look for all matching cards\n",
    "        # ((card pair query), matching card)\n",
    "        for card in cards:\n",
    "            num_matched_concepts = match_concept_to_card(shared_concept, card)\n",
    "            if num_matched_concepts:\n",
    "                all_matches.append((cardpair_to_idx[cardpair[0]], card_to_idx[card]))\n",
    "                cardpair_to_matches[cardpair_to_idx[cardpair[0]]][card_to_idx[card]] = num_matched_concepts\n",
    "                number_of_shared_concepts_per_match.append(num_matched_concepts)\n",
    "    cardpair_to_matches = default_to_regular(cardpair_to_matches)\n",
    "    print('Total number of matches = ', len(all_matches))\n",
    "    print('Number of shared concepts per match = ', Counter(number_of_shared_concepts_per_match).most_common())\n",
    "    \n",
    "    # hold out some cardpairs\n",
    "    unseen_cardpair_indices = list(np.random.choice(len(cardpair_to_idx), size=num_unseen_cardpairs, replace=False))\n",
    "    train_cardpair_indices = [idx for idx in range(len(cardpair_to_idx)) if idx not in unseen_cardpair_indices]\n",
    "    assert len(set(unseen_cardpair_indices) | set(train_cardpair_indices)) == len(cardpair_to_idx)\n",
    "    \n",
    "    data = {\n",
    "        'idx_to_key': idx_to_card,\n",
    "        'key_to_idx': card_to_idx,\n",
    "        'query_to_idx': cardpair_to_idx, \n",
    "        'idx_to_query': idx_to_cardpair,\n",
    "        'all_matches': all_matches, # list of tuples (query idx, answer card)\n",
    "        'query_to_keys': cardpair_to_matches, # lookup query idx:{'card1':num matched concepts, 'card2':num matched concepts,...}\n",
    "        'unseen_query_indices': unseen_cardpair_indices,\n",
    "        'seen_query_indices': train_cardpair_indices\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "attributes = {\n",
    "    'color': ['red', 'green', 'blue', 'orange', 'cyan', 'magenta', 'black', 'yellow'],\n",
    "    'fill': ['void', 'dashed', 'solid', 'checkered', 'dotted', 'mosaic', 'noise', 'brushed'],\n",
    "    'shape': ['square', 'circle', 'triangle', 'star', 'hexagon', 'pentagon', 'ellipse', 'rectangle'],\n",
    "    'config': ['OOO', 'OOX', 'OXO', 'OXX', 'XOO', 'XOX', 'XXO', 'XXX']\n",
    "}\n",
    "\n",
    "attr_order = ['color', 'fill', 'shape', 'config']\n",
    "\n",
    "num_attributes = 2\n",
    "num_attr_vals = 3\n",
    "game_data = gen_card_data(attributes, attr_order, num_attributes, num_attr_vals, num_unseen_cardpairs=0, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-atlantic",
   "metadata": {},
   "source": [
    "## Data -- Distribution, Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eligible-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_epsilon = 1e-100\n",
    "\n",
    "count_table = np.zeros((len(game_data['idx_to_key']), len(game_data['idx_to_query'])))\n",
    "        \n",
    "for q, k in game_data['all_matches']:\n",
    "    count_table[k, q] += 1    \n",
    "        \n",
    "xy = count_table/np.sum(count_table)\n",
    "xy += distribution_epsilon\n",
    "xy /= np.sum(xy)\n",
    "\n",
    "x = np.sum(xy,0)\n",
    "y = np.sum(xy,1)\n",
    "xyind = y[None].T @ x[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-100, 270), (0.018518518518518517, 54)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 0.1\n",
    "[(0.0029940119760479044, 270), (0.003548458638278998, 54)]\n",
    "# + nothing\n",
    "Counter(list(xy.reshape(-1))).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exterior-patch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(count_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "irish-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_rank= 5\n",
      "xy_div_xyind_rank= 5\n"
     ]
    }
   ],
   "source": [
    "xy_rank = np.linalg.matrix_rank(xy)\n",
    "print('xy_rank=',xy_rank)\n",
    "\n",
    "xy_div_xyind_rank = np.linalg.matrix_rank((xy/xyind))\n",
    "print('xy_div_xyind_rank=',xy_div_xyind_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quality-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (20,15)\n",
    "# figrange = (fx_s, fx_e, fy_s, fy_e)\n",
    "figrange = (0, len(game_data['idx_to_key']), 0, len(game_data['idx_to_query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fresh-southeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9784d42e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xy)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authentic-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9783f9550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "textile-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9783be3d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xy/xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comfortable-consideration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd97831fdf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xy/xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "british-extraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 162), (1.6200000000000002e-98, 108), (3.0, 54)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list((xy/xyind).reshape(-1))).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-cabinet",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "experimental-clark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      " 10 : 1 4 ('red', 'dashed') ('green', 'dashed')\n",
      "key\n",
      " 0 ('red', 'void')\n",
      "all matches \n",
      " [('red', 'dashed'), ('green', 'dashed'), ('blue', 'dashed')]\n",
      "-----\n",
      "query\n",
      " 7 : 0 8 ('red', 'void') ('blue', 'solid')\n",
      "key\n",
      " 0 ('red', 'void')\n",
      "all matches \n",
      " []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, tensor([7]), tensor([0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### One embed per query!\n",
    "\n",
    "class GameDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by gen_card_data.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        # y\n",
    "        self.query_support_size = len(self.raw_data['idx_to_query'])\n",
    "        # x\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.query_support_size * self.key_support_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: (xy_i) * (xy.shape[1]=self.query_support_size) + (xy_j)\n",
    "        '''\n",
    "        x_i, y_j = idx//self.query_support_size, idx%self.query_support_size\n",
    "        all_matches = list(self.raw_data['query_to_keys'].get(y_j, {}).keys())\n",
    "        gt = np.zeros(self.key_support_size)\n",
    "        gt[all_matches] = 1.0\n",
    "        \n",
    "        if self.debug:\n",
    "            yj1, yj2 = self.raw_data['idx_to_query'][y_j]\n",
    "            print('query\\n', y_j,\":\", yj1, yj2, self.raw_data['idx_to_key'][yj1], self.raw_data['idx_to_key'][yj2])\n",
    "            print('key\\n', x_i, self.raw_data['idx_to_key'][x_i])\n",
    "            print('all matches \\n', [self.raw_data['idx_to_key'][m] for m in all_matches])\n",
    "        \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        )    \n",
    "\n",
    "game_dataset = GameDataset(raw_data=game_data, debug=True)\n",
    "game_dataset[10] \n",
    "print('-----')\n",
    "game_dataset[7] # 7 has zero prob of being sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mediterranean-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      " 10 : 1 4 ('red', 'dashed') ('green', 'dashed')\n",
      "all matches \n",
      " [('red', 'dashed'), ('green', 'dashed'), ('blue', 'dashed')]\n",
      "-----\n",
      "query\n",
      " 7 : 0 8 ('red', 'void') ('blue', 'solid')\n",
      "all matches \n",
      " []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " tensor([7]),\n",
       " tensor([-9223372036854775808]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameTestFullDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, debug=False):\n",
    "        '''\n",
    "        raw_data: object returned by gen_card_data.\n",
    "        queries_with_shared_attr_only: bool.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.debug = debug\n",
    "        # y\n",
    "        self.query_support_size = len(self.raw_data['idx_to_query'])\n",
    "        # x\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data['idx_to_query'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        key_idx: int. 0 to query_support_size-1\n",
    "        '''\n",
    "        y_j = idx\n",
    "        x_i = torch.empty(1) # just a meaningless value\n",
    "        all_matches = list(self.raw_data['query_to_keys'].get(y_j, {}).keys())\n",
    "        gt = np.zeros(self.key_support_size)\n",
    "        gt[all_matches] = 1.0\n",
    "\n",
    "        if self.debug:\n",
    "            yj1, yj2 = self.raw_data['idx_to_query'][y_j]\n",
    "            print('query\\n', y_j,\":\", yj1, yj2, self.raw_data['idx_to_key'][yj1], self.raw_data['idx_to_key'][yj2])\n",
    "            print('all matches \\n', [self.raw_data['idx_to_key'][m] for m in all_matches])\n",
    "            \n",
    "            \n",
    "        return (\n",
    "            idx, \n",
    "            torch.tensor([y_j]).long(), # query\n",
    "            torch.tensor([x_i]).long(), # gt key\n",
    "            torch.tensor(gt).long()     # all gt keys\n",
    "        ) \n",
    "    \n",
    "game_testdataset = GameTestFullDataset(raw_data=game_data, debug=True)\n",
    "game_testdataset[10]\n",
    "print('-----')\n",
    "game_testdataset[7] # 7 has zero prob of being sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "broadband-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size, raw_data, seen_train_xy, seen_val_xy, debug=False):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = GameDataset(raw_data=raw_data, debug=debug)\n",
    "#         self.testdataset = GameTestFullDataset(raw_data=raw_data, debug=debug)\n",
    "        self.seen_train_xy = seen_train_xy\n",
    "        self.seen_val_xy = seen_val_xy\n",
    "        self.setup_samplers()\n",
    "        \n",
    "    def setup_samplers(self):\n",
    "        self.train_sampler = WeightedRandomSampler(\n",
    "            weights=self.seen_train_xy.reshape(-1), num_samples=self.batch_size, replacement=True\n",
    "        ) \n",
    "        self.val_sampler = WeightedRandomSampler(\n",
    "            weights=self.seen_val_xy.reshape(-1), num_samples=self.batch_size, replacement=True\n",
    "        )\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train = self.dataset\n",
    "            self.val = self.dataset\n",
    "#         if stage == 'test' or stage is None:\n",
    "#             self.test = self.testdataset\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=False, sampler=self.train_sampler\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val, batch_size=self.batch_size,  shuffle=False, sampler=self.val_sampler\n",
    "        )\n",
    "        return val_loader\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         test_loader = DataLoader(\n",
    "#             self.test, batch_size=self.batch_size, shuffle=False\n",
    "#         )\n",
    "#         return test_loader  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-domestic",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "entertaining-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_full_model(hparams):\n",
    "    '''\n",
    "    return: nn.Module.\n",
    "    '''\n",
    "    # embeddings\n",
    "    query_embed_X = ScaledEmbedding(hparams['query_support_size'], hparams['d_model'])\n",
    "    key_embed_X = ScaledEmbedding(hparams['key_support_size'], hparams['d_model'])\n",
    "    embed_dropout = nn.Dropout(hparams['embed_dropout'])\n",
    "    \n",
    "    # full model\n",
    "    model = EncoderPredictor(\n",
    "        inp_query_layer = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('scaled_embed', query_embed_X),\n",
    "                ('embed_dropout', embed_dropout)\n",
    "            ])\n",
    "        ),\n",
    "        inp_key_layer = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('scaled_embed', key_embed_X),\n",
    "                ('embed_dropout', embed_dropout)\n",
    "            ])\n",
    "        ),\n",
    "        classifier = nn.Sequential(\n",
    "            OrderedDict([   \n",
    "                ('linear1', nn.Linear(2*hparams['d_model'], hparams['d_model'])),\n",
    "                ('nonLinear1', nn.ReLU()),\n",
    "                ('linear-out', nn.Linear(hparams['d_model'], 1)),\n",
    "            ])\n",
    "        ) if not hparams['dotproduct_bottleneck'] else None, \n",
    "        \n",
    "        key_support_size = hparams['key_support_size'],\n",
    "        d_model = hparams['d_model'],\n",
    "        debug = hparams['debug'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "jewish-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, V, d_model):\n",
    "        super(ScaledEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(V, d_model)\n",
    "        # scale embedding to have variance 0.01\n",
    "        nn.init.normal_(self.embedding.weight, mean=0., std=(0.01)**(1/2))\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        '''\n",
    "        tokens: shape (batch_size=b, len)\n",
    "        '''\n",
    "        # shape (b, len, d_model)\n",
    "        embedded = self.embedding(tokens) * math.sqrt(self.d_model)\n",
    "        if torch.max(embedded) > 2000.:\n",
    "            import pdb; pdb.set_trace()\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brilliant-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_query_layer, inp_key_layer, classifier, key_support_size, d_model, debug=False):\n",
    "        super().__init__()\n",
    "        self.inp_query_layer = inp_query_layer\n",
    "        self.inp_key_layer = inp_key_layer\n",
    "        self.classifier = classifier\n",
    "        self.key_support_size = key_support_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, X_query, X_key, val_bool, debug=False):\n",
    "        '''\n",
    "        X_query: (b, 1)\n",
    "        X_key: (b, 1) if test bool, else (b, num matched cards) or None.\n",
    "        '''\n",
    "        if X_key is not None: assert X_query.shape == X_key.shape\n",
    "        if val_bool:\n",
    "            return self.forward_norm_support(X_query, debug=debug)\n",
    "        else:\n",
    "            assert X_key is not None, 'X_key should not be None for normalizing over minibatch keys.'\n",
    "            return self.forward_norm_minibatch(X_query, X_key, debug=debug)\n",
    "\n",
    "    def forward_norm_minibatch(self, X_query, X_key, debug=False):\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        query_repr = self.encode_query(X_query).squeeze(1)\n",
    "        assert query_repr.shape == (b, self.d_model)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        key_repr = self.encode_key(X_key).squeeze(1)\n",
    "        assert key_repr.shape == (b, self.d_model)\n",
    "\n",
    "        if self.classifier:\n",
    "            raise ValueError('Not supposed to use classifier!')\n",
    "            # shape(b, b, d_model)\n",
    "            query_repr_tiled = query_repr.unsqueeze(1).expand(b, b, self.d_model)\n",
    "            # shape(b, b, d_model)\n",
    "            key_repr_tiled = key_repr.unsqueeze(0).expand(b, b, self.d_model)\n",
    "            # shape(b, b, 2*d_model)\n",
    "            query_key_concat = torch.cat([query_repr_tiled, key_repr_tiled], dim=2)\n",
    "            assert query_key_concat.shape == (b, b, 2*self.d_model)\n",
    "            # shape(b*b, 2*d_model)\n",
    "            query_key_concat = query_key_concat.reshape(b*b, 2*self.d_model)\n",
    "            # shape(b*b, 1)\n",
    "            logits = self.classifier(query_key_concat)\n",
    "            assert logits.shape == (b*b, 1)\n",
    "            # shape(b, b)\n",
    "            logits = logits.squeeze(1).reshape(b, b)\n",
    "        else:\n",
    "            # shape(b, b) dotproduct=logit matrix\n",
    "            logits = torch.matmul(query_repr, key_repr.T)\n",
    "        assert logits.shape == (b, b)\n",
    "        \n",
    "        # shape(b, b)\n",
    "        return logits\n",
    "\n",
    "    def forward_norm_support(self, X_query, debug=False):\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        \n",
    "        # shape(b, d_model)\n",
    "        query_repr = self.encode_query(X_query).squeeze(1)\n",
    "        assert query_repr.shape == (b, self.d_model)\n",
    "\n",
    "        # shape(size(support), d_model)\n",
    "        keys_repr = self.encode_all_keys()\n",
    "        assert keys_repr.shape == (self.key_support_size, self.d_model)\n",
    "        \n",
    "        if self.classifier:\n",
    "            # shape(b, size(support), d_model)\n",
    "            query_repr_tiled = query_repr.unsqueeze(1).expand(b, self.key_support_size, self.d_model)\n",
    "            # shape(b, size(support), d_model)\n",
    "            key_repr_tiled = keys_repr.unsqueeze(0).expand(b, self.key_support_size, self.d_model)\n",
    "            # shape(b, size(support), 2*d_model)\n",
    "            query_key_concat = torch.cat([query_repr_tiled, key_repr_tiled], dim=2)\n",
    "            # shape(b*size(support), 2*d_model)\n",
    "            query_key_concat = query_key_concat.reshape(b*self.key_support_size, 2*self.d_model)\n",
    "            # shape(b*size(support), 1)\n",
    "            logits = self.classifier(query_key_concat)\n",
    "            # shape(b, size(support))\n",
    "            logits = logits.squeeze(1).reshape(b, self.key_support_size)\n",
    "        else:\n",
    "            # shape(b, size(support)) dotproduct=logit matrix\n",
    "            logits = torch.matmul(query_repr, keys_repr.T)\n",
    "        assert logits.shape == (b, self.key_support_size)\n",
    "        \n",
    "        # shape(b, size(support)) \n",
    "        return logits\n",
    "\n",
    "    def encode_query(self, X):\n",
    "        '''\n",
    "        X: (batch_size=b,1)\n",
    "        '''\n",
    "        b = X.shape[0] \n",
    "        # shape(b, 1, embed_dim)\n",
    "        inp_embed = self.inp_query_layer(X)\n",
    "        assert inp_embed.shape == (b, 1, self.d_model)\n",
    "        return inp_embed     \n",
    "        \n",
    "    def encode_key(self, X):\n",
    "        '''\n",
    "        X: (batch_size=b)\n",
    "        '''\n",
    "        b = X.shape[0] \n",
    "        # shape(b, 1, embed_dim)\n",
    "        inp_embed = self.inp_key_layer(X) \n",
    "        assert inp_embed.shape == (b, 1, self.d_model)\n",
    "        return inp_embed  \n",
    "\n",
    "    def encode_all_keys(self):\n",
    "        \n",
    "        # shape(size(support), embed_dim)\n",
    "        all_embed = self.inp_key_layer.scaled_embed.embedding.weight\n",
    "        assert all_embed.requires_grad == True\n",
    "        assert all_embed.shape == (self.key_support_size, self.d_model)\n",
    "        \n",
    "        return all_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-walnut",
   "metadata": {},
   "source": [
    "## Loss, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suspected-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_loss_per_row= tensor(20.0001)\n",
      "sum_loss_per_col= tensor(60.0001)\n",
      "avg loss= tensor(40.0001)\n",
      "-------------\n",
      "sum_loss_per_row= tensor(0.6933)\n",
      "sum_loss_per_col= tensor(40.0001)\n",
      "avg loss= tensor(20.3467)\n"
     ]
    }
   ],
   "source": [
    "class InfoCELoss(nn.Module):\n",
    "    '''\n",
    "    InfoCE Loss on a (b, b) logits matrix with Temperature scaling\n",
    "    '''\n",
    "\n",
    "    def __init__(self, temperature_const=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature_const = temperature_const\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, logits, debug=False):\n",
    "        '''\n",
    "        logits: shape (batch_size=b, b)\n",
    "        '''\n",
    "        assert logits.shape[0] == logits.shape[1]\n",
    "        b = logits.shape[0]\n",
    "        \n",
    "        logits /= self.temperature_const\n",
    "        \n",
    "        labels = torch.arange(b).type_as(logits).long()\n",
    "        sum_loss_per_row = self.CE_loss(logits, labels)\n",
    "        sum_loss_per_col = self.CE_loss(logits.T, labels)\n",
    "        \n",
    "        if debug:\n",
    "            print('sum_loss_per_row=',sum_loss_per_row)\n",
    "            print('sum_loss_per_col=',sum_loss_per_col)\n",
    "\n",
    "        loss = (sum_loss_per_row + sum_loss_per_col) * 0.5\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ---------------------------    \n",
    "loss_criterion = InfoCELoss(temperature_const=0.1)\n",
    "\n",
    "logits = torch.tensor([\n",
    "    [1.,2.,3.],\n",
    "    [6.,7.,4.],\n",
    "    [5.,8.,9.]\n",
    "])\n",
    "print('avg loss=',loss_criterion(logits, True))\n",
    "print('-------------')\n",
    "logits = torch.tensor([\n",
    "    [3.,2.,3.],\n",
    "    [6.,7.,4.],\n",
    "    [5.,8.,9.]\n",
    "])\n",
    "print('avg loss=',loss_criterion(logits, True))\n",
    "\n",
    "\n",
    "# ref\n",
    "# sum_loss_per_row= tensor(20.0001)\n",
    "# sum_loss_per_col= tensor(60.0001)\n",
    "# avg loss= tensor(40.0001)\n",
    "# -------------\n",
    "# sum_loss_per_row= tensor(0.6933)\n",
    "# sum_loss_per_col= tensor(40.0001)\n",
    "# avg loss= tensor(20.3467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "following-bathroom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ThresholdedMetrics(nn.Module):\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        '''\n",
    "        tot_k: total number of candidates. e.g. 81 cards\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.raw_data = raw_data\n",
    "        self.key_support_size = len(self.raw_data['idx_to_key'])\n",
    "        self.threshold = 1.0 / (self.key_support_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def breakdown_errors(self, X_query, corrects):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        corrects: shape (b, support size)\n",
    "        '''\n",
    "        b = X_query.shape[0]\n",
    "        assert corrects.shape == (b, self.key_support_size)\n",
    "        \n",
    "        X_query_list = X_query.squeeze(-1)\n",
    "        wrongs = (1 - corrects).cpu().numpy()\n",
    "        \n",
    "        num_matched_concepts = [\n",
    "             self.raw_data['query_to_keys'].get(X_query_list[batch_i].item(), {}).get(card_idx, 0)\n",
    "            for batch_i in range(b) for card_idx in range(self.key_support_size)  \n",
    "        ]\n",
    "        \n",
    "        num_matched_concepts = np.array(num_matched_concepts).reshape(b, self.key_support_size)\n",
    "        assert num_matched_concepts.shape == wrongs.shape\n",
    "        \n",
    "        wrongs_mask = wrongs.reshape(-1).tolist()\n",
    "        num_matched_concepts = num_matched_concepts.reshape(-1).tolist()\n",
    "\n",
    "        error_count_by_num_matched_concepts = {k:0 for k in range(num_attributes)}\n",
    "        total_count_by_num_matched_concepts = {k:0 for k in range(num_attributes)}\n",
    "        for w,k in zip(wrongs_mask, num_matched_concepts):\n",
    "            if w == 1:\n",
    "                error_count_by_num_matched_concepts[k] += 1\n",
    "            total_count_by_num_matched_concepts[k] += 1    \n",
    "\n",
    "        error_counts, total_counts = {}, {}\n",
    "        for k in range(num_attributes):\n",
    "            err_ct = error_count_by_num_matched_concepts[k]\n",
    "            tot_ct = total_count_by_num_matched_concepts[k]\n",
    "            error_counts[f'error_rate_for_{k}_matched_concepts'] = 0 if tot_ct == 0 else (err_ct *1.0 / tot_ct)\n",
    "            total_counts[f'total_count_for_{k}_matched_concepts'] = tot_ct\n",
    "\n",
    "        return {**error_counts, **total_counts}    \n",
    "    \n",
    "    def forward(self, X_query, logits, X_keys, full_test_bool=False, breakdown_errors_bool=False, debug=False):\n",
    "        b = X_query.shape[0]\n",
    "        assert X_query.shape == (b, 1)\n",
    "        b, key_support_size = logits.shape\n",
    "        assert key_support_size == self.key_support_size\n",
    "        assert logits.shape == X_keys.shape\n",
    "        \n",
    "        if full_test_bool:\n",
    "            # filter down to query cards with >0 true matches\n",
    "            queryIdxs_with_nonzero_matches = list(self.raw_data['query_to_keys'].keys())\n",
    "            X_query_flat = X_query.squeeze(-1)\n",
    "            fil = torch.stack([(X_query_flat == qId) for qId in queryIdxs_with_nonzero_matches], dim=1).any(dim=1)\n",
    "            assert fil.shape == (b, )\n",
    "            \n",
    "            # queries with shared attributes\n",
    "            fil_metrics = self.compute_metrics(X_query[fil], logits[fil], X_keys[fil], breakdown_errors_bool, debug)\n",
    "            fil_metrics = {'nonNullQueries_'+k:fil_metrics[k] for k in fil_metrics}\n",
    "            \n",
    "            # queries without shared attributes\n",
    "            not_fil = torch.logical_not(fil)\n",
    "            not_fil_metrics = self.compute_metrics(X_query[not_fil], logits[not_fil], X_keys[not_fil], breakdown_errors_bool, debug)\n",
    "            not_fil_metrics = {'NullQueries_'+k:not_fil_metrics[k] for k in not_fil_metrics}\n",
    "            \n",
    "            # all queries\n",
    "            all_metrics = self.compute_metrics(X_query, logits, X_keys, breakdown_errors_bool, debug)\n",
    "            \n",
    "            return {**fil_metrics, **not_fil_metrics, **all_metrics}\n",
    "        else:\n",
    "            return self.compute_metrics(X_query, logits, X_keys, breakdown_errors_bool, debug)\n",
    "    \n",
    "    def compute_metrics(self, X_query, logits, X_keys, breakdown_errors_bool=False, debug=False):\n",
    "        '''\n",
    "        X_query: shape (b,1) (if one embed per query)\n",
    "        logits: shape (b, support size)\n",
    "        X_keys: shape (b, support size). value 1.0 at where card matches. value 0 otherwise.\n",
    "        '''\n",
    "        b, key_support_size = logits.shape\n",
    "        \n",
    "        # model predictions, shape (b, support size)\n",
    "        binary_predictions = (self.softmax(logits) >= self.threshold).type(torch.float)\n",
    "        # ground truth, shape (b, support size)\n",
    "        gt = X_keys\n",
    "        # correct predictions, shape (b, support size)\n",
    "        corrects = (binary_predictions == gt).type(torch.float)\n",
    "        \n",
    "        # accuracy, computed per query, average across queries\n",
    "        # (b,)\n",
    "        accuracy_row = torch.sum(corrects, dim=1) / key_support_size\n",
    "        # scalar\n",
    "        accuracy_meanrows = torch.mean(accuracy_row)\n",
    "        # accuracy, computed per query-key, average across all\n",
    "        accuracy_all = torch.sum(corrects) / (b * key_support_size)\n",
    "        \n",
    "        # precision, computed per query, average across queries\n",
    "        # (b,)\n",
    "        precision_row = torch.sum((corrects * binary_predictions), dim=1) / torch.sum(binary_predictions, dim=1)\n",
    "        # scalar\n",
    "        precision_meanrows = torch.mean(precision_row)\n",
    "        # precision, computed per query-key, average across all\n",
    "        precision_all = torch.sum((corrects * binary_predictions)) / torch.sum(binary_predictions)\n",
    "\n",
    "        # recall, computed per query, average across queries\n",
    "        # (b,)\n",
    "        recall_row = torch.sum((corrects * gt), dim=1) / torch.sum(gt, dim=1)\n",
    "        # scalar\n",
    "        recall_meanrows = torch.mean(recall_row)\n",
    "        # recall, computed per query-key, average across all\n",
    "        recall_all = torch.sum((corrects * gt)) / torch.sum(gt)\n",
    "        \n",
    "        # f1, computed per query, average across queries\n",
    "        # (b,)\n",
    "        f1_row = 2 * (precision_row * recall_row) / (precision_row + recall_row)\n",
    "        # scalar\n",
    "        f1_meanrows = torch.mean(f1_row)\n",
    "        # f1, computed per query-key, average across all\n",
    "        f1_all = (precision_all * recall_all) / (precision_all + recall_all)\n",
    "        \n",
    "        if breakdown_errors_bool:\n",
    "            error_breakdown_by_num_matched_concepts = self.breakdown_errors(X_query, corrects)\n",
    "        else:\n",
    "            error_breakdown_by_num_matched_concepts = {} \n",
    "            \n",
    "        if debug:\n",
    "            print('####################################################')\n",
    "            print('Metrics Per Query:')\n",
    "            print('accuracy_rows', accuracy_row)\n",
    "            print('precision_row', precision_row)\n",
    "            print('recall_row', recall_row)\n",
    "            print('f1_row', f1_row)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across Queries')\n",
    "            print('accuracy_meanrows', accuracy_meanrows)\n",
    "            print('precision_meanrows', precision_meanrows)\n",
    "            print('recall_meanrows', recall_meanrows)\n",
    "            print('f1_meanrows', f1_meanrows)\n",
    "            print('####################################################')\n",
    "            print('Metrics Averaged Across All Query-Key Pairs:')\n",
    "            print('accuracy_all', accuracy_all)\n",
    "            print('precision_all', precision_all)\n",
    "            print('recall_all', recall_all)\n",
    "            print('f1_all', f1_all)\n",
    "            print('####################################################')\n",
    "            print('error_breakdown', error_breakdown_by_num_matched_concepts)\n",
    "            \n",
    "        metrics = {\n",
    "            'accuracy_by_Query': accuracy_meanrows,\n",
    "            'precision_by_Query': precision_meanrows,\n",
    "            'recall_by_Query': recall_meanrows,\n",
    "            'f1_by_Query': f1_meanrows,\n",
    "            'accuracy_by_QueryKey': accuracy_all,\n",
    "            'precision_by_QueryKey': precision_all,\n",
    "            'recall_by_QueryKey': recall_all,\n",
    "            'f1_by_QueryKey': f1_all\n",
    "        }\n",
    "        metrics = {\n",
    "            **metrics, **error_breakdown_by_num_matched_concepts\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def test_metric_module():\n",
    "    key_support_size=3\n",
    "    thresh=1./key_support_size\n",
    "\n",
    "    m_f = ThresholdedMetrics(raw_data=game_data)\n",
    "    m_f.key_support_size = 3\n",
    "\n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    logits = torch.tensor(\n",
    "        [\n",
    "            [0.4, 0.2, 0.4],\n",
    "            [thresh, thresh, thresh],\n",
    "            [thresh, thresh, thresh]\n",
    "        ]\n",
    "    )\n",
    "    X_keys = torch.tensor(\n",
    "        [\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,1,1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(m_f(None, logits, X_keys, debug=True))\n",
    "    \n",
    "    \n",
    "# test_metric_module()\n",
    "\n",
    "# reference output\n",
    "# accuracy_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.3333, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.5000, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.5556), 'precision': tensor(0.5556), 'recall': tensor(1.), 'f1': tensor(0.6667)}\n",
    "# ----------------------------------------------\n",
    "# accuracy_row tensor([0.6667, 0.3333, 1.0000])\n",
    "# precision_row tensor([0.5000, 0.3333, 1.0000])\n",
    "# recall_row tensor([1., 1., 1.])\n",
    "# f1_row tensor([0.6667, 0.5000, 1.0000])\n",
    "# {'accuracy': tensor(0.6667), 'precision': tensor(0.6111), 'recall': tensor(1.), 'f1': tensor(0.7222)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-question",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "killing-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lightning Module\n",
    "# https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=UIXLW8CO-W8w\n",
    "\n",
    "thresholded_metrics = ThresholdedMetrics(raw_data=game_data)\n",
    "\n",
    "class TrainModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams, gt_distributions):\n",
    "        '''\n",
    "        hparams: dictionary of hyperparams\n",
    "        gt_distributions: dictionary that stores the groundtruth 'xy', 'xyind' distributions.\n",
    "                         each is a key_support_size by query_support_size matrix that sums up to 1.0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.debug = hparams['debug']\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = construct_full_model(hparams)\n",
    "        self.loss_criterion = InfoCELoss(temperature_const=self.hparams['loss_temperature_const'])\n",
    "        self.metrics = thresholded_metrics\n",
    "\n",
    "        self.key_support_size = self.hparams['key_support_size']\n",
    "        self.query_support_size = self.hparams['query_support_size']\n",
    "        \n",
    "        # for pulling model p(x,y) and p(x,y)/[pxpy]\n",
    "        self.populate_logits_matrix = hparams['populate_logits_matrix']\n",
    "        if self.populate_logits_matrix:\n",
    "            self.register_buffer(\n",
    "                name='model_logits_matrix',\n",
    "                tensor= torch.zeros(hparams['key_support_size'], hparams['query_support_size'])\n",
    "            )\n",
    "            self.setup_gt_distributions(gt_distributions)\n",
    "        \n",
    "    def log_metrics(self, metrics_dict):\n",
    "        for k, v in metrics_dict.items():\n",
    "            self.log(k, v)\n",
    "            \n",
    "    def get_max_memory_alloc(self):\n",
    "        devices_max_memory_alloc = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.device(f'cuda:{i}')\n",
    "            devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            torch.cuda.reset_max_memory_allocated(device)\n",
    "        return devices_max_memory_alloc\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def forward(self, X_query, X_key, X_keys, val_bool, full_test_bool=False, debug=False):\n",
    "        '''\n",
    "        X_query: (b, 1)\n",
    "        X_key: (b, 1)\n",
    "        X_keys: (b, key_support_size) 1s and 0s.\n",
    "        test_bool: boolean.\n",
    "        '''\n",
    "        batch_size = X_query.shape[0]\n",
    "        \n",
    "        # ToDo batch_size_b\n",
    "        \n",
    "        # shape (b,support) if test_bool else (b, b)\n",
    "        logits = self.model(X_query, X_key, val_bool, debug=debug)\n",
    "        # scalar\n",
    "        loss = None if val_bool else self.loss_criterion(logits, debug=debug)\n",
    "        # scalar\n",
    "        metrics = self.metrics(\n",
    "            logits=logits, X_keys=X_keys, X_query=X_query, \n",
    "            debug=debug, full_test_bool=full_test_bool, breakdown_errors_bool=True, \n",
    "        ) if val_bool else None\n",
    "\n",
    "        return logits, loss, metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def setup_gt_distributions(self, gt_distributions):\n",
    "        '''called once during init to setup groundtruth distributions'''\n",
    "        assert gt_distributions['xy'].shape == gt_distributions['xyind'].shape\n",
    "        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy',\n",
    "            tensor= torch.tensor(gt_distributions['xy'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xyind',\n",
    "            tensor= torch.tensor(gt_distributions['xyind'])\n",
    "        )        \n",
    "        # (key_support_size, query_support_size)\n",
    "        self.register_buffer(\n",
    "            name='gt_xy_div_xyind',\n",
    "            tensor= self.gt_xy/self.gt_xyind\n",
    "        )\n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='one',\n",
    "            tensor= torch.tensor([1.0])\n",
    "        )   \n",
    "        # scalar\n",
    "        self.register_buffer(\n",
    "            name='gt_mi',\n",
    "            tensor= self.compute_mutual_information(self.gt_xy, self.gt_xy_div_xyind)\n",
    "        ) \n",
    "   \n",
    "    \n",
    "    def populate_model_logits_matrix(self, query_idx, logits):\n",
    "        '''\n",
    "        query_idx: shape (b,)\n",
    "        logits: shape(b, key_support_size)\n",
    "        '''  \n",
    "        assert query_idx.shape[0] == logits.shape[0]\n",
    "        b = query_idx.shape[0]\n",
    "        assert logits.shape[1] == self.key_support_size\n",
    "        for i in range(b):\n",
    "            self.model_logits_matrix[:,query_idx[i]] = logits[i]\n",
    "    \n",
    "    def compute_mutual_information(self, xy, xy_div_xyind):\n",
    "        '''\n",
    "        xy: p(xy). shape(b, key_support_size)\n",
    "        xy_div_xyind_hat: p(xy)/[p(x)(y)].\n",
    "                          shape(b, key_support_size)\n",
    "        '''\n",
    "        assert torch.isclose(torch.sum(xy), self.one.type_as(xy))\n",
    "        assert xy.shape == xy_div_xyind.shape == (\n",
    "            self.key_support_size, self.query_support_size\n",
    "        )\n",
    "        pmi = torch.log(xy_div_xyind)\n",
    "        mi = torch.sum(xy * pmi)\n",
    "        return mi\n",
    "    \n",
    "    def pull_model_distribution(self, debug=True):\n",
    "\n",
    "        # sanity check\n",
    "        sum_logits = torch.sum(self.model_logits_matrix)\n",
    "        assert sum_logits != 0.0\n",
    "        \n",
    "        if debug:\n",
    "            print('Sum of model logits matrix\\n', sum_logits)\n",
    "            print('Number of model logits with zero value\\n', torch.sum(self.model_logits_matrix == 0.0)) \n",
    "            print('Variance of model logits\\n', torch.var(self.model_logits_matrix))\n",
    "        \n",
    "        # estimate the full distribution\n",
    "        # hat( k * pxy/(pxpy)\n",
    "        f = torch.exp(self.model_logits_matrix)\n",
    "        # hat( k * pxy)\n",
    "        xy_hat = f * self.gt_xyind\n",
    "        # hat( pxy)\n",
    "        xy_hat = (xy_hat / torch.sum(xy_hat))\n",
    "        \n",
    "        # estimate exp(pmi)\n",
    "        # hat(k)\n",
    "        k_hat = torch.sum(f) / torch.sum(self.gt_xy_div_xyind)\n",
    "        # hat(pxy/(pxpy)\n",
    "        xy_div_xyind_hat = (f / k_hat)\n",
    "        if torch.any(torch.isnan(xy_div_xyind_hat)):\n",
    "            import pdb; pdb.set_trace()\n",
    "        \n",
    "        # estimate MI\n",
    "        # scalar\n",
    "        mi_hat = self.compute_mutual_information(xy_hat, xy_div_xyind_hat)\n",
    "        # scalar\n",
    "        mi_gt_minus_hat = self.gt_mi - mi_hat\n",
    "        \n",
    "        # estimate KL divergence\n",
    "        kl_div_val = F.kl_div(torch.log(xy_hat), self.gt_xy)\n",
    "\n",
    "        # estimate ranks\n",
    "        xy_hat = xy_hat.detach().cpu().numpy()\n",
    "        xy_div_xyind_hat = xy_div_xyind_hat.detach().cpu().numpy()\n",
    "        # hat(pxy rank)\n",
    "        xy_hat_rank = np.linalg.matrix_rank(xy_hat)\n",
    "        # hat(pxy/(pxpy rank)\n",
    "        xy_div_xyind_hat_rank = np.linalg.matrix_rank(xy_div_xyind_hat) \n",
    "        \n",
    "        pulled_distribution_results = {\n",
    "            'xy_hat':xy_hat,\n",
    "            'xy_div_xyind_hat':xy_div_xyind_hat,\n",
    "            'xy_hat_rank':xy_hat_rank,\n",
    "            'xy_div_xyind_hat_rank':xy_div_xyind_hat_rank,\n",
    "            'mi_hat':mi_hat,\n",
    "            'mi_gt_minus_hat':mi_gt_minus_hat,\n",
    "            'kl_div':kl_div_val\n",
    "        }\n",
    "        \n",
    "        return pulled_distribution_results\n",
    "\n",
    "    ###################################################\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        # scalar\n",
    "        _, loss, _ = self(X_query, X_key, None, val_bool=False, debug=self.debug)\n",
    "        # dict\n",
    "        _, _, metrics = self(X_query, None, X_keys, val_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('train step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\nX_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:\\n', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {**{'train_loss': loss}, **{'train_'+m:metrics[m] for m in metrics}}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        # _, (b, 1), (b, 1), (b, support size)\n",
    "        _, X_query, X_key, X_keys = batch\n",
    "        _, loss, _ = self(X_query, X_key, None, val_bool=False, debug=self.debug)\n",
    "        _, _, metrics = self(X_query, None, X_keys, val_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('-----------------------------')\n",
    "            print('validation step')\n",
    "            print(Counter(torch.sum(X_keys, dim=1).tolist()).most_common())\n",
    "            print(\n",
    "                'X_query:',X_query[0], '\\X_key:',\n",
    "                X_key[0], '\\nloss:', loss, '\\nmetrics:', [(m,metrics[m]) for m in metrics]\n",
    "            )\n",
    "            \n",
    "        # log \n",
    "        step_metrics = {**{'val_loss': loss}, **{'val_'+m:metrics[m] for m in metrics}}\n",
    "        devices_max_memory_alloc = self.get_max_memory_alloc()\n",
    "        for device, val in devices_max_memory_alloc.items():\n",
    "            step_metrics[f'step_max_memory_alloc_cuda:{device}'] = val\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        # (b,1), (b,1), _, (b, support size)\n",
    "        query_idx, X_query, _, X_keys = batch\n",
    "        \n",
    "        # compute scores for all keys\n",
    "        # shape(b, key_support_size), _, dictionary\n",
    "        logits, _, metrics = self(X_query, None, X_keys, val_bool=True, full_test_bool=True, debug=self.debug)\n",
    "        \n",
    "        if self.populate_logits_matrix:\n",
    "            self.populate_model_logits_matrix(query_idx, logits)\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {'test_'+m:metrics[m] for m in metrics}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics \n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def aggregate_metrics_at_epoch_end(self, outputs):\n",
    "        # log metrics\n",
    "        epoch_metrics = {}\n",
    "        metric_names = outputs[0].keys()\n",
    "        for m in metric_names:\n",
    "            if not ('max_memory_alloc_cuda' in m or 'count' in m or 'rate' in m):\n",
    "                epoch_metrics['avg_'+m] = torch.stack([x[m] for x in outputs]).mean()\n",
    "            elif '_matched_concepts' in m:\n",
    "                epoch_metrics['avg_'+m] = np.mean([x[m] for x in outputs])\n",
    "        self.log_metrics(epoch_metrics)\n",
    "        return epoch_metrics         \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        return averaged_metrics\n",
    "    \n",
    "    def test_epoch_end(self, outputs):        \n",
    "        averaged_metrics = self.aggregate_metrics_at_epoch_end(outputs)\n",
    "        try:\n",
    "            assert 'avg_test_error_rate_for_1_matched_concepts' in averaged_metrics\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "            \n",
    "        return averaged_metrics\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.hparams['lr'],\n",
    "            betas=(\n",
    "                self.hparams['adam_beta1'], self.hparams['adam_beta2']),\n",
    "            eps=self.hparams['adam_epsilon'],\n",
    "            weight_decay=self.hparams['adam_weight_decay']\n",
    "        )\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-liquid",
   "metadata": {},
   "source": [
    "## hparams, init train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "little-south",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                                         | Type               | Params\n",
      "-------------------------------------------------------------------------------------\n",
      "0  | model                                        | EncoderPredictor   | 360   \n",
      "1  | model.inp_query_layer                        | Sequential         | 288   \n",
      "2  | model.inp_query_layer.scaled_embed           | ScaledEmbedding    | 288   \n",
      "3  | model.inp_query_layer.scaled_embed.embedding | Embedding          | 288   \n",
      "4  | model.inp_query_layer.embed_dropout          | Dropout            | 0     \n",
      "5  | model.inp_key_layer                          | Sequential         | 72    \n",
      "6  | model.inp_key_layer.scaled_embed             | ScaledEmbedding    | 72    \n",
      "7  | model.inp_key_layer.scaled_embed.embedding   | Embedding          | 72    \n",
      "8  | loss_criterion                               | InfoCELoss         | 0     \n",
      "9  | loss_criterion.CE_loss                       | CrossEntropyLoss   | 0     \n",
      "10 | metrics                                      | ThresholdedMetrics | 0     \n",
      "11 | metrics.softmax                              | Softmax            | 0     \n",
      "-------------------------------------------------------------------------------------\n",
      "360       Trainable params\n",
      "0         Non-trainable params\n",
      "360       Total params \n",
      "\n",
      "RUN NAME :\n",
      " CardGame:OR;attr2-val3;epsilon1e-100;d_model8;params0.36K;dot-product\n"
     ]
    }
   ],
   "source": [
    "# W&B References\n",
    "# https://docs.wandb.com/library/integrations/lightning\n",
    "# colab example\n",
    "# https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Supercharge_your_Training_with_Pytorch_Lightning_%2B_Weights_%26_Biases.ipynb\n",
    "# step by step guide\n",
    "# https://wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-Biases--Vmlldzo2NjQ1Mw\n",
    "\n",
    "\n",
    "# Distributed Weighted Sampler\n",
    "# https://discuss.pytorch.org/t/how-to-use-my-own-sampler-when-i-already-use-distributedsampler/62143/8\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/discussions/3716#discussioncomment-238296\n",
    "\n",
    "# torch lightning -- how sampler is added or removed\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/multi_gpu.html?highlight=sampler#remove-samplers\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    # game difficulty\n",
    "    'num_attributes': num_attributes,\n",
    "    'num_attr_vals': num_attr_vals,\n",
    "    # distribution\n",
    "    'distribution_epsilon': distribution_epsilon,\n",
    "    'xy_size': xy.shape,\n",
    "    # \n",
    "    'batch_size': 128,\n",
    "    # Arch\n",
    "    'key_support_size': len(game_data['idx_to_key']),\n",
    "    'query_support_size': len(game_data['idx_to_query']),\n",
    "    # embedding\n",
    "    'd_model': 8,\n",
    "    'embed_dropout': 0.0,\n",
    "    # final prediction\n",
    "    'dotproduct_bottleneck':True,\n",
    "    # loss\n",
    "    'loss_temperature_const': 1.0,\n",
    "    # optimizer\n",
    "    'lr': 0.001,\n",
    "    'adam_beta1': 0.9,\n",
    "    'adam_beta2': 0.999,\n",
    "    'adam_epsilon': 1e-08,\n",
    "    'warmup_steps': 12000,\n",
    "    'adam_weight_decay':0,\n",
    "    'gradient_clip_val': 0,\n",
    "    # others\n",
    "    'debug':False,\n",
    "    'populate_logits_matrix': True\n",
    "}\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# model\n",
    "trainmodule =  TrainModule(hparams, gt_distributions={'xy':xy, 'xyind':xyind})# , raw_data=game_data)\n",
    "model_summary = pl.core.memory.ModelSummary(trainmodule, mode='full')\n",
    "print(model_summary,'\\n')\n",
    "\n",
    "# data\n",
    "game_datamodule = GameDataModule(\n",
    "    batch_size = hparams['batch_size'],\n",
    "    raw_data = game_data,\n",
    "    seen_train_xy = xy,\n",
    "    seen_val_xy = xy,\n",
    "    debug=hparams['debug']\n",
    ")\n",
    "\n",
    "# logger\n",
    "run_name = 'CardGame:OR;attr{}-val{};epsilon{};d_model{};params{}K;dot-product'.format(num_attributes, num_attr_vals, str(distribution_epsilon), hparams['d_model'], round(max(model_summary.param_nums)/1000,2))\n",
    "project_name = 'ContrastiveLearning-cardgame-Scaling'\n",
    "wd_logger = WandbLogger(name=run_name, project=project_name)\n",
    "print('RUN NAME :\\n', run_name)\n",
    "# check point path\n",
    "ckpt_dir_PATH = os.path.join('checkpoints', project_name, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-simon",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complete-september",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory checkpoints/ContrastiveLearning-cardgame-Scaling/CardGame:OR;attr2-val3;epsilon1e-100;d_model8;params0.36K;dot-product exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "<ipython-input-23-2e0f8c45a6fa>:26: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">CardGame:OR;attr2-val3;epsilon1e-100;d_model8;params0.36K;dot-product</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling\" target=\"_blank\">https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling/runs/tki26w1t\" target=\"_blank\">https://wandb.ai/chucooleg/ContrastiveLearning-cardgame-Scaling/runs/tki26w1t</a><br/>\n",
       "                Run data is saved locally in <code>/app/Contrastive-Learning-Benchmarking/models/wandb/run-20210211_181420-tki26w1t</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                                         | Type               | Params\n",
      "-------------------------------------------------------------------------------------\n",
      "0  | model                                        | EncoderPredictor   | 360   \n",
      "1  | model.inp_query_layer                        | Sequential         | 288   \n",
      "2  | model.inp_query_layer.scaled_embed           | ScaledEmbedding    | 288   \n",
      "3  | model.inp_query_layer.scaled_embed.embedding | Embedding          | 288   \n",
      "4  | model.inp_query_layer.embed_dropout          | Dropout            | 0     \n",
      "5  | model.inp_key_layer                          | Sequential         | 72    \n",
      "6  | model.inp_key_layer.scaled_embed             | ScaledEmbedding    | 72    \n",
      "7  | model.inp_key_layer.scaled_embed.embedding   | Embedding          | 72    \n",
      "8  | loss_criterion                               | InfoCELoss         | 0     \n",
      "9  | loss_criterion.CE_loss                       | CrossEntropyLoss   | 0     \n",
      "10 | metrics                                      | ThresholdedMetrics | 0     \n",
      "11 | metrics.softmax                              | Softmax            | 0     \n",
      "-------------------------------------------------------------------------------------\n",
      "360       Trainable params\n",
      "0         Non-trainable params\n",
      "360       Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00, 24.57it/s, loss=630, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 27.83it/s, loss=630, v_num=6w1t]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s, loss=630, v_num=6w1t]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.8/site-packages/torch/cuda/memory.py:231: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/miniconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 28.25it/s, loss=627, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 24.67it/s, loss=627, v_num=6w1t]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 31.30it/s, loss=627, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 29.47it/s, loss=627, v_num=6w1t]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 24.10it/s, loss=626, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 28.90it/s, loss=626, v_num=6w1t]\n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 35.05it/s, loss=626, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 26.41it/s, loss=626, v_num=6w1t]\n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 31.12it/s, loss=626, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 34.55it/s, loss=626, v_num=6w1t]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 31.88it/s, loss=626, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 25.71it/s, loss=626, v_num=6w1t]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 27.90it/s, loss=625, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 29.57it/s, loss=625, v_num=6w1t]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 26.57it/s, loss=625, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 30.72it/s, loss=625, v_num=6w1t]\n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 23.37it/s, loss=624, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 18.19it/s, loss=624, v_num=6w1t]\n",
      "Epoch 10:  50%|█████     | 1/2 [00:00<00:00, 25.78it/s, loss=624, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 27.96it/s, loss=624, v_num=6w1t]\n",
      "Epoch 11:  50%|█████     | 1/2 [00:00<00:00, 32.36it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 2/2 [00:00<00:00, 34.86it/s, loss=623, v_num=6w1t]\n",
      "Epoch 12:  50%|█████     | 1/2 [00:00<00:00, 30.38it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 2/2 [00:00<00:00, 28.90it/s, loss=623, v_num=6w1t]\n",
      "Epoch 13:  50%|█████     | 1/2 [00:00<00:00, 30.75it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 30.23it/s, loss=623, v_num=6w1t]\n",
      "Epoch 14:  50%|█████     | 1/2 [00:00<00:00, 29.71it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00, 29.00it/s, loss=623, v_num=6w1t]\n",
      "Epoch 15:  50%|█████     | 1/2 [00:00<00:00, 25.76it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 2/2 [00:00<00:00, 29.24it/s, loss=623, v_num=6w1t]\n",
      "Epoch 16:  50%|█████     | 1/2 [00:00<00:00, 29.40it/s, loss=623, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 29.95it/s, loss=623, v_num=6w1t]\n",
      "Epoch 17:  50%|█████     | 1/2 [00:00<00:00, 33.15it/s, loss=622, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 2/2 [00:00<00:00, 35.06it/s, loss=622, v_num=6w1t]\n",
      "Epoch 18:  50%|█████     | 1/2 [00:00<00:00, 26.34it/s, loss=622, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 2/2 [00:00<00:00, 26.10it/s, loss=622, v_num=6w1t]\n",
      "Epoch 19:  50%|█████     | 1/2 [00:00<00:00, 30.11it/s, loss=622, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 29.12it/s, loss=622, v_num=6w1t]\n",
      "Epoch 20:  50%|█████     | 1/2 [00:00<00:00, 28.14it/s, loss=621, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 2/2 [00:00<00:00, 19.68it/s, loss=621, v_num=6w1t]\n",
      "Epoch 21:  50%|█████     | 1/2 [00:00<00:00, 32.01it/s, loss=621, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 2/2 [00:00<00:00, 30.27it/s, loss=621, v_num=6w1t]\n",
      "Epoch 22:  50%|█████     | 1/2 [00:00<00:00, 28.75it/s, loss=620, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 2/2 [00:00<00:00, 29.39it/s, loss=620, v_num=6w1t]\n",
      "Epoch 23:  50%|█████     | 1/2 [00:00<00:00, 30.20it/s, loss=619, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 2/2 [00:00<00:00, 26.26it/s, loss=619, v_num=6w1t]\n",
      "Epoch 24:  50%|█████     | 1/2 [00:00<00:00, 33.65it/s, loss=619, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 2/2 [00:00<00:00, 33.45it/s, loss=619, v_num=6w1t]\n",
      "Epoch 25:  50%|█████     | 1/2 [00:00<00:00, 29.94it/s, loss=618, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 2/2 [00:00<00:00, 28.13it/s, loss=618, v_num=6w1t]\n",
      "Epoch 26:  50%|█████     | 1/2 [00:00<00:00, 26.20it/s, loss=618, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|██████████| 2/2 [00:00<00:00, 28.90it/s, loss=618, v_num=6w1t]\n",
      "Epoch 27:  50%|█████     | 1/2 [00:00<00:00, 28.13it/s, loss=618, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|██████████| 2/2 [00:00<00:00, 26.22it/s, loss=618, v_num=6w1t]\n",
      "Epoch 28:  50%|█████     | 1/2 [00:00<00:00, 32.14it/s, loss=617, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|██████████| 2/2 [00:00<00:00, 26.42it/s, loss=617, v_num=6w1t]\n",
      "Epoch 29:  50%|█████     | 1/2 [00:00<00:00, 30.60it/s, loss=617, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 31.70it/s, loss=617, v_num=6w1t]\n",
      "Epoch 30:  50%|█████     | 1/2 [00:00<00:00, 31.76it/s, loss=617, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|██████████| 2/2 [00:00<00:00, 34.40it/s, loss=617, v_num=6w1t]\n",
      "Epoch 31:  50%|█████     | 1/2 [00:00<00:00, 29.68it/s, loss=616, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|██████████| 2/2 [00:00<00:00, 31.51it/s, loss=616, v_num=6w1t]\n",
      "Epoch 32:  50%|█████     | 1/2 [00:00<00:00, 28.96it/s, loss=616, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|██████████| 2/2 [00:00<00:00, 24.96it/s, loss=616, v_num=6w1t]\n",
      "Epoch 33:  50%|█████     | 1/2 [00:00<00:00, 28.41it/s, loss=615, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|██████████| 2/2 [00:00<00:00, 24.05it/s, loss=615, v_num=6w1t]\n",
      "Epoch 34:  50%|█████     | 1/2 [00:00<00:00, 28.44it/s, loss=615, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|██████████| 2/2 [00:00<00:00, 30.94it/s, loss=615, v_num=6w1t]\n",
      "Epoch 35:  50%|█████     | 1/2 [00:00<00:00, 30.68it/s, loss=614, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|██████████| 2/2 [00:00<00:00, 32.57it/s, loss=614, v_num=6w1t]\n",
      "Epoch 36:  50%|█████     | 1/2 [00:00<00:00, 29.21it/s, loss=614, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|██████████| 2/2 [00:00<00:00, 33.26it/s, loss=614, v_num=6w1t]\n",
      "Epoch 37:  50%|█████     | 1/2 [00:00<00:00, 28.33it/s, loss=614, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|██████████| 2/2 [00:00<00:00, 29.82it/s, loss=614, v_num=6w1t]\n",
      "Epoch 38:  50%|█████     | 1/2 [00:00<00:00, 21.07it/s, loss=613, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|██████████| 2/2 [00:00<00:00, 25.28it/s, loss=613, v_num=6w1t]\n",
      "Epoch 39:  50%|█████     | 1/2 [00:00<00:00, 31.37it/s, loss=613, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|██████████| 2/2 [00:00<00:00, 35.02it/s, loss=613, v_num=6w1t]\n",
      "Epoch 40:  50%|█████     | 1/2 [00:00<00:00, 27.68it/s, loss=612, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|██████████| 2/2 [00:00<00:00, 30.71it/s, loss=612, v_num=6w1t]\n",
      "Epoch 41:  50%|█████     | 1/2 [00:00<00:00, 28.06it/s, loss=612, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|██████████| 2/2 [00:00<00:00, 31.43it/s, loss=612, v_num=6w1t]\n",
      "Epoch 42:  50%|█████     | 1/2 [00:00<00:00, 32.07it/s, loss=612, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|██████████| 2/2 [00:00<00:00, 32.89it/s, loss=612, v_num=6w1t]\n",
      "Epoch 43:  50%|█████     | 1/2 [00:00<00:00, 24.37it/s, loss=611, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|██████████| 2/2 [00:00<00:00, 28.79it/s, loss=611, v_num=6w1t]\n",
      "Epoch 44:  50%|█████     | 1/2 [00:00<00:00, 27.82it/s, loss=611, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|██████████| 2/2 [00:00<00:00, 26.12it/s, loss=611, v_num=6w1t]\n",
      "Epoch 45:  50%|█████     | 1/2 [00:00<00:00, 29.26it/s, loss=610, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|██████████| 2/2 [00:00<00:00, 30.63it/s, loss=610, v_num=6w1t]\n",
      "Epoch 46:  50%|█████     | 1/2 [00:00<00:00, 33.06it/s, loss=610, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|██████████| 2/2 [00:00<00:00, 29.50it/s, loss=610, v_num=6w1t]\n",
      "Epoch 47:  50%|█████     | 1/2 [00:00<00:00, 29.28it/s, loss=609, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|██████████| 2/2 [00:00<00:00, 29.23it/s, loss=609, v_num=6w1t]\n",
      "Epoch 48:  50%|█████     | 1/2 [00:00<00:00, 29.80it/s, loss=609, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|██████████| 2/2 [00:00<00:00, 31.05it/s, loss=609, v_num=6w1t]\n",
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 15.29it/s, loss=608, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 11.93it/s, loss=608, v_num=6w1t]\n",
      "Epoch 50:  50%|█████     | 1/2 [00:00<00:00, 27.83it/s, loss=608, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|██████████| 2/2 [00:00<00:00, 31.26it/s, loss=608, v_num=6w1t]\n",
      "Epoch 51:  50%|█████     | 1/2 [00:00<00:00, 31.62it/s, loss=607, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|██████████| 2/2 [00:00<00:00, 28.09it/s, loss=607, v_num=6w1t]\n",
      "Epoch 52:  50%|█████     | 1/2 [00:00<00:00, 29.82it/s, loss=607, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|██████████| 2/2 [00:00<00:00, 25.30it/s, loss=607, v_num=6w1t]\n",
      "Epoch 53:  50%|█████     | 1/2 [00:00<00:00, 32.29it/s, loss=606, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|██████████| 2/2 [00:00<00:00, 28.28it/s, loss=606, v_num=6w1t]\n",
      "Epoch 54:  50%|█████     | 1/2 [00:00<00:00, 31.37it/s, loss=606, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|██████████| 2/2 [00:00<00:00, 36.07it/s, loss=606, v_num=6w1t]\n",
      "Epoch 55:  50%|█████     | 1/2 [00:00<00:00, 25.91it/s, loss=605, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|██████████| 2/2 [00:00<00:00, 27.40it/s, loss=605, v_num=6w1t]\n",
      "Epoch 56:  50%|█████     | 1/2 [00:00<00:00, 29.75it/s, loss=604, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|██████████| 2/2 [00:00<00:00, 29.44it/s, loss=604, v_num=6w1t]\n",
      "Epoch 57:  50%|█████     | 1/2 [00:00<00:00, 32.19it/s, loss=604, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|██████████| 2/2 [00:00<00:00, 29.61it/s, loss=604, v_num=6w1t]\n",
      "Epoch 58:  50%|█████     | 1/2 [00:00<00:00, 29.25it/s, loss=604, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|██████████| 2/2 [00:00<00:00, 31.09it/s, loss=604, v_num=6w1t]\n",
      "Epoch 59:  50%|█████     | 1/2 [00:00<00:00, 30.68it/s, loss=603, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|██████████| 2/2 [00:00<00:00, 32.54it/s, loss=603, v_num=6w1t]\n",
      "Epoch 60:  50%|█████     | 1/2 [00:00<00:00, 31.97it/s, loss=603, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|██████████| 2/2 [00:00<00:00, 32.27it/s, loss=603, v_num=6w1t]\n",
      "Epoch 61:  50%|█████     | 1/2 [00:00<00:00, 25.78it/s, loss=602, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|██████████| 2/2 [00:00<00:00, 30.96it/s, loss=602, v_num=6w1t]\n",
      "Epoch 62:  50%|█████     | 1/2 [00:00<00:00, 31.37it/s, loss=601, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|██████████| 2/2 [00:00<00:00, 31.54it/s, loss=601, v_num=6w1t]\n",
      "Epoch 63:  50%|█████     | 1/2 [00:00<00:00, 30.96it/s, loss=601, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|██████████| 2/2 [00:00<00:00, 30.61it/s, loss=601, v_num=6w1t]\n",
      "Epoch 64:  50%|█████     | 1/2 [00:00<00:00, 29.48it/s, loss=600, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|██████████| 2/2 [00:00<00:00, 30.96it/s, loss=600, v_num=6w1t]\n",
      "Epoch 65:  50%|█████     | 1/2 [00:00<00:00, 30.12it/s, loss=600, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|██████████| 2/2 [00:00<00:00, 31.08it/s, loss=600, v_num=6w1t]\n",
      "Epoch 66:  50%|█████     | 1/2 [00:00<00:00, 26.13it/s, loss=599, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|██████████| 2/2 [00:00<00:00, 26.82it/s, loss=599, v_num=6w1t]\n",
      "Epoch 67:  50%|█████     | 1/2 [00:00<00:00, 27.79it/s, loss=599, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|██████████| 2/2 [00:00<00:00, 32.20it/s, loss=599, v_num=6w1t]\n",
      "Epoch 68:  50%|█████     | 1/2 [00:00<00:00, 29.58it/s, loss=599, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|██████████| 2/2 [00:00<00:00, 26.92it/s, loss=599, v_num=6w1t]\n",
      "Epoch 69:  50%|█████     | 1/2 [00:00<00:00, 24.89it/s, loss=598, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|██████████| 2/2 [00:00<00:00, 27.01it/s, loss=598, v_num=6w1t]\n",
      "Epoch 70:  50%|█████     | 1/2 [00:00<00:00, 32.82it/s, loss=598, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|██████████| 2/2 [00:00<00:00, 32.26it/s, loss=598, v_num=6w1t]\n",
      "Epoch 71:  50%|█████     | 1/2 [00:00<00:00, 30.27it/s, loss=597, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|██████████| 2/2 [00:00<00:00, 32.09it/s, loss=597, v_num=6w1t]\n",
      "Epoch 72:  50%|█████     | 1/2 [00:00<00:00, 27.08it/s, loss=596, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|██████████| 2/2 [00:00<00:00, 25.90it/s, loss=596, v_num=6w1t]\n",
      "Epoch 73:  50%|█████     | 1/2 [00:00<00:00, 28.19it/s, loss=596, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|██████████| 2/2 [00:00<00:00, 27.91it/s, loss=596, v_num=6w1t]\n",
      "Epoch 74:  50%|█████     | 1/2 [00:00<00:00, 24.81it/s, loss=595, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|██████████| 2/2 [00:00<00:00, 24.77it/s, loss=595, v_num=6w1t]\n",
      "Epoch 75:  50%|█████     | 1/2 [00:00<00:00, 31.44it/s, loss=595, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|██████████| 2/2 [00:00<00:00, 29.85it/s, loss=595, v_num=6w1t]\n",
      "Epoch 76:  50%|█████     | 1/2 [00:00<00:00, 28.08it/s, loss=594, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|██████████| 2/2 [00:00<00:00, 30.49it/s, loss=594, v_num=6w1t]\n",
      "Epoch 77:  50%|█████     | 1/2 [00:00<00:00, 28.65it/s, loss=594, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|██████████| 2/2 [00:00<00:00, 30.43it/s, loss=594, v_num=6w1t]\n",
      "Epoch 78:  50%|█████     | 1/2 [00:00<00:00, 30.66it/s, loss=593, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|██████████| 2/2 [00:00<00:00, 19.20it/s, loss=593, v_num=6w1t]\n",
      "Epoch 79:  50%|█████     | 1/2 [00:00<00:00, 24.71it/s, loss=593, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|██████████| 2/2 [00:00<00:00, 25.38it/s, loss=593, v_num=6w1t]\n",
      "Epoch 80:  50%|█████     | 1/2 [00:00<00:00, 25.70it/s, loss=592, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 80: 100%|██████████| 2/2 [00:00<00:00, 30.92it/s, loss=592, v_num=6w1t]\n",
      "Epoch 81:  50%|█████     | 1/2 [00:00<00:00, 29.56it/s, loss=592, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 81: 100%|██████████| 2/2 [00:00<00:00, 27.82it/s, loss=592, v_num=6w1t]\n",
      "Epoch 82:  50%|█████     | 1/2 [00:00<00:00, 28.47it/s, loss=591, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 82: 100%|██████████| 2/2 [00:00<00:00, 33.68it/s, loss=591, v_num=6w1t]\n",
      "Epoch 83:  50%|█████     | 1/2 [00:00<00:00, 27.43it/s, loss=591, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 83: 100%|██████████| 2/2 [00:00<00:00, 28.58it/s, loss=591, v_num=6w1t]\n",
      "Epoch 84:  50%|█████     | 1/2 [00:00<00:00, 26.07it/s, loss=590, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 84: 100%|██████████| 2/2 [00:00<00:00, 28.01it/s, loss=590, v_num=6w1t]\n",
      "Epoch 85:  50%|█████     | 1/2 [00:00<00:00, 29.54it/s, loss=590, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 85: 100%|██████████| 2/2 [00:00<00:00, 27.12it/s, loss=590, v_num=6w1t]\n",
      "Epoch 86:  50%|█████     | 1/2 [00:00<00:00, 33.15it/s, loss=589, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 86: 100%|██████████| 2/2 [00:00<00:00, 21.54it/s, loss=589, v_num=6w1t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87:  50%|█████     | 1/2 [00:00<00:00, 27.04it/s, loss=588, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 87: 100%|██████████| 2/2 [00:00<00:00, 30.06it/s, loss=588, v_num=6w1t]\n",
      "Epoch 88:  50%|█████     | 1/2 [00:00<00:00, 29.69it/s, loss=588, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 88: 100%|██████████| 2/2 [00:00<00:00, 31.73it/s, loss=588, v_num=6w1t]\n",
      "Epoch 89:  50%|█████     | 1/2 [00:00<00:00, 31.88it/s, loss=587, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 89: 100%|██████████| 2/2 [00:00<00:00, 30.97it/s, loss=587, v_num=6w1t]\n",
      "Epoch 90:  50%|█████     | 1/2 [00:00<00:00, 29.33it/s, loss=587, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 90: 100%|██████████| 2/2 [00:00<00:00, 30.12it/s, loss=587, v_num=6w1t]\n",
      "Epoch 91:  50%|█████     | 1/2 [00:00<00:00, 27.65it/s, loss=586, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 91: 100%|██████████| 2/2 [00:00<00:00, 27.66it/s, loss=586, v_num=6w1t]\n",
      "Epoch 92:  50%|█████     | 1/2 [00:00<00:00, 28.27it/s, loss=586, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 92: 100%|██████████| 2/2 [00:00<00:00, 25.86it/s, loss=586, v_num=6w1t]\n",
      "Epoch 93:  50%|█████     | 1/2 [00:00<00:00, 31.76it/s, loss=585, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 93: 100%|██████████| 2/2 [00:00<00:00, 30.79it/s, loss=585, v_num=6w1t]\n",
      "Epoch 94:  50%|█████     | 1/2 [00:00<00:00, 24.76it/s, loss=584, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 94: 100%|██████████| 2/2 [00:00<00:00, 26.37it/s, loss=584, v_num=6w1t]\n",
      "Epoch 95:  50%|█████     | 1/2 [00:00<00:00, 33.50it/s, loss=583, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 95: 100%|██████████| 2/2 [00:00<00:00, 31.05it/s, loss=583, v_num=6w1t]\n",
      "Epoch 96:  50%|█████     | 1/2 [00:00<00:00, 33.44it/s, loss=583, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 96: 100%|██████████| 2/2 [00:00<00:00, 33.48it/s, loss=583, v_num=6w1t]\n",
      "Epoch 97:  50%|█████     | 1/2 [00:00<00:00, 32.57it/s, loss=582, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 97: 100%|██████████| 2/2 [00:00<00:00, 27.02it/s, loss=582, v_num=6w1t]\n",
      "Epoch 98:  50%|█████     | 1/2 [00:00<00:00, 29.13it/s, loss=581, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 98: 100%|██████████| 2/2 [00:00<00:00, 30.41it/s, loss=581, v_num=6w1t]\n",
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 13.69it/s, loss=581, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 10.84it/s, loss=581, v_num=6w1t]\n",
      "Epoch 100:  50%|█████     | 1/2 [00:00<00:00, 31.02it/s, loss=580, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 100: 100%|██████████| 2/2 [00:00<00:00, 24.75it/s, loss=580, v_num=6w1t]\n",
      "Epoch 101:  50%|█████     | 1/2 [00:00<00:00, 31.54it/s, loss=579, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 101: 100%|██████████| 2/2 [00:00<00:00, 31.75it/s, loss=579, v_num=6w1t]\n",
      "Epoch 102:  50%|█████     | 1/2 [00:00<00:00, 29.89it/s, loss=579, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 102: 100%|██████████| 2/2 [00:00<00:00, 29.68it/s, loss=579, v_num=6w1t]\n",
      "Epoch 103:  50%|█████     | 1/2 [00:00<00:00, 31.32it/s, loss=578, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 103: 100%|██████████| 2/2 [00:00<00:00, 32.34it/s, loss=578, v_num=6w1t]\n",
      "Epoch 104:  50%|█████     | 1/2 [00:00<00:00, 32.10it/s, loss=578, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 104: 100%|██████████| 2/2 [00:00<00:00, 31.53it/s, loss=578, v_num=6w1t]\n",
      "Epoch 105:  50%|█████     | 1/2 [00:00<00:00, 29.59it/s, loss=577, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 105: 100%|██████████| 2/2 [00:00<00:00, 23.44it/s, loss=577, v_num=6w1t]\n",
      "Epoch 106:  50%|█████     | 1/2 [00:00<00:00, 31.62it/s, loss=577, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 106: 100%|██████████| 2/2 [00:00<00:00, 31.10it/s, loss=577, v_num=6w1t]\n",
      "Epoch 107:  50%|█████     | 1/2 [00:00<00:00, 29.58it/s, loss=576, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 107: 100%|██████████| 2/2 [00:00<00:00, 29.74it/s, loss=576, v_num=6w1t]\n",
      "Epoch 108:  50%|█████     | 1/2 [00:00<00:00, 33.28it/s, loss=575, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 108: 100%|██████████| 2/2 [00:00<00:00, 34.98it/s, loss=575, v_num=6w1t]\n",
      "Epoch 109:  50%|█████     | 1/2 [00:00<00:00, 24.90it/s, loss=575, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 109: 100%|██████████| 2/2 [00:00<00:00, 26.93it/s, loss=575, v_num=6w1t]\n",
      "Epoch 110:  50%|█████     | 1/2 [00:00<00:00, 28.63it/s, loss=574, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 110: 100%|██████████| 2/2 [00:00<00:00, 30.40it/s, loss=574, v_num=6w1t]\n",
      "Epoch 111:  50%|█████     | 1/2 [00:00<00:00, 24.82it/s, loss=573, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 111: 100%|██████████| 2/2 [00:00<00:00, 27.83it/s, loss=573, v_num=6w1t]\n",
      "Epoch 112:  50%|█████     | 1/2 [00:00<00:00, 29.32it/s, loss=573, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 112: 100%|██████████| 2/2 [00:00<00:00, 22.92it/s, loss=573, v_num=6w1t]\n",
      "Epoch 113:  50%|█████     | 1/2 [00:00<00:00, 30.19it/s, loss=572, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 113: 100%|██████████| 2/2 [00:00<00:00, 30.21it/s, loss=572, v_num=6w1t]\n",
      "Epoch 114:  50%|█████     | 1/2 [00:00<00:00, 32.96it/s, loss=572, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 114: 100%|██████████| 2/2 [00:00<00:00, 35.63it/s, loss=572, v_num=6w1t]\n",
      "Epoch 115:  50%|█████     | 1/2 [00:00<00:00, 28.39it/s, loss=571, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 115: 100%|██████████| 2/2 [00:00<00:00, 31.23it/s, loss=571, v_num=6w1t]\n",
      "Epoch 116:  50%|█████     | 1/2 [00:00<00:00, 29.70it/s, loss=571, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 116: 100%|██████████| 2/2 [00:00<00:00, 29.37it/s, loss=571, v_num=6w1t]\n",
      "Epoch 117:  50%|█████     | 1/2 [00:00<00:00, 27.27it/s, loss=570, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 117: 100%|██████████| 2/2 [00:00<00:00, 28.17it/s, loss=570, v_num=6w1t]\n",
      "Epoch 118:  50%|█████     | 1/2 [00:00<00:00, 28.70it/s, loss=570, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 118: 100%|██████████| 2/2 [00:00<00:00, 25.12it/s, loss=570, v_num=6w1t]\n",
      "Epoch 119:  50%|█████     | 1/2 [00:00<00:00, 33.76it/s, loss=569, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 119: 100%|██████████| 2/2 [00:00<00:00, 32.16it/s, loss=569, v_num=6w1t]\n",
      "Epoch 120:  50%|█████     | 1/2 [00:00<00:00, 27.89it/s, loss=569, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 120: 100%|██████████| 2/2 [00:00<00:00, 32.35it/s, loss=569, v_num=6w1t]\n",
      "Epoch 121:  50%|█████     | 1/2 [00:00<00:00, 23.99it/s, loss=568, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 121: 100%|██████████| 2/2 [00:00<00:00, 26.86it/s, loss=568, v_num=6w1t]\n",
      "Epoch 122:  50%|█████     | 1/2 [00:00<00:00, 28.70it/s, loss=567, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 122: 100%|██████████| 2/2 [00:00<00:00, 30.52it/s, loss=567, v_num=6w1t]\n",
      "Epoch 123:  50%|█████     | 1/2 [00:00<00:00, 32.60it/s, loss=567, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 123: 100%|██████████| 2/2 [00:00<00:00, 32.87it/s, loss=567, v_num=6w1t]\n",
      "Epoch 124:  50%|█████     | 1/2 [00:00<00:00, 31.70it/s, loss=566, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 124: 100%|██████████| 2/2 [00:00<00:00, 30.04it/s, loss=566, v_num=6w1t]\n",
      "Epoch 125:  50%|█████     | 1/2 [00:00<00:00, 29.10it/s, loss=565, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 125: 100%|██████████| 2/2 [00:00<00:00, 31.72it/s, loss=565, v_num=6w1t]\n",
      "Epoch 126:  50%|█████     | 1/2 [00:00<00:00, 31.50it/s, loss=565, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 126: 100%|██████████| 2/2 [00:00<00:00, 32.28it/s, loss=565, v_num=6w1t]\n",
      "Epoch 127:  50%|█████     | 1/2 [00:00<00:00, 24.69it/s, loss=564, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 127: 100%|██████████| 2/2 [00:00<00:00, 29.36it/s, loss=564, v_num=6w1t]\n",
      "Epoch 128:  50%|█████     | 1/2 [00:00<00:00, 29.12it/s, loss=564, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 128: 100%|██████████| 2/2 [00:00<00:00, 32.00it/s, loss=564, v_num=6w1t]\n",
      "Epoch 129:  50%|█████     | 1/2 [00:00<00:00, 27.77it/s, loss=563, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 129: 100%|██████████| 2/2 [00:00<00:00, 27.35it/s, loss=563, v_num=6w1t]\n",
      "Epoch 130:  50%|█████     | 1/2 [00:00<00:00, 34.70it/s, loss=562, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 130: 100%|██████████| 2/2 [00:00<00:00, 29.47it/s, loss=562, v_num=6w1t]\n",
      "Epoch 131:  50%|█████     | 1/2 [00:00<00:00, 31.66it/s, loss=561, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 131: 100%|██████████| 2/2 [00:00<00:00, 32.78it/s, loss=561, v_num=6w1t]\n",
      "Epoch 132:  50%|█████     | 1/2 [00:00<00:00, 25.80it/s, loss=560, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 132: 100%|██████████| 2/2 [00:00<00:00, 31.26it/s, loss=560, v_num=6w1t]\n",
      "Epoch 133:  50%|█████     | 1/2 [00:00<00:00, 31.04it/s, loss=560, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 133: 100%|██████████| 2/2 [00:00<00:00, 33.13it/s, loss=560, v_num=6w1t]\n",
      "Epoch 134:  50%|█████     | 1/2 [00:00<00:00, 24.71it/s, loss=559, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 134: 100%|██████████| 2/2 [00:00<00:00, 24.28it/s, loss=559, v_num=6w1t]\n",
      "Epoch 135:  50%|█████     | 1/2 [00:00<00:00, 32.45it/s, loss=558, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 135: 100%|██████████| 2/2 [00:00<00:00, 28.07it/s, loss=558, v_num=6w1t]\n",
      "Epoch 136:  50%|█████     | 1/2 [00:00<00:00, 28.72it/s, loss=558, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 136: 100%|██████████| 2/2 [00:00<00:00, 28.11it/s, loss=558, v_num=6w1t]\n",
      "Epoch 137:  50%|█████     | 1/2 [00:00<00:00, 29.18it/s, loss=558, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 137: 100%|██████████| 2/2 [00:00<00:00, 28.74it/s, loss=558, v_num=6w1t]\n",
      "Epoch 138:  50%|█████     | 1/2 [00:00<00:00, 32.47it/s, loss=557, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 138: 100%|██████████| 2/2 [00:00<00:00, 33.45it/s, loss=557, v_num=6w1t]\n",
      "Epoch 139:  50%|█████     | 1/2 [00:00<00:00, 29.31it/s, loss=556, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 139: 100%|██████████| 2/2 [00:00<00:00, 29.12it/s, loss=556, v_num=6w1t]\n",
      "Epoch 140:  50%|█████     | 1/2 [00:00<00:00, 27.02it/s, loss=555, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 140: 100%|██████████| 2/2 [00:00<00:00, 22.02it/s, loss=555, v_num=6w1t]\n",
      "Epoch 141:  50%|█████     | 1/2 [00:00<00:00, 32.12it/s, loss=555, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 141: 100%|██████████| 2/2 [00:00<00:00, 29.98it/s, loss=555, v_num=6w1t]\n",
      "Epoch 142:  50%|█████     | 1/2 [00:00<00:00, 28.69it/s, loss=554, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 142: 100%|██████████| 2/2 [00:00<00:00, 30.44it/s, loss=554, v_num=6w1t]\n",
      "Epoch 143:  50%|█████     | 1/2 [00:00<00:00, 30.94it/s, loss=553, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 143: 100%|██████████| 2/2 [00:00<00:00, 31.97it/s, loss=553, v_num=6w1t]\n",
      "Epoch 144:  50%|█████     | 1/2 [00:00<00:00, 26.86it/s, loss=553, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 144: 100%|██████████| 2/2 [00:00<00:00, 29.14it/s, loss=553, v_num=6w1t]\n",
      "Epoch 145:  50%|█████     | 1/2 [00:00<00:00, 32.93it/s, loss=552, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 145: 100%|██████████| 2/2 [00:00<00:00, 28.98it/s, loss=552, v_num=6w1t]\n",
      "Epoch 146:  50%|█████     | 1/2 [00:00<00:00, 30.62it/s, loss=551, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 146: 100%|██████████| 2/2 [00:00<00:00, 30.52it/s, loss=551, v_num=6w1t]\n",
      "Epoch 147:  50%|█████     | 1/2 [00:00<00:00, 30.84it/s, loss=551, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 147: 100%|██████████| 2/2 [00:00<00:00, 32.26it/s, loss=551, v_num=6w1t]\n",
      "Epoch 148:  50%|█████     | 1/2 [00:00<00:00, 30.62it/s, loss=550, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 148: 100%|██████████| 2/2 [00:00<00:00, 30.92it/s, loss=550, v_num=6w1t]\n",
      "Epoch 149: 100%|██████████| 2/2 [00:00<00:00, 13.32it/s, loss=550, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 149: 100%|██████████| 2/2 [00:00<00:00, 11.11it/s, loss=550, v_num=6w1t]\n",
      "Epoch 150:  50%|█████     | 1/2 [00:00<00:00, 26.88it/s, loss=550, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 150: 100%|██████████| 2/2 [00:00<00:00, 28.87it/s, loss=550, v_num=6w1t]\n",
      "Epoch 151:  50%|█████     | 1/2 [00:00<00:00, 32.64it/s, loss=549, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 151: 100%|██████████| 2/2 [00:00<00:00, 28.06it/s, loss=549, v_num=6w1t]\n",
      "Epoch 152:  50%|█████     | 1/2 [00:00<00:00, 30.70it/s, loss=549, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 152: 100%|██████████| 2/2 [00:00<00:00, 34.57it/s, loss=549, v_num=6w1t]\n",
      "Epoch 153:  50%|█████     | 1/2 [00:00<00:00, 28.53it/s, loss=548, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 153: 100%|██████████| 2/2 [00:00<00:00, 28.17it/s, loss=548, v_num=6w1t]\n",
      "Epoch 154:  50%|█████     | 1/2 [00:00<00:00, 31.58it/s, loss=547, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 154: 100%|██████████| 2/2 [00:00<00:00, 33.32it/s, loss=547, v_num=6w1t]\n",
      "Epoch 155:  50%|█████     | 1/2 [00:00<00:00, 29.38it/s, loss=547, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 155: 100%|██████████| 2/2 [00:00<00:00, 29.69it/s, loss=547, v_num=6w1t]\n",
      "Epoch 156:  50%|█████     | 1/2 [00:00<00:00, 29.76it/s, loss=546, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 156: 100%|██████████| 2/2 [00:00<00:00, 29.80it/s, loss=546, v_num=6w1t]\n",
      "Epoch 157:  50%|█████     | 1/2 [00:00<00:00, 31.60it/s, loss=545, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 157: 100%|██████████| 2/2 [00:00<00:00, 36.08it/s, loss=545, v_num=6w1t]\n",
      "Epoch 158:  50%|█████     | 1/2 [00:00<00:00, 30.06it/s, loss=544, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 158: 100%|██████████| 2/2 [00:00<00:00, 31.13it/s, loss=544, v_num=6w1t]\n",
      "Epoch 159:  50%|█████     | 1/2 [00:00<00:00, 31.58it/s, loss=544, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 159: 100%|██████████| 2/2 [00:00<00:00, 32.27it/s, loss=544, v_num=6w1t]\n",
      "Epoch 160:  50%|█████     | 1/2 [00:00<00:00, 36.64it/s, loss=544, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 160: 100%|██████████| 2/2 [00:00<00:00, 32.46it/s, loss=544, v_num=6w1t]\n",
      "Epoch 161:  50%|█████     | 1/2 [00:00<00:00, 29.80it/s, loss=543, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 161: 100%|██████████| 2/2 [00:00<00:00, 31.07it/s, loss=543, v_num=6w1t]\n",
      "Epoch 162:  50%|█████     | 1/2 [00:00<00:00, 28.93it/s, loss=543, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 162: 100%|██████████| 2/2 [00:00<00:00, 31.26it/s, loss=543, v_num=6w1t]\n",
      "Epoch 163:  50%|█████     | 1/2 [00:00<00:00, 31.80it/s, loss=542, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 163: 100%|██████████| 2/2 [00:00<00:00, 31.03it/s, loss=542, v_num=6w1t]\n",
      "Epoch 164:  50%|█████     | 1/2 [00:00<00:00, 26.87it/s, loss=542, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 164: 100%|██████████| 2/2 [00:00<00:00, 26.52it/s, loss=542, v_num=6w1t]\n",
      "Epoch 165:  50%|█████     | 1/2 [00:00<00:00, 33.43it/s, loss=541, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 165: 100%|██████████| 2/2 [00:00<00:00, 30.94it/s, loss=541, v_num=6w1t]\n",
      "Epoch 166:  50%|█████     | 1/2 [00:00<00:00, 26.33it/s, loss=541, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 166: 100%|██████████| 2/2 [00:00<00:00, 27.65it/s, loss=541, v_num=6w1t]\n",
      "Epoch 167:  50%|█████     | 1/2 [00:00<00:00, 24.61it/s, loss=540, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 167: 100%|██████████| 2/2 [00:00<00:00, 28.70it/s, loss=540, v_num=6w1t]\n",
      "Epoch 168:  50%|█████     | 1/2 [00:00<00:00, 34.81it/s, loss=539, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 168: 100%|██████████| 2/2 [00:00<00:00, 30.55it/s, loss=539, v_num=6w1t]\n",
      "Epoch 169:  50%|█████     | 1/2 [00:00<00:00, 24.96it/s, loss=538, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 169: 100%|██████████| 2/2 [00:00<00:00, 27.93it/s, loss=538, v_num=6w1t]\n",
      "Epoch 170:  50%|█████     | 1/2 [00:00<00:00, 28.81it/s, loss=538, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 170: 100%|██████████| 2/2 [00:00<00:00, 31.87it/s, loss=538, v_num=6w1t]\n",
      "Epoch 171:  50%|█████     | 1/2 [00:00<00:00, 31.35it/s, loss=537, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 171: 100%|██████████| 2/2 [00:00<00:00, 30.56it/s, loss=537, v_num=6w1t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172:  50%|█████     | 1/2 [00:00<00:00, 31.01it/s, loss=536, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 172: 100%|██████████| 2/2 [00:00<00:00, 30.08it/s, loss=536, v_num=6w1t]\n",
      "Epoch 173:  50%|█████     | 1/2 [00:00<00:00, 26.03it/s, loss=536, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 173: 100%|██████████| 2/2 [00:00<00:00, 30.12it/s, loss=536, v_num=6w1t]\n",
      "Epoch 174:  50%|█████     | 1/2 [00:00<00:00, 22.28it/s, loss=535, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 174: 100%|██████████| 2/2 [00:00<00:00, 23.73it/s, loss=535, v_num=6w1t]\n",
      "Epoch 175:  50%|█████     | 1/2 [00:00<00:00, 28.43it/s, loss=535, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 175: 100%|██████████| 2/2 [00:00<00:00, 30.06it/s, loss=535, v_num=6w1t]\n",
      "Epoch 176:  50%|█████     | 1/2 [00:00<00:00, 32.85it/s, loss=534, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 176: 100%|██████████| 2/2 [00:00<00:00, 32.28it/s, loss=534, v_num=6w1t]\n",
      "Epoch 177:  50%|█████     | 1/2 [00:00<00:00, 30.16it/s, loss=534, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 177: 100%|██████████| 2/2 [00:00<00:00, 31.94it/s, loss=534, v_num=6w1t]\n",
      "Epoch 178:  50%|█████     | 1/2 [00:00<00:00, 27.52it/s, loss=533, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 178: 100%|██████████| 2/2 [00:00<00:00, 29.74it/s, loss=533, v_num=6w1t]\n",
      "Epoch 179:  50%|█████     | 1/2 [00:00<00:00, 25.19it/s, loss=532, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 179: 100%|██████████| 2/2 [00:00<00:00, 26.29it/s, loss=532, v_num=6w1t]\n",
      "Epoch 180:  50%|█████     | 1/2 [00:00<00:00, 29.62it/s, loss=532, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 180: 100%|██████████| 2/2 [00:00<00:00, 27.37it/s, loss=532, v_num=6w1t]\n",
      "Epoch 181:  50%|█████     | 1/2 [00:00<00:00, 31.70it/s, loss=531, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 181: 100%|██████████| 2/2 [00:00<00:00, 28.68it/s, loss=531, v_num=6w1t]\n",
      "Epoch 182:  50%|█████     | 1/2 [00:00<00:00, 31.53it/s, loss=531, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 182: 100%|██████████| 2/2 [00:00<00:00, 30.39it/s, loss=531, v_num=6w1t]\n",
      "Epoch 183:  50%|█████     | 1/2 [00:00<00:00, 31.13it/s, loss=530, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 183: 100%|██████████| 2/2 [00:00<00:00, 32.54it/s, loss=530, v_num=6w1t]\n",
      "Epoch 184:  50%|█████     | 1/2 [00:00<00:00, 27.40it/s, loss=530, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 184: 100%|██████████| 2/2 [00:00<00:00, 34.12it/s, loss=530, v_num=6w1t]\n",
      "Epoch 185:  50%|█████     | 1/2 [00:00<00:00, 28.57it/s, loss=529, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 185: 100%|██████████| 2/2 [00:00<00:00, 29.66it/s, loss=529, v_num=6w1t]\n",
      "Epoch 186:  50%|█████     | 1/2 [00:00<00:00, 28.20it/s, loss=529, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 186: 100%|██████████| 2/2 [00:00<00:00, 30.15it/s, loss=529, v_num=6w1t]\n",
      "Epoch 187:  50%|█████     | 1/2 [00:00<00:00, 23.79it/s, loss=528, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 187: 100%|██████████| 2/2 [00:00<00:00, 17.44it/s, loss=528, v_num=6w1t]\n",
      "Epoch 188:  50%|█████     | 1/2 [00:00<00:00, 32.19it/s, loss=528, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 188: 100%|██████████| 2/2 [00:00<00:00, 28.92it/s, loss=528, v_num=6w1t]\n",
      "Epoch 189:  50%|█████     | 1/2 [00:00<00:00, 34.34it/s, loss=528, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 189: 100%|██████████| 2/2 [00:00<00:00, 31.33it/s, loss=528, v_num=6w1t]\n",
      "Epoch 190:  50%|█████     | 1/2 [00:00<00:00, 25.88it/s, loss=527, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 190: 100%|██████████| 2/2 [00:00<00:00, 28.09it/s, loss=527, v_num=6w1t]\n",
      "Epoch 191:  50%|█████     | 1/2 [00:00<00:00, 31.18it/s, loss=527, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 191: 100%|██████████| 2/2 [00:00<00:00, 33.58it/s, loss=527, v_num=6w1t]\n",
      "Epoch 192:  50%|█████     | 1/2 [00:00<00:00, 28.24it/s, loss=527, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 192: 100%|██████████| 2/2 [00:00<00:00, 24.48it/s, loss=527, v_num=6w1t]\n",
      "Epoch 193:  50%|█████     | 1/2 [00:00<00:00, 25.78it/s, loss=526, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 193: 100%|██████████| 2/2 [00:00<00:00, 26.52it/s, loss=526, v_num=6w1t]\n",
      "Epoch 194:  50%|█████     | 1/2 [00:00<00:00, 31.31it/s, loss=526, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 194: 100%|██████████| 2/2 [00:00<00:00, 25.14it/s, loss=526, v_num=6w1t]\n",
      "Epoch 195:  50%|█████     | 1/2 [00:00<00:00, 32.64it/s, loss=525, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 195: 100%|██████████| 2/2 [00:00<00:00, 27.35it/s, loss=525, v_num=6w1t]\n",
      "Epoch 196:  50%|█████     | 1/2 [00:00<00:00, 31.53it/s, loss=525, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 196: 100%|██████████| 2/2 [00:00<00:00, 32.30it/s, loss=525, v_num=6w1t]\n",
      "Epoch 197:  50%|█████     | 1/2 [00:00<00:00, 30.39it/s, loss=524, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 197: 100%|██████████| 2/2 [00:00<00:00, 35.06it/s, loss=524, v_num=6w1t]\n",
      "Epoch 198:  50%|█████     | 1/2 [00:00<00:00, 31.75it/s, loss=524, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 198: 100%|██████████| 2/2 [00:00<00:00, 33.51it/s, loss=524, v_num=6w1t]\n",
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 13.78it/s, loss=524, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 11.09it/s, loss=524, v_num=6w1t]\n",
      "Epoch 200:  50%|█████     | 1/2 [00:00<00:00, 32.06it/s, loss=523, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 200: 100%|██████████| 2/2 [00:00<00:00, 30.23it/s, loss=523, v_num=6w1t]\n",
      "Epoch 201:  50%|█████     | 1/2 [00:00<00:00, 31.59it/s, loss=523, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 201: 100%|██████████| 2/2 [00:00<00:00, 29.30it/s, loss=523, v_num=6w1t]\n",
      "Epoch 202:  50%|█████     | 1/2 [00:00<00:00, 25.65it/s, loss=522, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 202: 100%|██████████| 2/2 [00:00<00:00, 27.86it/s, loss=522, v_num=6w1t]\n",
      "Epoch 203:  50%|█████     | 1/2 [00:00<00:00, 24.86it/s, loss=522, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 203: 100%|██████████| 2/2 [00:00<00:00, 29.90it/s, loss=522, v_num=6w1t]\n",
      "Epoch 204:  50%|█████     | 1/2 [00:00<00:00, 27.22it/s, loss=521, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 204: 100%|██████████| 2/2 [00:00<00:00, 29.34it/s, loss=521, v_num=6w1t]\n",
      "Epoch 205:  50%|█████     | 1/2 [00:00<00:00, 31.13it/s, loss=521, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 205: 100%|██████████| 2/2 [00:00<00:00, 28.01it/s, loss=521, v_num=6w1t]\n",
      "Epoch 206:  50%|█████     | 1/2 [00:00<00:00, 27.07it/s, loss=521, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 206: 100%|██████████| 2/2 [00:00<00:00, 24.97it/s, loss=521, v_num=6w1t]\n",
      "Epoch 207:  50%|█████     | 1/2 [00:00<00:00, 30.25it/s, loss=520, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 207: 100%|██████████| 2/2 [00:00<00:00, 33.51it/s, loss=520, v_num=6w1t]\n",
      "Epoch 208:  50%|█████     | 1/2 [00:00<00:00, 28.69it/s, loss=520, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 208: 100%|██████████| 2/2 [00:00<00:00, 30.46it/s, loss=520, v_num=6w1t]\n",
      "Epoch 209:  50%|█████     | 1/2 [00:00<00:00, 28.62it/s, loss=519, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 209: 100%|██████████| 2/2 [00:00<00:00, 27.12it/s, loss=519, v_num=6w1t]\n",
      "Epoch 210:  50%|█████     | 1/2 [00:00<00:00, 25.42it/s, loss=519, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 210: 100%|██████████| 2/2 [00:00<00:00, 28.60it/s, loss=519, v_num=6w1t]\n",
      "Epoch 211:  50%|█████     | 1/2 [00:00<00:00, 33.12it/s, loss=518, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 211: 100%|██████████| 2/2 [00:00<00:00, 34.74it/s, loss=518, v_num=6w1t]\n",
      "Epoch 212:  50%|█████     | 1/2 [00:00<00:00, 29.13it/s, loss=518, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 212: 100%|██████████| 2/2 [00:00<00:00, 19.90it/s, loss=518, v_num=6w1t]\n",
      "Epoch 213:  50%|█████     | 1/2 [00:00<00:00, 22.39it/s, loss=518, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 213: 100%|██████████| 2/2 [00:00<00:00, 27.04it/s, loss=518, v_num=6w1t]\n",
      "Epoch 214:  50%|█████     | 1/2 [00:00<00:00, 29.72it/s, loss=517, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 214: 100%|██████████| 2/2 [00:00<00:00, 32.41it/s, loss=517, v_num=6w1t]\n",
      "Epoch 215:  50%|█████     | 1/2 [00:00<00:00, 31.32it/s, loss=517, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 215: 100%|██████████| 2/2 [00:00<00:00, 33.31it/s, loss=517, v_num=6w1t]\n",
      "Epoch 216:  50%|█████     | 1/2 [00:00<00:00, 32.11it/s, loss=517, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 216: 100%|██████████| 2/2 [00:00<00:00, 31.57it/s, loss=517, v_num=6w1t]\n",
      "Epoch 217:  50%|█████     | 1/2 [00:00<00:00, 29.99it/s, loss=516, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 217: 100%|██████████| 2/2 [00:00<00:00, 29.68it/s, loss=516, v_num=6w1t]\n",
      "Epoch 218:  50%|█████     | 1/2 [00:00<00:00, 29.83it/s, loss=516, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 218: 100%|██████████| 2/2 [00:00<00:00, 32.50it/s, loss=516, v_num=6w1t]\n",
      "Epoch 219:  50%|█████     | 1/2 [00:00<00:00, 28.02it/s, loss=516, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 219: 100%|██████████| 2/2 [00:00<00:00, 31.54it/s, loss=516, v_num=6w1t]\n",
      "Epoch 220:  50%|█████     | 1/2 [00:00<00:00, 32.93it/s, loss=515, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 220: 100%|██████████| 2/2 [00:00<00:00, 36.91it/s, loss=515, v_num=6w1t]\n",
      "Epoch 221:  50%|█████     | 1/2 [00:00<00:00, 33.64it/s, loss=515, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 221: 100%|██████████| 2/2 [00:00<00:00, 28.91it/s, loss=515, v_num=6w1t]\n",
      "Epoch 222:  50%|█████     | 1/2 [00:00<00:00, 30.56it/s, loss=515, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 222: 100%|██████████| 2/2 [00:00<00:00, 32.46it/s, loss=515, v_num=6w1t]\n",
      "Epoch 223:  50%|█████     | 1/2 [00:00<00:00, 26.39it/s, loss=514, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 223: 100%|██████████| 2/2 [00:00<00:00, 29.03it/s, loss=514, v_num=6w1t]\n",
      "Epoch 224:  50%|█████     | 1/2 [00:00<00:00, 28.44it/s, loss=514, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 224: 100%|██████████| 2/2 [00:00<00:00, 31.10it/s, loss=514, v_num=6w1t]\n",
      "Epoch 225:  50%|█████     | 1/2 [00:00<00:00, 29.07it/s, loss=514, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 225: 100%|██████████| 2/2 [00:00<00:00, 26.61it/s, loss=514, v_num=6w1t]\n",
      "Epoch 226:  50%|█████     | 1/2 [00:00<00:00, 26.59it/s, loss=514, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 226: 100%|██████████| 2/2 [00:00<00:00, 30.72it/s, loss=514, v_num=6w1t]\n",
      "Epoch 227:  50%|█████     | 1/2 [00:00<00:00, 32.18it/s, loss=513, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 227: 100%|██████████| 2/2 [00:00<00:00, 13.58it/s, loss=513, v_num=6w1t]\n",
      "Epoch 228:  50%|█████     | 1/2 [00:00<00:00, 32.82it/s, loss=513, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 228: 100%|██████████| 2/2 [00:00<00:00, 24.70it/s, loss=513, v_num=6w1t]\n",
      "Epoch 229:  50%|█████     | 1/2 [00:00<00:00, 30.71it/s, loss=513, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 229: 100%|██████████| 2/2 [00:00<00:00, 26.66it/s, loss=513, v_num=6w1t]\n",
      "Epoch 230:  50%|█████     | 1/2 [00:00<00:00, 30.66it/s, loss=512, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 230: 100%|██████████| 2/2 [00:00<00:00, 30.70it/s, loss=512, v_num=6w1t]\n",
      "Epoch 231:  50%|█████     | 1/2 [00:00<00:00, 32.34it/s, loss=512, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 231: 100%|██████████| 2/2 [00:00<00:00, 33.88it/s, loss=512, v_num=6w1t]\n",
      "Epoch 232:  50%|█████     | 1/2 [00:00<00:00, 30.36it/s, loss=512, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 232: 100%|██████████| 2/2 [00:00<00:00, 32.62it/s, loss=512, v_num=6w1t]\n",
      "Epoch 233:  50%|█████     | 1/2 [00:00<00:00, 30.54it/s, loss=511, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 233: 100%|██████████| 2/2 [00:00<00:00, 34.82it/s, loss=511, v_num=6w1t]\n",
      "Epoch 234:  50%|█████     | 1/2 [00:00<00:00, 30.14it/s, loss=511, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 234: 100%|██████████| 2/2 [00:00<00:00, 28.66it/s, loss=511, v_num=6w1t]\n",
      "Epoch 235:  50%|█████     | 1/2 [00:00<00:00, 33.62it/s, loss=511, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 235: 100%|██████████| 2/2 [00:00<00:00, 27.51it/s, loss=511, v_num=6w1t]\n",
      "Epoch 236:  50%|█████     | 1/2 [00:00<00:00, 30.48it/s, loss=510, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 236: 100%|██████████| 2/2 [00:00<00:00, 32.24it/s, loss=510, v_num=6w1t]\n",
      "Epoch 237:  50%|█████     | 1/2 [00:00<00:00, 31.64it/s, loss=510, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 237: 100%|██████████| 2/2 [00:00<00:00, 32.82it/s, loss=510, v_num=6w1t]\n",
      "Epoch 238:  50%|█████     | 1/2 [00:00<00:00, 33.35it/s, loss=510, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 238: 100%|██████████| 2/2 [00:00<00:00, 35.52it/s, loss=510, v_num=6w1t]\n",
      "Epoch 239:  50%|█████     | 1/2 [00:00<00:00, 31.25it/s, loss=509, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 239: 100%|██████████| 2/2 [00:00<00:00, 33.11it/s, loss=509, v_num=6w1t]\n",
      "Epoch 240:  50%|█████     | 1/2 [00:00<00:00, 29.92it/s, loss=509, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 240: 100%|██████████| 2/2 [00:00<00:00, 30.18it/s, loss=509, v_num=6w1t]\n",
      "Epoch 241:  50%|█████     | 1/2 [00:00<00:00, 28.34it/s, loss=509, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 241: 100%|██████████| 2/2 [00:00<00:00, 28.71it/s, loss=509, v_num=6w1t]\n",
      "Epoch 242:  50%|█████     | 1/2 [00:00<00:00, 33.18it/s, loss=509, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 242: 100%|██████████| 2/2 [00:00<00:00, 32.08it/s, loss=509, v_num=6w1t]\n",
      "Epoch 243:  50%|█████     | 1/2 [00:00<00:00, 33.94it/s, loss=508, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 243: 100%|██████████| 2/2 [00:00<00:00, 31.04it/s, loss=508, v_num=6w1t]\n",
      "Epoch 244:  50%|█████     | 1/2 [00:00<00:00, 24.15it/s, loss=508, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 244: 100%|██████████| 2/2 [00:00<00:00, 28.58it/s, loss=508, v_num=6w1t]\n",
      "Epoch 245:  50%|█████     | 1/2 [00:00<00:00, 30.63it/s, loss=507, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 245: 100%|██████████| 2/2 [00:00<00:00, 33.92it/s, loss=507, v_num=6w1t]\n",
      "Epoch 246:  50%|█████     | 1/2 [00:00<00:00, 25.92it/s, loss=507, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 246: 100%|██████████| 2/2 [00:00<00:00, 28.11it/s, loss=507, v_num=6w1t]\n",
      "Epoch 247:  50%|█████     | 1/2 [00:00<00:00, 32.16it/s, loss=507, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 247: 100%|██████████| 2/2 [00:00<00:00, 16.10it/s, loss=507, v_num=6w1t]\n",
      "Epoch 248:  50%|█████     | 1/2 [00:00<00:00, 28.21it/s, loss=506, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 248: 100%|██████████| 2/2 [00:00<00:00, 33.03it/s, loss=506, v_num=6w1t]\n",
      "Epoch 249: 100%|██████████| 2/2 [00:00<00:00, 15.40it/s, loss=506, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 249: 100%|██████████| 2/2 [00:00<00:00, 12.99it/s, loss=506, v_num=6w1t]\n",
      "Epoch 250:  50%|█████     | 1/2 [00:00<00:00, 25.17it/s, loss=506, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 250: 100%|██████████| 2/2 [00:00<00:00, 30.50it/s, loss=506, v_num=6w1t]\n",
      "Epoch 251:  50%|█████     | 1/2 [00:00<00:00, 28.02it/s, loss=505, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 251: 100%|██████████| 2/2 [00:00<00:00, 32.85it/s, loss=505, v_num=6w1t]\n",
      "Epoch 252:  50%|█████     | 1/2 [00:00<00:00, 30.32it/s, loss=505, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 252: 100%|██████████| 2/2 [00:00<00:00, 32.83it/s, loss=505, v_num=6w1t]\n",
      "Epoch 253:  50%|█████     | 1/2 [00:00<00:00, 25.67it/s, loss=505, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 253: 100%|██████████| 2/2 [00:00<00:00, 22.04it/s, loss=505, v_num=6w1t]\n",
      "Epoch 254:  50%|█████     | 1/2 [00:00<00:00, 27.28it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 254: 100%|██████████| 2/2 [00:00<00:00, 26.72it/s, loss=504, v_num=6w1t]\n",
      "Epoch 255:  50%|█████     | 1/2 [00:00<00:00, 29.60it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 255: 100%|██████████| 2/2 [00:00<00:00, 26.61it/s, loss=504, v_num=6w1t]\n",
      "Epoch 256:  50%|█████     | 1/2 [00:00<00:00, 32.80it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 256: 100%|██████████| 2/2 [00:00<00:00, 33.43it/s, loss=504, v_num=6w1t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257:  50%|█████     | 1/2 [00:00<00:00, 27.94it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 257: 100%|██████████| 2/2 [00:00<00:00, 29.61it/s, loss=504, v_num=6w1t]\n",
      "Epoch 258:  50%|█████     | 1/2 [00:00<00:00, 27.98it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 258: 100%|██████████| 2/2 [00:00<00:00, 32.13it/s, loss=504, v_num=6w1t]\n",
      "Epoch 259:  50%|█████     | 1/2 [00:00<00:00, 30.20it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 259: 100%|██████████| 2/2 [00:00<00:00, 32.18it/s, loss=504, v_num=6w1t]\n",
      "Epoch 260:  50%|█████     | 1/2 [00:00<00:00, 31.12it/s, loss=504, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 260: 100%|██████████| 2/2 [00:00<00:00, 30.19it/s, loss=504, v_num=6w1t]\n",
      "Epoch 261:  50%|█████     | 1/2 [00:00<00:00, 34.36it/s, loss=503, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 261: 100%|██████████| 2/2 [00:00<00:00, 30.46it/s, loss=503, v_num=6w1t]\n",
      "Epoch 262:  50%|█████     | 1/2 [00:00<00:00, 31.65it/s, loss=503, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 262: 100%|██████████| 2/2 [00:00<00:00, 30.04it/s, loss=503, v_num=6w1t]\n",
      "Epoch 263:  50%|█████     | 1/2 [00:00<00:00, 32.46it/s, loss=503, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 263: 100%|██████████| 2/2 [00:00<00:00, 28.35it/s, loss=503, v_num=6w1t]\n",
      "Epoch 264:  50%|█████     | 1/2 [00:00<00:00, 30.18it/s, loss=503, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 264: 100%|██████████| 2/2 [00:00<00:00, 30.97it/s, loss=503, v_num=6w1t]\n",
      "Epoch 265:  50%|█████     | 1/2 [00:00<00:00, 29.75it/s, loss=503, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 265: 100%|██████████| 2/2 [00:00<00:00, 33.49it/s, loss=503, v_num=6w1t]\n",
      "Epoch 266:  50%|█████     | 1/2 [00:00<00:00, 27.10it/s, loss=502, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 266: 100%|██████████| 2/2 [00:00<00:00, 28.59it/s, loss=502, v_num=6w1t]\n",
      "Epoch 267:  50%|█████     | 1/2 [00:00<00:00, 31.99it/s, loss=502, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 267: 100%|██████████| 2/2 [00:00<00:00, 24.50it/s, loss=502, v_num=6w1t]\n",
      "Epoch 268:  50%|█████     | 1/2 [00:00<00:00, 31.01it/s, loss=502, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 268: 100%|██████████| 2/2 [00:00<00:00, 35.02it/s, loss=502, v_num=6w1t]\n",
      "Epoch 269:  50%|█████     | 1/2 [00:00<00:00, 29.04it/s, loss=502, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 269: 100%|██████████| 2/2 [00:00<00:00, 32.67it/s, loss=502, v_num=6w1t]\n",
      "Epoch 270:  50%|█████     | 1/2 [00:00<00:00, 30.00it/s, loss=502, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 270: 100%|██████████| 2/2 [00:00<00:00, 33.71it/s, loss=502, v_num=6w1t]\n",
      "Epoch 271:  50%|█████     | 1/2 [00:00<00:00, 28.69it/s, loss=501, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 271: 100%|██████████| 2/2 [00:00<00:00, 32.79it/s, loss=501, v_num=6w1t]\n",
      "Epoch 272:  50%|█████     | 1/2 [00:00<00:00, 33.17it/s, loss=501, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 272: 100%|██████████| 2/2 [00:00<00:00, 31.07it/s, loss=501, v_num=6w1t]\n",
      "Epoch 273:  50%|█████     | 1/2 [00:00<00:00, 27.70it/s, loss=501, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 273: 100%|██████████| 2/2 [00:00<00:00, 29.08it/s, loss=501, v_num=6w1t]\n",
      "Epoch 274:  50%|█████     | 1/2 [00:00<00:00, 29.26it/s, loss=501, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 274: 100%|██████████| 2/2 [00:00<00:00, 28.16it/s, loss=501, v_num=6w1t]\n",
      "Epoch 275:  50%|█████     | 1/2 [00:00<00:00, 28.02it/s, loss=500, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 275: 100%|██████████| 2/2 [00:00<00:00, 29.25it/s, loss=500, v_num=6w1t]\n",
      "Epoch 276:  50%|█████     | 1/2 [00:00<00:00, 28.23it/s, loss=500, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 276: 100%|██████████| 2/2 [00:00<00:00, 31.19it/s, loss=500, v_num=6w1t]\n",
      "Epoch 277:  50%|█████     | 1/2 [00:00<00:00, 25.06it/s, loss=500, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 277: 100%|██████████| 2/2 [00:00<00:00, 27.46it/s, loss=500, v_num=6w1t]\n",
      "Epoch 278:  50%|█████     | 1/2 [00:00<00:00, 30.60it/s, loss=500, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 278: 100%|██████████| 2/2 [00:00<00:00, 31.43it/s, loss=500, v_num=6w1t]\n",
      "Epoch 279:  50%|█████     | 1/2 [00:00<00:00, 26.61it/s, loss=500, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 279: 100%|██████████| 2/2 [00:00<00:00, 29.35it/s, loss=500, v_num=6w1t]\n",
      "Epoch 280:  50%|█████     | 1/2 [00:00<00:00, 25.69it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 280: 100%|██████████| 2/2 [00:00<00:00, 31.46it/s, loss=499, v_num=6w1t]\n",
      "Epoch 281:  50%|█████     | 1/2 [00:00<00:00, 33.95it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 281: 100%|██████████| 2/2 [00:00<00:00, 35.76it/s, loss=499, v_num=6w1t]\n",
      "Epoch 282:  50%|█████     | 1/2 [00:00<00:00, 25.98it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 282: 100%|██████████| 2/2 [00:00<00:00, 26.16it/s, loss=499, v_num=6w1t]\n",
      "Epoch 283:  50%|█████     | 1/2 [00:00<00:00, 27.78it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 283: 100%|██████████| 2/2 [00:00<00:00, 29.66it/s, loss=499, v_num=6w1t]\n",
      "Epoch 284:  50%|█████     | 1/2 [00:00<00:00, 29.86it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 284: 100%|██████████| 2/2 [00:00<00:00, 32.89it/s, loss=499, v_num=6w1t]\n",
      "Epoch 285:  50%|█████     | 1/2 [00:00<00:00, 26.53it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 285: 100%|██████████| 2/2 [00:00<00:00, 27.15it/s, loss=499, v_num=6w1t]\n",
      "Epoch 286:  50%|█████     | 1/2 [00:00<00:00, 26.37it/s, loss=499, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 286: 100%|██████████| 2/2 [00:00<00:00, 27.69it/s, loss=499, v_num=6w1t]\n",
      "Epoch 287:  50%|█████     | 1/2 [00:00<00:00, 31.35it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 287: 100%|██████████| 2/2 [00:00<00:00, 31.72it/s, loss=498, v_num=6w1t]\n",
      "Epoch 288:  50%|█████     | 1/2 [00:00<00:00, 30.58it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 288: 100%|██████████| 2/2 [00:00<00:00, 19.10it/s, loss=498, v_num=6w1t]\n",
      "Epoch 289:  50%|█████     | 1/2 [00:00<00:00, 32.57it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 289: 100%|██████████| 2/2 [00:00<00:00, 34.21it/s, loss=498, v_num=6w1t]\n",
      "Epoch 290:  50%|█████     | 1/2 [00:00<00:00, 30.29it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 290: 100%|██████████| 2/2 [00:00<00:00, 35.32it/s, loss=498, v_num=6w1t]\n",
      "Epoch 291:  50%|█████     | 1/2 [00:00<00:00, 25.08it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 291: 100%|██████████| 2/2 [00:00<00:00, 26.16it/s, loss=498, v_num=6w1t]\n",
      "Epoch 292:  50%|█████     | 1/2 [00:00<00:00, 27.12it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 292: 100%|██████████| 2/2 [00:00<00:00, 30.81it/s, loss=498, v_num=6w1t]\n",
      "Epoch 293:  50%|█████     | 1/2 [00:00<00:00, 31.23it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 293: 100%|██████████| 2/2 [00:00<00:00, 31.14it/s, loss=498, v_num=6w1t]\n",
      "Epoch 294:  50%|█████     | 1/2 [00:00<00:00, 26.43it/s, loss=498, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 294: 100%|██████████| 2/2 [00:00<00:00, 22.29it/s, loss=498, v_num=6w1t]\n",
      "Epoch 295:  50%|█████     | 1/2 [00:00<00:00, 28.29it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 295: 100%|██████████| 2/2 [00:00<00:00, 32.13it/s, loss=497, v_num=6w1t]\n",
      "Epoch 296:  50%|█████     | 1/2 [00:00<00:00, 31.00it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 296: 100%|██████████| 2/2 [00:00<00:00, 32.79it/s, loss=497, v_num=6w1t]\n",
      "Epoch 297:  50%|█████     | 1/2 [00:00<00:00, 30.61it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 297: 100%|██████████| 2/2 [00:00<00:00, 33.71it/s, loss=497, v_num=6w1t]\n",
      "Epoch 298:  50%|█████     | 1/2 [00:00<00:00, 31.43it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 298: 100%|██████████| 2/2 [00:00<00:00, 30.72it/s, loss=497, v_num=6w1t]\n",
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 10.27it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00,  8.96it/s, loss=497, v_num=6w1t]\n",
      "Epoch 300:  50%|█████     | 1/2 [00:00<00:00, 31.93it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 300: 100%|██████████| 2/2 [00:00<00:00, 26.38it/s, loss=497, v_num=6w1t]\n",
      "Epoch 301:  50%|█████     | 1/2 [00:00<00:00, 30.50it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 301: 100%|██████████| 2/2 [00:00<00:00, 26.93it/s, loss=497, v_num=6w1t]\n",
      "Epoch 302:  50%|█████     | 1/2 [00:00<00:00, 22.96it/s, loss=497, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 302: 100%|██████████| 2/2 [00:00<00:00, 20.60it/s, loss=497, v_num=6w1t]\n",
      "Epoch 303:  50%|█████     | 1/2 [00:00<00:00, 24.03it/s, loss=496, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 303: 100%|██████████| 2/2 [00:00<00:00, 26.19it/s, loss=496, v_num=6w1t]\n",
      "Epoch 304:  50%|█████     | 1/2 [00:00<00:00, 30.23it/s, loss=496, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 304: 100%|██████████| 2/2 [00:00<00:00, 31.12it/s, loss=496, v_num=6w1t]\n",
      "Epoch 305:  50%|█████     | 1/2 [00:00<00:00, 30.29it/s, loss=496, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 305: 100%|██████████| 2/2 [00:00<00:00, 29.68it/s, loss=496, v_num=6w1t]\n",
      "Epoch 306:  50%|█████     | 1/2 [00:00<00:00, 28.23it/s, loss=496, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 306: 100%|██████████| 2/2 [00:00<00:00, 28.56it/s, loss=496, v_num=6w1t]\n",
      "Epoch 307:  50%|█████     | 1/2 [00:00<00:00, 30.51it/s, loss=496, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 307: 100%|██████████| 2/2 [00:00<00:00, 17.89it/s, loss=496, v_num=6w1t]\n",
      "Epoch 308:  50%|█████     | 1/2 [00:00<00:00, 31.59it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 308: 100%|██████████| 2/2 [00:00<00:00, 31.01it/s, loss=495, v_num=6w1t]\n",
      "Epoch 309:  50%|█████     | 1/2 [00:00<00:00, 28.24it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 309: 100%|██████████| 2/2 [00:00<00:00, 33.22it/s, loss=495, v_num=6w1t]\n",
      "Epoch 310:  50%|█████     | 1/2 [00:00<00:00, 28.60it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 310: 100%|██████████| 2/2 [00:00<00:00, 31.22it/s, loss=495, v_num=6w1t]\n",
      "Epoch 311:  50%|█████     | 1/2 [00:00<00:00, 30.80it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 311: 100%|██████████| 2/2 [00:00<00:00, 31.91it/s, loss=495, v_num=6w1t]\n",
      "Epoch 312:  50%|█████     | 1/2 [00:00<00:00, 26.30it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 312: 100%|██████████| 2/2 [00:00<00:00, 29.25it/s, loss=495, v_num=6w1t]\n",
      "Epoch 313:  50%|█████     | 1/2 [00:00<00:00, 31.64it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 313: 100%|██████████| 2/2 [00:00<00:00, 26.02it/s, loss=495, v_num=6w1t]\n",
      "Epoch 314:  50%|█████     | 1/2 [00:00<00:00, 17.74it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 314: 100%|██████████| 2/2 [00:00<00:00, 17.90it/s, loss=495, v_num=6w1t]\n",
      "Epoch 315:  50%|█████     | 1/2 [00:00<00:00, 33.05it/s, loss=495, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 315: 100%|██████████| 2/2 [00:00<00:00, 32.89it/s, loss=495, v_num=6w1t]\n",
      "Epoch 316:  50%|█████     | 1/2 [00:00<00:00, 29.03it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 316: 100%|██████████| 2/2 [00:00<00:00, 32.60it/s, loss=494, v_num=6w1t]\n",
      "Epoch 317:  50%|█████     | 1/2 [00:00<00:00, 28.62it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 317: 100%|██████████| 2/2 [00:00<00:00, 32.28it/s, loss=494, v_num=6w1t]\n",
      "Epoch 318:  50%|█████     | 1/2 [00:00<00:00, 33.11it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 318: 100%|██████████| 2/2 [00:00<00:00, 34.33it/s, loss=494, v_num=6w1t]\n",
      "Epoch 319:  50%|█████     | 1/2 [00:00<00:00, 29.40it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 319: 100%|██████████| 2/2 [00:00<00:00, 31.83it/s, loss=494, v_num=6w1t]\n",
      "Epoch 320:  50%|█████     | 1/2 [00:00<00:00, 29.01it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 320: 100%|██████████| 2/2 [00:00<00:00, 25.34it/s, loss=494, v_num=6w1t]\n",
      "Epoch 321:  50%|█████     | 1/2 [00:00<00:00, 31.22it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 321: 100%|██████████| 2/2 [00:00<00:00, 28.56it/s, loss=494, v_num=6w1t]\n",
      "Epoch 322:  50%|█████     | 1/2 [00:00<00:00, 28.37it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 322: 100%|██████████| 2/2 [00:00<00:00, 25.74it/s, loss=493, v_num=6w1t]\n",
      "Epoch 323:  50%|█████     | 1/2 [00:00<00:00, 31.86it/s, loss=494, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 323: 100%|██████████| 2/2 [00:00<00:00, 35.74it/s, loss=494, v_num=6w1t]\n",
      "Epoch 324:  50%|█████     | 1/2 [00:00<00:00, 26.58it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 324: 100%|██████████| 2/2 [00:00<00:00, 28.59it/s, loss=493, v_num=6w1t]\n",
      "Epoch 325:  50%|█████     | 1/2 [00:00<00:00, 32.27it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 325: 100%|██████████| 2/2 [00:00<00:00, 33.04it/s, loss=493, v_num=6w1t]\n",
      "Epoch 326:  50%|█████     | 1/2 [00:00<00:00, 32.41it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 326: 100%|██████████| 2/2 [00:00<00:00, 35.32it/s, loss=493, v_num=6w1t]\n",
      "Epoch 327:  50%|█████     | 1/2 [00:00<00:00, 30.25it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 327: 100%|██████████| 2/2 [00:00<00:00, 24.35it/s, loss=493, v_num=6w1t]\n",
      "Epoch 328:  50%|█████     | 1/2 [00:00<00:00, 32.46it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 328: 100%|██████████| 2/2 [00:00<00:00, 32.50it/s, loss=493, v_num=6w1t]\n",
      "Epoch 329:  50%|█████     | 1/2 [00:00<00:00, 29.17it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 329: 100%|██████████| 2/2 [00:00<00:00, 32.99it/s, loss=493, v_num=6w1t]\n",
      "Epoch 330:  50%|█████     | 1/2 [00:00<00:00, 31.73it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 330: 100%|██████████| 2/2 [00:00<00:00, 35.61it/s, loss=493, v_num=6w1t]\n",
      "Epoch 331:  50%|█████     | 1/2 [00:00<00:00, 28.41it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 331: 100%|██████████| 2/2 [00:00<00:00, 29.00it/s, loss=493, v_num=6w1t]\n",
      "Epoch 332:  50%|█████     | 1/2 [00:00<00:00, 33.56it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 332: 100%|██████████| 2/2 [00:00<00:00, 12.91it/s, loss=493, v_num=6w1t]\n",
      "Epoch 333:  50%|█████     | 1/2 [00:00<00:00, 32.93it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 333: 100%|██████████| 2/2 [00:00<00:00, 26.55it/s, loss=493, v_num=6w1t]\n",
      "Epoch 334:  50%|█████     | 1/2 [00:00<00:00, 34.72it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 334: 100%|██████████| 2/2 [00:00<00:00, 30.91it/s, loss=493, v_num=6w1t]\n",
      "Epoch 335:  50%|█████     | 1/2 [00:00<00:00, 32.31it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 335: 100%|██████████| 2/2 [00:00<00:00, 37.09it/s, loss=493, v_num=6w1t]\n",
      "Epoch 336:  50%|█████     | 1/2 [00:00<00:00, 33.93it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 336: 100%|██████████| 2/2 [00:00<00:00, 35.50it/s, loss=493, v_num=6w1t]\n",
      "Epoch 337:  50%|█████     | 1/2 [00:00<00:00, 31.91it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 337: 100%|██████████| 2/2 [00:00<00:00, 33.86it/s, loss=493, v_num=6w1t]\n",
      "Epoch 338:  50%|█████     | 1/2 [00:00<00:00, 28.71it/s, loss=493, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 338: 100%|██████████| 2/2 [00:00<00:00, 30.59it/s, loss=493, v_num=6w1t]\n",
      "Epoch 339:  50%|█████     | 1/2 [00:00<00:00, 27.66it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 339: 100%|██████████| 2/2 [00:00<00:00, 13.46it/s, loss=492, v_num=6w1t]\n",
      "Epoch 340:  50%|█████     | 1/2 [00:00<00:00, 31.99it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 340: 100%|██████████| 2/2 [00:00<00:00, 27.17it/s, loss=492, v_num=6w1t]\n",
      "Epoch 341:  50%|█████     | 1/2 [00:00<00:00, 37.43it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 341: 100%|██████████| 2/2 [00:00<00:00, 34.58it/s, loss=492, v_num=6w1t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342:  50%|█████     | 1/2 [00:00<00:00, 27.90it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 342: 100%|██████████| 2/2 [00:00<00:00, 22.66it/s, loss=492, v_num=6w1t]\n",
      "Epoch 343:  50%|█████     | 1/2 [00:00<00:00, 34.09it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 343: 100%|██████████| 2/2 [00:00<00:00, 18.52it/s, loss=492, v_num=6w1t]\n",
      "Epoch 344:  50%|█████     | 1/2 [00:00<00:00, 35.85it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 344: 100%|██████████| 2/2 [00:00<00:00, 37.25it/s, loss=492, v_num=6w1t]\n",
      "Epoch 345:  50%|█████     | 1/2 [00:00<00:00, 31.89it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 345: 100%|██████████| 2/2 [00:00<00:00, 34.96it/s, loss=492, v_num=6w1t]\n",
      "Epoch 346:  50%|█████     | 1/2 [00:00<00:00, 26.06it/s, loss=492, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 346: 100%|██████████| 2/2 [00:00<00:00, 28.67it/s, loss=492, v_num=6w1t]\n",
      "Epoch 347:  50%|█████     | 1/2 [00:00<00:00, 29.01it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 347: 100%|██████████| 2/2 [00:00<00:00, 31.44it/s, loss=491, v_num=6w1t]\n",
      "Epoch 348:  50%|█████     | 1/2 [00:00<00:00, 30.23it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 348: 100%|██████████| 2/2 [00:00<00:00, 31.95it/s, loss=491, v_num=6w1t]\n",
      "Epoch 349: 100%|██████████| 2/2 [00:00<00:00, 13.00it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 349: 100%|██████████| 2/2 [00:00<00:00, 10.90it/s, loss=491, v_num=6w1t]\n",
      "Epoch 350:  50%|█████     | 1/2 [00:00<00:00, 32.91it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 350: 100%|██████████| 2/2 [00:00<00:00, 33.87it/s, loss=491, v_num=6w1t]\n",
      "Epoch 351:  50%|█████     | 1/2 [00:00<00:00, 32.03it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 351: 100%|██████████| 2/2 [00:00<00:00, 31.58it/s, loss=491, v_num=6w1t]\n",
      "Epoch 352:  50%|█████     | 1/2 [00:00<00:00, 27.35it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 352: 100%|██████████| 2/2 [00:00<00:00, 30.55it/s, loss=491, v_num=6w1t]\n",
      "Epoch 353:  50%|█████     | 1/2 [00:00<00:00, 28.79it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 353: 100%|██████████| 2/2 [00:00<00:00, 31.22it/s, loss=491, v_num=6w1t]\n",
      "Epoch 354:  50%|█████     | 1/2 [00:00<00:00, 31.52it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 354: 100%|██████████| 2/2 [00:00<00:00, 33.91it/s, loss=491, v_num=6w1t]\n",
      "Epoch 355:  50%|█████     | 1/2 [00:00<00:00, 27.87it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 355: 100%|██████████| 2/2 [00:00<00:00, 29.86it/s, loss=491, v_num=6w1t]\n",
      "Epoch 356:  50%|█████     | 1/2 [00:00<00:00, 28.49it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 356: 100%|██████████| 2/2 [00:00<00:00, 31.13it/s, loss=491, v_num=6w1t]\n",
      "Epoch 357:  50%|█████     | 1/2 [00:00<00:00, 31.40it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 357: 100%|██████████| 2/2 [00:00<00:00, 32.03it/s, loss=491, v_num=6w1t]\n",
      "Epoch 358:  50%|█████     | 1/2 [00:00<00:00, 28.88it/s, loss=491, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 358: 100%|██████████| 2/2 [00:00<00:00, 29.01it/s, loss=491, v_num=6w1t]\n",
      "Epoch 359:  50%|█████     | 1/2 [00:00<00:00, 29.60it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 359: 100%|██████████| 2/2 [00:00<00:00, 29.04it/s, loss=490, v_num=6w1t]\n",
      "Epoch 360:  50%|█████     | 1/2 [00:00<00:00, 33.18it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 360: 100%|██████████| 2/2 [00:00<00:00, 33.98it/s, loss=490, v_num=6w1t]\n",
      "Epoch 361:  50%|█████     | 1/2 [00:00<00:00, 28.94it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 361: 100%|██████████| 2/2 [00:00<00:00, 30.38it/s, loss=490, v_num=6w1t]\n",
      "Epoch 362:  50%|█████     | 1/2 [00:00<00:00, 31.48it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 362: 100%|██████████| 2/2 [00:00<00:00, 33.84it/s, loss=490, v_num=6w1t]\n",
      "Epoch 363:  50%|█████     | 1/2 [00:00<00:00, 23.03it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 363: 100%|██████████| 2/2 [00:00<00:00, 26.35it/s, loss=490, v_num=6w1t]\n",
      "Epoch 364:  50%|█████     | 1/2 [00:00<00:00, 24.52it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 364: 100%|██████████| 2/2 [00:00<00:00, 25.52it/s, loss=490, v_num=6w1t]\n",
      "Epoch 365:  50%|█████     | 1/2 [00:00<00:00, 32.11it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 365: 100%|██████████| 2/2 [00:00<00:00, 34.46it/s, loss=490, v_num=6w1t]\n",
      "Epoch 366:  50%|█████     | 1/2 [00:00<00:00, 24.08it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 366: 100%|██████████| 2/2 [00:00<00:00, 28.88it/s, loss=490, v_num=6w1t]\n",
      "Epoch 367:  50%|█████     | 1/2 [00:00<00:00, 29.59it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 367: 100%|██████████| 2/2 [00:00<00:00, 32.67it/s, loss=490, v_num=6w1t]\n",
      "Epoch 368:  50%|█████     | 1/2 [00:00<00:00, 30.31it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 368: 100%|██████████| 2/2 [00:00<00:00, 31.15it/s, loss=490, v_num=6w1t]\n",
      "Epoch 369:  50%|█████     | 1/2 [00:00<00:00, 32.48it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 369: 100%|██████████| 2/2 [00:00<00:00, 30.37it/s, loss=490, v_num=6w1t]\n",
      "Epoch 370:  50%|█████     | 1/2 [00:00<00:00, 28.65it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 370: 100%|██████████| 2/2 [00:00<00:00, 24.23it/s, loss=490, v_num=6w1t]\n",
      "Epoch 371:  50%|█████     | 1/2 [00:00<00:00, 33.33it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 371: 100%|██████████| 2/2 [00:00<00:00, 28.87it/s, loss=490, v_num=6w1t]\n",
      "Epoch 372:  50%|█████     | 1/2 [00:00<00:00, 30.38it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 372: 100%|██████████| 2/2 [00:00<00:00, 34.56it/s, loss=490, v_num=6w1t]\n",
      "Epoch 373:  50%|█████     | 1/2 [00:00<00:00, 27.07it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 373: 100%|██████████| 2/2 [00:00<00:00, 24.30it/s, loss=490, v_num=6w1t]\n",
      "Epoch 374:  50%|█████     | 1/2 [00:00<00:00, 24.09it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 374: 100%|██████████| 2/2 [00:00<00:00, 21.57it/s, loss=490, v_num=6w1t]\n",
      "Epoch 375:  50%|█████     | 1/2 [00:00<00:00, 31.35it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 375: 100%|██████████| 2/2 [00:00<00:00, 27.87it/s, loss=489, v_num=6w1t]\n",
      "Epoch 376:  50%|█████     | 1/2 [00:00<00:00, 33.74it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 376: 100%|██████████| 2/2 [00:00<00:00, 31.15it/s, loss=489, v_num=6w1t]\n",
      "Epoch 377:  50%|█████     | 1/2 [00:00<00:00, 37.08it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 377: 100%|██████████| 2/2 [00:00<00:00, 35.06it/s, loss=489, v_num=6w1t]\n",
      "Epoch 378:  50%|█████     | 1/2 [00:00<00:00, 30.12it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 378: 100%|██████████| 2/2 [00:00<00:00, 33.53it/s, loss=489, v_num=6w1t]\n",
      "Epoch 379:  50%|█████     | 1/2 [00:00<00:00, 29.26it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 379: 100%|██████████| 2/2 [00:00<00:00, 31.34it/s, loss=489, v_num=6w1t]\n",
      "Epoch 380:  50%|█████     | 1/2 [00:00<00:00, 31.38it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 380: 100%|██████████| 2/2 [00:00<00:00, 32.90it/s, loss=489, v_num=6w1t]\n",
      "Epoch 381:  50%|█████     | 1/2 [00:00<00:00, 29.62it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 381: 100%|██████████| 2/2 [00:00<00:00, 27.92it/s, loss=490, v_num=6w1t]\n",
      "Epoch 382:  50%|█████     | 1/2 [00:00<00:00, 26.06it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 382: 100%|██████████| 2/2 [00:00<00:00, 28.38it/s, loss=490, v_num=6w1t]\n",
      "Epoch 383:  50%|█████     | 1/2 [00:00<00:00, 31.13it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 383: 100%|██████████| 2/2 [00:00<00:00, 29.07it/s, loss=490, v_num=6w1t]\n",
      "Epoch 384:  50%|█████     | 1/2 [00:00<00:00, 31.16it/s, loss=490, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 384: 100%|██████████| 2/2 [00:00<00:00, 34.44it/s, loss=490, v_num=6w1t]\n",
      "Epoch 385:  50%|█████     | 1/2 [00:00<00:00, 30.33it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 385: 100%|██████████| 2/2 [00:00<00:00, 30.55it/s, loss=489, v_num=6w1t]\n",
      "Epoch 386:  50%|█████     | 1/2 [00:00<00:00, 31.77it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 386: 100%|██████████| 2/2 [00:00<00:00, 28.79it/s, loss=489, v_num=6w1t]\n",
      "Epoch 387:  50%|█████     | 1/2 [00:00<00:00, 29.45it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 387: 100%|██████████| 2/2 [00:00<00:00, 31.32it/s, loss=489, v_num=6w1t]\n",
      "Epoch 388:  50%|█████     | 1/2 [00:00<00:00, 29.98it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 388: 100%|██████████| 2/2 [00:00<00:00, 31.99it/s, loss=489, v_num=6w1t]\n",
      "Epoch 389:  50%|█████     | 1/2 [00:00<00:00, 31.71it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 389: 100%|██████████| 2/2 [00:00<00:00, 33.16it/s, loss=489, v_num=6w1t]\n",
      "Epoch 390:  50%|█████     | 1/2 [00:00<00:00, 26.47it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 390: 100%|██████████| 2/2 [00:00<00:00, 26.92it/s, loss=489, v_num=6w1t]\n",
      "Epoch 391:  50%|█████     | 1/2 [00:00<00:00, 25.11it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 391: 100%|██████████| 2/2 [00:00<00:00, 25.59it/s, loss=489, v_num=6w1t]\n",
      "Epoch 392:  50%|█████     | 1/2 [00:00<00:00, 33.87it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 392: 100%|██████████| 2/2 [00:00<00:00, 35.99it/s, loss=489, v_num=6w1t]\n",
      "Epoch 393:  50%|█████     | 1/2 [00:00<00:00, 30.27it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 393: 100%|██████████| 2/2 [00:00<00:00, 33.08it/s, loss=489, v_num=6w1t]\n",
      "Epoch 394:  50%|█████     | 1/2 [00:00<00:00, 31.09it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 394: 100%|██████████| 2/2 [00:00<00:00, 28.31it/s, loss=489, v_num=6w1t]\n",
      "Epoch 395:  50%|█████     | 1/2 [00:00<00:00, 26.05it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 395: 100%|██████████| 2/2 [00:00<00:00, 26.13it/s, loss=489, v_num=6w1t]\n",
      "Epoch 396:  50%|█████     | 1/2 [00:00<00:00, 28.47it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 396: 100%|██████████| 2/2 [00:00<00:00, 26.32it/s, loss=489, v_num=6w1t]\n",
      "Epoch 397:  50%|█████     | 1/2 [00:00<00:00, 29.00it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 397: 100%|██████████| 2/2 [00:00<00:00, 30.06it/s, loss=489, v_num=6w1t]\n",
      "Epoch 398:  50%|█████     | 1/2 [00:00<00:00, 31.92it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 398: 100%|██████████| 2/2 [00:00<00:00, 32.73it/s, loss=489, v_num=6w1t]\n",
      "Epoch 399: 100%|██████████| 2/2 [00:00<00:00, 11.11it/s, loss=489, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 2/2 [00:00<00:00,  9.57it/s, loss=489, v_num=6w1t]\n",
      "Epoch 400:  50%|█████     | 1/2 [00:00<00:00, 26.00it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 400: 100%|██████████| 2/2 [00:00<00:00, 24.70it/s, loss=488, v_num=6w1t]\n",
      "Epoch 401:  50%|█████     | 1/2 [00:00<00:00, 26.78it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 401: 100%|██████████| 2/2 [00:00<00:00, 26.96it/s, loss=488, v_num=6w1t]\n",
      "Epoch 402:  50%|█████     | 1/2 [00:00<00:00, 30.76it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 402: 100%|██████████| 2/2 [00:00<00:00, 17.92it/s, loss=488, v_num=6w1t]\n",
      "Epoch 403:  50%|█████     | 1/2 [00:00<00:00, 35.09it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 403: 100%|██████████| 2/2 [00:00<00:00, 28.77it/s, loss=488, v_num=6w1t]\n",
      "Epoch 404:  50%|█████     | 1/2 [00:00<00:00, 28.66it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 404: 100%|██████████| 2/2 [00:00<00:00, 30.71it/s, loss=488, v_num=6w1t]\n",
      "Epoch 405:  50%|█████     | 1/2 [00:00<00:00, 29.57it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 405: 100%|██████████| 2/2 [00:00<00:00, 31.60it/s, loss=488, v_num=6w1t]\n",
      "Epoch 406:  50%|█████     | 1/2 [00:00<00:00, 30.69it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 406: 100%|██████████| 2/2 [00:00<00:00, 32.27it/s, loss=488, v_num=6w1t]\n",
      "Epoch 407:  50%|█████     | 1/2 [00:00<00:00, 26.84it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 407: 100%|██████████| 2/2 [00:00<00:00, 28.13it/s, loss=488, v_num=6w1t]\n",
      "Epoch 408:  50%|█████     | 1/2 [00:00<00:00, 27.97it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 408: 100%|██████████| 2/2 [00:00<00:00, 29.22it/s, loss=488, v_num=6w1t]\n",
      "Epoch 409:  50%|█████     | 1/2 [00:00<00:00, 33.39it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 409: 100%|██████████| 2/2 [00:00<00:00, 31.60it/s, loss=488, v_num=6w1t]\n",
      "Epoch 410:  50%|█████     | 1/2 [00:00<00:00, 25.56it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 410: 100%|██████████| 2/2 [00:00<00:00, 29.28it/s, loss=488, v_num=6w1t]\n",
      "Epoch 411:  50%|█████     | 1/2 [00:00<00:00, 27.98it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 411: 100%|██████████| 2/2 [00:00<00:00, 33.24it/s, loss=488, v_num=6w1t]\n",
      "Epoch 412:  50%|█████     | 1/2 [00:00<00:00, 32.65it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 412: 100%|██████████| 2/2 [00:00<00:00, 33.94it/s, loss=488, v_num=6w1t]\n",
      "Epoch 413:  50%|█████     | 1/2 [00:00<00:00, 28.74it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 413: 100%|██████████| 2/2 [00:00<00:00, 34.18it/s, loss=488, v_num=6w1t]\n",
      "Epoch 414:  50%|█████     | 1/2 [00:00<00:00, 26.18it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 414: 100%|██████████| 2/2 [00:00<00:00, 26.40it/s, loss=488, v_num=6w1t]\n",
      "Epoch 415:  50%|█████     | 1/2 [00:00<00:00, 30.89it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 415: 100%|██████████| 2/2 [00:00<00:00, 24.59it/s, loss=488, v_num=6w1t]\n",
      "Epoch 416:  50%|█████     | 1/2 [00:00<00:00, 29.60it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 416: 100%|██████████| 2/2 [00:00<00:00, 29.64it/s, loss=488, v_num=6w1t]\n",
      "Epoch 417:  50%|█████     | 1/2 [00:00<00:00, 28.69it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 417: 100%|██████████| 2/2 [00:00<00:00, 27.85it/s, loss=488, v_num=6w1t]\n",
      "Epoch 418:  50%|█████     | 1/2 [00:00<00:00, 27.90it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 418: 100%|██████████| 2/2 [00:00<00:00, 32.77it/s, loss=488, v_num=6w1t]\n",
      "Epoch 419:  50%|█████     | 1/2 [00:00<00:00, 29.86it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 419: 100%|██████████| 2/2 [00:00<00:00, 28.37it/s, loss=488, v_num=6w1t]\n",
      "Epoch 420:  50%|█████     | 1/2 [00:00<00:00, 26.85it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 420: 100%|██████████| 2/2 [00:00<00:00, 25.06it/s, loss=488, v_num=6w1t]\n",
      "Epoch 421:  50%|█████     | 1/2 [00:00<00:00, 31.40it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 421: 100%|██████████| 2/2 [00:00<00:00, 27.12it/s, loss=488, v_num=6w1t]\n",
      "Epoch 422:  50%|█████     | 1/2 [00:00<00:00, 32.26it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 422: 100%|██████████| 2/2 [00:00<00:00, 31.86it/s, loss=488, v_num=6w1t]\n",
      "Epoch 423:  50%|█████     | 1/2 [00:00<00:00, 33.03it/s, loss=488, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 423: 100%|██████████| 2/2 [00:00<00:00, 27.46it/s, loss=488, v_num=6w1t]\n",
      "Epoch 424:  50%|█████     | 1/2 [00:00<00:00, 33.10it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 424: 100%|██████████| 2/2 [00:00<00:00, 33.97it/s, loss=487, v_num=6w1t]\n",
      "Epoch 425:  50%|█████     | 1/2 [00:00<00:00, 33.20it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 425: 100%|██████████| 2/2 [00:00<00:00, 32.33it/s, loss=487, v_num=6w1t]\n",
      "Epoch 426:  50%|█████     | 1/2 [00:00<00:00, 31.25it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 426: 100%|██████████| 2/2 [00:00<00:00, 35.46it/s, loss=487, v_num=6w1t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427:  50%|█████     | 1/2 [00:00<00:00, 30.48it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 427: 100%|██████████| 2/2 [00:00<00:00, 33.38it/s, loss=487, v_num=6w1t]\n",
      "Epoch 428:  50%|█████     | 1/2 [00:00<00:00, 31.25it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 428: 100%|██████████| 2/2 [00:00<00:00, 27.56it/s, loss=487, v_num=6w1t]\n",
      "Epoch 429:  50%|█████     | 1/2 [00:00<00:00, 31.73it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 429: 100%|██████████| 2/2 [00:00<00:00, 32.18it/s, loss=487, v_num=6w1t]\n",
      "Epoch 430:  50%|█████     | 1/2 [00:00<00:00, 28.62it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 430: 100%|██████████| 2/2 [00:00<00:00, 32.49it/s, loss=487, v_num=6w1t]\n",
      "Epoch 431:  50%|█████     | 1/2 [00:00<00:00, 28.79it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 431: 100%|██████████| 2/2 [00:00<00:00, 31.70it/s, loss=487, v_num=6w1t]\n",
      "Epoch 432:  50%|█████     | 1/2 [00:00<00:00, 30.04it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 432: 100%|██████████| 2/2 [00:00<00:00, 32.93it/s, loss=487, v_num=6w1t]\n",
      "Epoch 433:  50%|█████     | 1/2 [00:00<00:00, 29.31it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 433: 100%|██████████| 2/2 [00:00<00:00, 30.64it/s, loss=487, v_num=6w1t]\n",
      "Epoch 434:  50%|█████     | 1/2 [00:00<00:00, 29.40it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 434: 100%|██████████| 2/2 [00:00<00:00, 28.75it/s, loss=487, v_num=6w1t]\n",
      "Epoch 435:  50%|█████     | 1/2 [00:00<00:00, 29.28it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 435: 100%|██████████| 2/2 [00:00<00:00, 28.84it/s, loss=487, v_num=6w1t]\n",
      "Epoch 436:  50%|█████     | 1/2 [00:00<00:00, 31.95it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 436: 100%|██████████| 2/2 [00:00<00:00, 33.66it/s, loss=487, v_num=6w1t]\n",
      "Epoch 437:  50%|█████     | 1/2 [00:00<00:00, 33.26it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 437: 100%|██████████| 2/2 [00:00<00:00, 33.90it/s, loss=487, v_num=6w1t]\n",
      "Epoch 438:  50%|█████     | 1/2 [00:00<00:00, 30.06it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 438: 100%|██████████| 2/2 [00:00<00:00, 33.39it/s, loss=487, v_num=6w1t]\n",
      "Epoch 439:  50%|█████     | 1/2 [00:00<00:00, 30.16it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 439: 100%|██████████| 2/2 [00:00<00:00, 31.96it/s, loss=487, v_num=6w1t]\n",
      "Epoch 440:  50%|█████     | 1/2 [00:00<00:00, 28.96it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 440: 100%|██████████| 2/2 [00:00<00:00, 28.17it/s, loss=487, v_num=6w1t]\n",
      "Epoch 441:  50%|█████     | 1/2 [00:00<00:00, 25.10it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 441: 100%|██████████| 2/2 [00:00<00:00, 26.99it/s, loss=487, v_num=6w1t]\n",
      "Epoch 442:  50%|█████     | 1/2 [00:00<00:00, 30.35it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 442: 100%|██████████| 2/2 [00:00<00:00, 33.29it/s, loss=487, v_num=6w1t]\n",
      "Epoch 443:  50%|█████     | 1/2 [00:00<00:00, 33.31it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 443: 100%|██████████| 2/2 [00:00<00:00, 32.93it/s, loss=487, v_num=6w1t]\n",
      "Epoch 444:  50%|█████     | 1/2 [00:00<00:00, 28.72it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 444: 100%|██████████| 2/2 [00:00<00:00, 29.85it/s, loss=486, v_num=6w1t]\n",
      "Epoch 445:  50%|█████     | 1/2 [00:00<00:00, 28.24it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 445: 100%|██████████| 2/2 [00:00<00:00, 29.29it/s, loss=487, v_num=6w1t]\n",
      "Epoch 446:  50%|█████     | 1/2 [00:00<00:00, 28.88it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 446: 100%|██████████| 2/2 [00:00<00:00, 27.34it/s, loss=487, v_num=6w1t]\n",
      "Epoch 447:  50%|█████     | 1/2 [00:00<00:00, 25.38it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 447: 100%|██████████| 2/2 [00:00<00:00, 29.14it/s, loss=487, v_num=6w1t]\n",
      "Epoch 448:  50%|█████     | 1/2 [00:00<00:00, 31.34it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 448: 100%|██████████| 2/2 [00:00<00:00, 33.68it/s, loss=487, v_num=6w1t]\n",
      "Epoch 449: 100%|██████████| 2/2 [00:00<00:00, 13.63it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 449: 100%|██████████| 2/2 [00:00<00:00, 11.81it/s, loss=487, v_num=6w1t]\n",
      "Epoch 450:  50%|█████     | 1/2 [00:00<00:00, 31.26it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 450: 100%|██████████| 2/2 [00:00<00:00, 27.88it/s, loss=487, v_num=6w1t]\n",
      "Epoch 451:  50%|█████     | 1/2 [00:00<00:00, 37.69it/s, loss=487, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 451: 100%|██████████| 2/2 [00:00<00:00, 29.37it/s, loss=487, v_num=6w1t]\n",
      "Epoch 452:  50%|█████     | 1/2 [00:00<00:00, 29.72it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 452: 100%|██████████| 2/2 [00:00<00:00, 31.79it/s, loss=486, v_num=6w1t]\n",
      "Epoch 453:  50%|█████     | 1/2 [00:00<00:00, 31.01it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 453: 100%|██████████| 2/2 [00:00<00:00, 34.04it/s, loss=486, v_num=6w1t]\n",
      "Epoch 454:  50%|█████     | 1/2 [00:00<00:00, 30.87it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 454: 100%|██████████| 2/2 [00:00<00:00, 31.95it/s, loss=486, v_num=6w1t]\n",
      "Epoch 455:  50%|█████     | 1/2 [00:00<00:00, 29.25it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 455: 100%|██████████| 2/2 [00:00<00:00, 32.09it/s, loss=486, v_num=6w1t]\n",
      "Epoch 456:  50%|█████     | 1/2 [00:00<00:00, 28.81it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 456: 100%|██████████| 2/2 [00:00<00:00, 32.38it/s, loss=486, v_num=6w1t]\n",
      "Epoch 457:  50%|█████     | 1/2 [00:00<00:00, 31.46it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 457: 100%|██████████| 2/2 [00:00<00:00, 26.55it/s, loss=486, v_num=6w1t]\n",
      "Epoch 458:  50%|█████     | 1/2 [00:00<00:00, 26.79it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 458: 100%|██████████| 2/2 [00:00<00:00, 20.60it/s, loss=486, v_num=6w1t]\n",
      "Epoch 459:  50%|█████     | 1/2 [00:00<00:00, 32.13it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 459: 100%|██████████| 2/2 [00:00<00:00, 33.64it/s, loss=486, v_num=6w1t]\n",
      "Epoch 460:  50%|█████     | 1/2 [00:00<00:00, 28.61it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 460: 100%|██████████| 2/2 [00:00<00:00, 30.82it/s, loss=486, v_num=6w1t]\n",
      "Epoch 461:  50%|█████     | 1/2 [00:00<00:00, 28.39it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 461: 100%|██████████| 2/2 [00:00<00:00, 27.27it/s, loss=486, v_num=6w1t]\n",
      "Epoch 462:  50%|█████     | 1/2 [00:00<00:00, 31.08it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 462: 100%|██████████| 2/2 [00:00<00:00, 30.94it/s, loss=486, v_num=6w1t]\n",
      "Epoch 463:  50%|█████     | 1/2 [00:00<00:00, 31.71it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 463: 100%|██████████| 2/2 [00:00<00:00, 35.35it/s, loss=486, v_num=6w1t]\n",
      "Epoch 464:  50%|█████     | 1/2 [00:00<00:00, 30.50it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 464: 100%|██████████| 2/2 [00:00<00:00, 36.03it/s, loss=486, v_num=6w1t]\n",
      "Epoch 465:  50%|█████     | 1/2 [00:00<00:00, 31.32it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 465: 100%|██████████| 2/2 [00:00<00:00, 32.24it/s, loss=486, v_num=6w1t]\n",
      "Epoch 466:  50%|█████     | 1/2 [00:00<00:00, 30.30it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 466: 100%|██████████| 2/2 [00:00<00:00, 27.18it/s, loss=486, v_num=6w1t]\n",
      "Epoch 467:  50%|█████     | 1/2 [00:00<00:00, 33.39it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 467: 100%|██████████| 2/2 [00:00<00:00, 26.06it/s, loss=486, v_num=6w1t]\n",
      "Epoch 468:  50%|█████     | 1/2 [00:00<00:00, 24.86it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 468: 100%|██████████| 2/2 [00:00<00:00, 27.58it/s, loss=486, v_num=6w1t]\n",
      "Epoch 469:  50%|█████     | 1/2 [00:00<00:00, 28.20it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 469: 100%|██████████| 2/2 [00:00<00:00, 33.88it/s, loss=486, v_num=6w1t]\n",
      "Epoch 470:  50%|█████     | 1/2 [00:00<00:00, 28.60it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 470: 100%|██████████| 2/2 [00:00<00:00, 30.66it/s, loss=486, v_num=6w1t]\n",
      "Epoch 471:  50%|█████     | 1/2 [00:00<00:00, 29.67it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 471: 100%|██████████| 2/2 [00:00<00:00, 29.12it/s, loss=486, v_num=6w1t]\n",
      "Epoch 472:  50%|█████     | 1/2 [00:00<00:00, 29.78it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 472: 100%|██████████| 2/2 [00:00<00:00, 31.90it/s, loss=486, v_num=6w1t]\n",
      "Epoch 473:  50%|█████     | 1/2 [00:00<00:00, 33.60it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 473: 100%|██████████| 2/2 [00:00<00:00, 31.52it/s, loss=486, v_num=6w1t]\n",
      "Epoch 474:  50%|█████     | 1/2 [00:00<00:00, 31.43it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 474: 100%|██████████| 2/2 [00:00<00:00, 33.81it/s, loss=486, v_num=6w1t]\n",
      "Epoch 475:  50%|█████     | 1/2 [00:00<00:00, 32.99it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 475: 100%|██████████| 2/2 [00:00<00:00, 36.70it/s, loss=486, v_num=6w1t]\n",
      "Epoch 476:  50%|█████     | 1/2 [00:00<00:00, 28.68it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 476: 100%|██████████| 2/2 [00:00<00:00, 29.50it/s, loss=486, v_num=6w1t]\n",
      "Epoch 477:  50%|█████     | 1/2 [00:00<00:00, 28.83it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 477: 100%|██████████| 2/2 [00:00<00:00, 27.78it/s, loss=486, v_num=6w1t]\n",
      "Epoch 478:  50%|█████     | 1/2 [00:00<00:00, 28.64it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 478: 100%|██████████| 2/2 [00:00<00:00, 26.35it/s, loss=486, v_num=6w1t]\n",
      "Epoch 479:  50%|█████     | 1/2 [00:00<00:00, 30.15it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 479: 100%|██████████| 2/2 [00:00<00:00, 34.67it/s, loss=486, v_num=6w1t]\n",
      "Epoch 480:  50%|█████     | 1/2 [00:00<00:00, 30.83it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 480: 100%|██████████| 2/2 [00:00<00:00, 33.56it/s, loss=486, v_num=6w1t]\n",
      "Epoch 481:  50%|█████     | 1/2 [00:00<00:00, 25.35it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 481: 100%|██████████| 2/2 [00:00<00:00, 27.95it/s, loss=486, v_num=6w1t]\n",
      "Epoch 482:  50%|█████     | 1/2 [00:00<00:00, 29.35it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 482: 100%|██████████| 2/2 [00:00<00:00, 26.94it/s, loss=486, v_num=6w1t]\n",
      "Epoch 483:  50%|█████     | 1/2 [00:00<00:00, 26.67it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 483: 100%|██████████| 2/2 [00:00<00:00, 29.00it/s, loss=486, v_num=6w1t]\n",
      "Epoch 484:  50%|█████     | 1/2 [00:00<00:00, 26.48it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 484: 100%|██████████| 2/2 [00:00<00:00, 29.20it/s, loss=486, v_num=6w1t]\n",
      "Epoch 485:  50%|█████     | 1/2 [00:00<00:00, 29.46it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 485: 100%|██████████| 2/2 [00:00<00:00, 29.33it/s, loss=486, v_num=6w1t]\n",
      "Epoch 486:  50%|█████     | 1/2 [00:00<00:00, 33.21it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 486: 100%|██████████| 2/2 [00:00<00:00, 34.01it/s, loss=485, v_num=6w1t]\n",
      "Epoch 487:  50%|█████     | 1/2 [00:00<00:00, 29.16it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 487: 100%|██████████| 2/2 [00:00<00:00, 32.77it/s, loss=485, v_num=6w1t]\n",
      "Epoch 488:  50%|█████     | 1/2 [00:00<00:00, 33.82it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 488: 100%|██████████| 2/2 [00:00<00:00, 35.17it/s, loss=485, v_num=6w1t]\n",
      "Epoch 489:  50%|█████     | 1/2 [00:00<00:00, 31.29it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 489: 100%|██████████| 2/2 [00:00<00:00, 35.58it/s, loss=485, v_num=6w1t]\n",
      "Epoch 490:  50%|█████     | 1/2 [00:00<00:00, 25.81it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 490: 100%|██████████| 2/2 [00:00<00:00, 28.23it/s, loss=485, v_num=6w1t]\n",
      "Epoch 491:  50%|█████     | 1/2 [00:00<00:00, 31.08it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 491: 100%|██████████| 2/2 [00:00<00:00, 29.62it/s, loss=485, v_num=6w1t]\n",
      "Epoch 492:  50%|█████     | 1/2 [00:00<00:00, 32.51it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 492: 100%|██████████| 2/2 [00:00<00:00, 24.54it/s, loss=485, v_num=6w1t]\n",
      "Epoch 493:  50%|█████     | 1/2 [00:00<00:00, 27.91it/s, loss=486, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 493: 100%|██████████| 2/2 [00:00<00:00, 23.89it/s, loss=486, v_num=6w1t]\n",
      "Epoch 494:  50%|█████     | 1/2 [00:00<00:00, 25.31it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 494: 100%|██████████| 2/2 [00:00<00:00, 21.55it/s, loss=485, v_num=6w1t]\n",
      "Epoch 495:  50%|█████     | 1/2 [00:00<00:00, 30.25it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 495: 100%|██████████| 2/2 [00:00<00:00, 20.42it/s, loss=485, v_num=6w1t]\n",
      "Epoch 496:  50%|█████     | 1/2 [00:00<00:00, 25.58it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 496: 100%|██████████| 2/2 [00:00<00:00, 30.50it/s, loss=485, v_num=6w1t]\n",
      "Epoch 497:  50%|█████     | 1/2 [00:00<00:00, 30.02it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 497: 100%|██████████| 2/2 [00:00<00:00, 33.16it/s, loss=485, v_num=6w1t]\n",
      "Epoch 498:  50%|█████     | 1/2 [00:00<00:00, 28.76it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 498: 100%|██████████| 2/2 [00:00<00:00, 31.61it/s, loss=485, v_num=6w1t]\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 14.24it/s, loss=485, v_num=6w1t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 11.98it/s, loss=485, v_num=6w1t]\n",
      "                                                 \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00,  7.59it/s, loss=485, v_num=6w1t]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/app/Contrastive-Learning-Benchmarking/models/wandb/run-20210211_181420-tki26w1t/files/checkpoints/ContrastiveLearning-cardgame-Scaling/CardGame:OR;attr2-val3;epsilon1e-100;d_model8;params0.36K;dot-product/last.ckpt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoints\n",
    "os.makedirs(ckpt_dir_PATH, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='avg_val_accuracy_by_Query',\n",
    "    dirpath=ckpt_dir_PATH,\n",
    "    filename='{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=6,\n",
    "    save_last=True,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[1], \n",
    "    min_epochs=2, max_epochs=500, \n",
    "    precision=32, \n",
    "    logger=wd_logger,\n",
    "    log_gpu_memory='all',\n",
    "    weights_summary = 'full',\n",
    "    gradient_clip_val=hparams['gradient_clip_val'],\n",
    "    replace_sampler_ddp=False,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "#fit\n",
    "with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(trainmodule, game_datamodule)\n",
    "    \n",
    "wandb.save(os.path.join(ckpt_dir_PATH, 'last.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-municipality",
   "metadata": {},
   "source": [
    "## Run Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "            GameTestFullDataset(raw_data=game_data, debug=True), \n",
    "            batch_size=hparams['batch_size'], shuffle=False\n",
    "        )\n",
    "res = trainer.test(model=trainmodule, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distribution_res = trainmodule.pull_model_distribution(debug=True)\n",
    "print('xy_hat_rank:', model_distribution_res['xy_hat_rank'])\n",
    "print('xy_div_xyind_hat_rank:', model_distribution_res['xy_div_xyind_hat_rank'])\n",
    "print('mi_hat:', model_distribution_res['mi_hat'])\n",
    "print('mi_gt_minus_hat:', model_distribution_res['mi_gt_minus_hat'])\n",
    "print('kl_div:', model_distribution_res['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(model_distribution_res['xy_hat'][figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(xy[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(model_distribution_res['xy_div_xyind_hat'][figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xy/xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-flooring",
   "metadata": {},
   "source": [
    "## Continue Training from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_PATH = os.path.join(ckpt_dir_PATH, 'last.ckpt')\n",
    "run_PATH = os.path.join(project_name, 'tki26w1t') # also from wandb interface\n",
    "\n",
    "wandb.restore(checkpoint_PATH, run_path=run_PATH)\n",
    "checkpoint = torch.load(checkpoint_PATH, map_location=lambda storage, loc: storage)\n",
    "trainmodule.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[1], \n",
    "    min_epochs=2, max_epochs=12000, \n",
    "    precision=32, \n",
    "    logger=wd_logger,\n",
    "    log_gpu_memory='all',\n",
    "    weights_summary = 'full',\n",
    "    gradient_clip_val=hparams['gradient_clip_val'],\n",
    "    replace_sampler_ddp=False,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(trainmodule, game_datamodule)\n",
    "    \n",
    "wandb.save(os.path.join(ckpt_dir_PATH, 'last.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-purple",
   "metadata": {},
   "source": [
    "## Test after reloading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_PATH = os.path.join(ckpt_dir_PATH, 'last.ckpt')\n",
    "run_PATH = os.path.join(project_name, 'tki26w1t') # also from wandb interface\n",
    "\n",
    "wandb.restore(checkpoint_PATH, run_path=run_PATH)\n",
    "checkpoint = torch.load(checkpoint_PATH, map_location=lambda storage, loc: storage)\n",
    "trainmodule.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[1], \n",
    "    min_epochs=2, max_epochs=300, \n",
    "    precision=32, \n",
    "    logger=wd_logger,\n",
    "    log_gpu_memory='all',\n",
    "    weights_summary = 'full',\n",
    "    gradient_clip_val=hparams['gradient_clip_val'],\n",
    "    replace_sampler_ddp=False,\n",
    "#     callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "            GameTestFullDataset(raw_data=game_data, debug=True), \n",
    "            batch_size=hparams['batch_size'], shuffle=False\n",
    "        )\n",
    "res = trainer.test(model=trainmodule, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distribution_res = trainmodule.pull_model_distribution(debug=True)\n",
    "print('xy_hat_rank:', model_distribution_res['xy_hat_rank'])\n",
    "print('xy_div_xyind_hat_rank:', model_distribution_res['xy_div_xyind_hat_rank'])\n",
    "print('mi_hat:', model_distribution_res['mi_hat'])\n",
    "print('mi_gt_minus_hat:', model_distribution_res['mi_gt_minus_hat'])\n",
    "print('kl_div:', model_distribution_res['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(model_distribution_res['xy_hat'][figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(xy[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(model_distribution_res['xy_div_xyind_hat'][figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = figsize)\n",
    "plt.imshow((xy/xyind)[figrange[0]:figrange[1], figrange[2]:figrange[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-exclusion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-conservation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
